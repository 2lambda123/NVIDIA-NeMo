name: 'wts${linear_combiner.cs_loss_weight}_${linear_combiner.sl1_loss_weight}_${linear_combiner.l1_loss_weight}_${linear_combiner.l2_loss_weight}_co${linear_combiner.data.high_freq_cutoff}_nx${linear_combiner.data.num_examples}_norm${linear_combiner.normalize}'

inference:
  greedy: True # Whether or not to use sampling ; use greedy decoding otherwise
  top_k: 0  # The number of highest probability vocabulary tokens to keep for top-k-filtering.
  top_p: 0.9 # If set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.
  temperature: 1.0 # sampling temperature
  add_BOS: True # add the bos token at the begining of the prompt
  tokens_to_generate: 30 # The minimum length of the sequence to be generated.
  all_probs: False  # whether return the log prob for all the tokens in vocab
  repetition_penalty: 1.2  # The parameter for repetition nalty. 1.0 means no penalty.
  min_tokens_to_generate: 0  # The minimum length of the sequence to be generated.
  compute_logprob: False  # a flag used to compute logprob of all the input text, a very special case of running inference, default False
  batch_size: 10

nemo_trainer:
  devices: 1
  num_nodes: 1
  accelerator: gpu
  logger: False # logger provided by exp_manager
  precision: 16 # 16, 32, or bf16


linear_combiner:
  save_checkpoint_prefix: "linear_combiner_checkpoints"
  save_checkpoint_path: '${linear_combiner.save_checkpoint_prefix}/${name}/'
  hidden_size: 164
  cs_loss_weight: 1.0
  sl1_loss_weight: 1.0
  l1_loss_weight: 1.0
  l2_loss_weight: 1.0
  normalize: False

  trainer:
    max_epochs: 200
    num_sanity_val_steps: 0
    check_val_every_n_epoch: 10
    devices: 1
    num_nodes: 1
    accelerator: gpu
    precision: 16 # 16, 32, or bf16

  data:
    high_freq_cutoff: 0
    num_examples: 40000
    train_dataset: "/home/adithyare/datasets/tasks/prompt-learning-data/squad/squad_train_10k.jsonl"
    eval_dataset: "/home/adithyare/datasets/tasks/prompt-learning-data/squad/squad_val_1k.jsonl"
  
  wandb:
    name: ${name}_lstm
    project: 'linear_combiner'

taskname: "squad"
small_model_path: "/home/adithyare/pretrained_models/megatron_gpt_125m/tp1pp1/megatron_gpt.nemo"
small_prompt_learning_model: "/home/adithyare/exp/125m_squad_prompt_tuning/prompt_learning.nemo"
large_model_path: "/home/adithyare/pretrained_models/megatron_gpt_1.3b/tp1pp1/megatron_gpt.nemo"
large_prompt_learning_model: "/home/adithyare/exp/1_3b_squad_prompt_tuning/prompt_learning.nemo"
projected_prompt_learning_model: '${linear_combiner.save_checkpoint_path}/projected_prompt_learning_model.nemo'
projected_pred_file_path: '${linear_combiner.save_checkpoint_path}/projected_predictions.txt'
