{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6249ae2",
   "metadata": {},
   "source": [
    "# FastPitch Speaker Adaptation and Speaker Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dcc082",
   "metadata": {},
   "source": [
    "This notebook is designed to provide a guide on how to run FastPitch Speaker Adaptation Pipeline. If you are not familiar with Adapters please go through the following [tutorial](https://github.com/NVIDIA/NeMo/blob/main/tutorials/02_NeMo_Adapters.ipynb). This tutorial contains the following sections:\n",
    "\n",
    "- Speaker Representation: discussion of few methods to represent speakers in FastPitch\n",
    "- Fine-tune FastPitch: fine-tune pre-trained multi-speaker FastPitch for a new speaker\n",
    "- Inference: generate speech from adapted FastPitch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf125bc",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "> Copyright 2022 NVIDIA. All Rights Reserved.\n",
    "> \n",
    "> Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "> \n",
    "> http://www.apache.org/licenses/LICENSE-2.0\n",
    "> \n",
    "> Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f82522",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can either run this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies# .\n",
    "\"\"\"\n",
    "BRANCH = 'main'\n",
    "# # If you're using Colab and not running locally, uncomment and run this cell.\n",
    "# !apt-get install sox libsndfile1 ffmpeg\n",
    "# !pip install wget unidecode pynini==2.1.4 scipy==1.7.3\n",
    "# !python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8dec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nemo\n",
    "import torch\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2e4f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "from nemo.collections.asr.parts.preprocessing.segment import AudioSegment\n",
    "from nemo.collections.tts.torch.g2ps import EnglishG2p\n",
    "from nemo.collections.tts.torch.data import TTSDataset\n",
    "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "from nemo.collections.tts.torch.tts_tokenizers import EnglishPhonemesTokenizer, EnglishCharsTokenizer\n",
    "\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d010d184",
   "metadata": {},
   "source": [
    "## Fine-tune FastPitch using Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e28d651",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Download a small dataset to demonstrate speaker adaptation using adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0651896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset download\n",
    "!wget https://nemo-public.s3.us-east-2.amazonaws.com/6097_5_mins.tar.gz  # Contains 10MB of data\n",
    "!tar -xzf 6097_5_mins.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 ./6097_5_mins/manifest.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a928a1d",
   "metadata": {},
   "source": [
    "For speaker adaptation the manifest must contain speaker ID (`speaker` field). Our downloaded manifest does not contain this field so we will add it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42fefe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_reader(filename):\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "            \n",
    "\n",
    "def json_writer(file, json_objects):\n",
    "    with open(file, \"w\") as f:\n",
    "        for jsonobj in json_objects:\n",
    "            jsonstr = json.dumps(jsonobj)\n",
    "            f.write(jsonstr + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = list(json_reader('./6097_5_mins/manifest.json'))\n",
    "for m in manifest:\n",
    "    m['speaker'] = 500\n",
    "json_writer('./6097_5_mins/manifest.json', manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce386b5",
   "metadata": {},
   "source": [
    "Split the data into train and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9fd1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./6097_5_mins/manifest.json | tail -n 2 > ./val_manifest.json\n",
    "!cat ./6097_5_mins/manifest.json | head -n -2 > ./train_manifest.json\n",
    "!ln -s ./6097_5_mins/audio audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a2c323",
   "metadata": {},
   "source": [
    "Download all additional files for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a8d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional files\n",
    "!mkdir -p tts_dataset_files && cd tts_dataset_files \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/scripts/tts_dataset_files/cmudict-0.7b_nv22.10 \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/scripts/tts_dataset_files/heteronyms-052722 \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/nemo_text_processing/text_normalization/en/data/whitelist/lj_speech.tsv \\\n",
    "&& cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bccacb",
   "metadata": {},
   "source": [
    "We use the `examples/tts/fastpitch_finetune_adapters.py` script to finetune the adapters with the `fastpitch_speaker_adaptation.yaml` configuration. So, let's download these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd5c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/fastpitch_finetune_adapters.py\n",
    "\n",
    "!mkdir -p conf \\\n",
    "&& cd conf \\\n",
    "&& wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/conf/fastpitch_speaker_adaptation.yaml \\\n",
    "&& cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c1a9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /workspace/NeMo/examples/tts/conf/fastpitch_speaker_adaptation.yaml conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6557a626",
   "metadata": {},
   "source": [
    "### Generate Supplementary Data\n",
    "\n",
    "It is recommended to precompute supplementary data like `pitch` and `alignment_matrix` before training.\n",
    "Let's define the normalizer and tokenizer for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a05cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text normalizer\n",
    "text_normalizer = Normalizer(\n",
    "    lang=\"en\", \n",
    "    input_case=\"cased\", \n",
    "    whitelist=\"tts_dataset_files/lj_speech.tsv\"\n",
    ")\n",
    "\n",
    "text_normalizer_call_kwargs = {\n",
    "    \"punct_pre_process\": True,\n",
    "    \"punct_post_process\": True\n",
    "}\n",
    "\n",
    "# Text tokenizer\n",
    "# Grapheme-to-phoneme module\n",
    "g2p = EnglishG2p(\n",
    "    phoneme_dict=\"tts_dataset_files/cmudict-0.7b_nv22.10\",\n",
    "    heteronyms=\"tts_dataset_files/heteronyms-052722\"\n",
    ")\n",
    "\n",
    "# Text tokenizer\n",
    "text_tokenizer = EnglishPhonemesTokenizer(\n",
    "    punct=True,\n",
    "    stresses=True,\n",
    "    chars=True,\n",
    "    apostrophe=True,\n",
    "    pad_with_space=True,\n",
    "    g2p=g2p,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190e3694",
   "metadata": {},
   "source": [
    "Defining a method that would compute and save the supplementary data. This method would return pitch statistics for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2845a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_calculate_supplementary_data(manifest_dir, sup_data_path, sup_data_types, text_tokenizer, text_normalizer, \n",
    "                                     text_normalizer_call_kwargs, \n",
    "                                     batch_size=32, \n",
    "                                     sample_rate=44100):\n",
    "    stages = [\"train\", \"val\"]\n",
    "    stage2dl = {}\n",
    "    for stage in stages:\n",
    "        ds = TTSDataset(\n",
    "            manifest_filepath=os.path.join(manifest_dir, f\"{stage}_manifest.json\"),\n",
    "            sample_rate=sample_rate,\n",
    "            sup_data_path=sup_data_path,\n",
    "            sup_data_types=sup_data_types,\n",
    "            n_fft=2048,\n",
    "            win_length=2048,\n",
    "            hop_length=512,\n",
    "            window=\"hann\",\n",
    "            n_mels=80,\n",
    "            lowfreq=0,\n",
    "            highfreq=22050,\n",
    "            text_tokenizer=text_tokenizer,\n",
    "            text_normalizer=text_normalizer,\n",
    "            text_normalizer_call_kwargs=text_normalizer_call_kwargs\n",
    "\n",
    "        ) \n",
    "        stage2dl[stage] = torch.utils.data.DataLoader(ds, batch_size=batch_size, collate_fn=ds._collate_fn, num_workers=12, pin_memory=True)\n",
    "    \n",
    "    # iteration over dataloaders\n",
    "    pitch_mean, pitch_std, pitch_min, pitch_max = None, None, None, None\n",
    "    for stage, dl in stage2dl.items():\n",
    "        pitch_list = []\n",
    "        for batch in tqdm(dl, total=len(dl)):\n",
    "            tokens, tokens_lengths, audios, audio_lengths, attn_prior, pitches, pitches_lengths = batch\n",
    "            pitch_list.append(pitches[pitches != 0])\n",
    "            \n",
    "        if stage == \"train\":\n",
    "            pitch_tensor = torch.cat(pitch_list)\n",
    "            pitch_mean, pitch_std = pitch_tensor.mean().item(), pitch_tensor.std().item()\n",
    "            pitch_min, pitch_max = pitch_tensor.min().item(), pitch_tensor.max().item()\n",
    "            \n",
    "    return pitch_mean, pitch_std, pitch_min, pitch_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca153ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastpitch_sup_data_path = \"fastpitch_sup_data_folder\"\n",
    "manifest_dir = \"./\"\n",
    "sup_data_types = [\"align_prior_matrix\", \"pitch\"]\n",
    "\n",
    "# Precompute the supplementary data like pitch, alignment matrix.\n",
    "pitch_mean, pitch_std, pitch_min, pitch_max = pre_calculate_supplementary_data(\n",
    "    manifest_dir, fastpitch_sup_data_path, sup_data_types, text_tokenizer, text_normalizer, text_normalizer_call_kwargs\n",
    ")\n",
    "\n",
    "pitch_mean, pitch_std, pitch_min, pitch_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac02843",
   "metadata": {},
   "source": [
    "New speakers are adapted on a pretrained multi-speaker model, in our example we will use [tts_en_fastpitch_multispeaker](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/tts_en_multispeaker_fastpitchhifigan) model. We will need the trained speaker IDs of the pretrained model, in our pretrained model the speaker IDs go from 1 to 20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d3b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = [spkr for spkr in range(1, 21)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d7cac",
   "metadata": {},
   "source": [
    "### Speaker Representation\n",
    "\n",
    "In this notebook we will go through few ways to represent speakers in TTS models, example FastPitch.\n",
    "\n",
    "1. **Lookup table embeddings:** This method represents each speaker by a fixed embedding which is learnt during training/finetuning of FastPitch. An embedding layer (`torch.nn.Embedding`) is added to FastPitch, these embeddings are used to condition the encoder, pitch/duration predictors and decoder. A disadvantage of this representation is that in case of adapting new speaker, the corresponding embedding does not exist and hence cannot be used for unseen speakers. A significant number of steps are needed for learning speaker embeddings for new/unseen speakers. An approach to solve this problem is to use weighted mean of the already existing speaker embeddings (see implementation `nemo.collections.tts.modules.submodules.WeightedSpeakerEmbedding`) but this method does not produce very good speach. To use this in NeMo add `speaker_id` to the list of `sup_data_types`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82459170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If Lookup table embeddings are used to represent speakers\n",
    "sup_data_types=\"['align_prior_matrix','pitch','speaker_id']\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a039fd65",
   "metadata": {},
   "source": [
    "2. **Global Style Tokens:** As introduced in the paper [Global Style Tokens (GST)](https://arxiv.org/pdf/1803.09017.pdf), in this method a bank of embeddings (tokens) are learnt along with the TTS model to represent a range of acoustic expressiveness. These style tokens are combined into a single embedding by an attention layer. During multi-speaker training, reference audio of the same speaker is used to calculate the GST embedding to represent the speaker. During inference, a fixed reference audio of the speaker or centroid of the speaker's GST embeddings can be used to condition the TTS model. To use this in NeMo add `speaker_id` and `gst_ref_audio` to the list of `sup_data_types` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f4d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If GST embeddings are used to represent speakers\n",
    "sup_data_types=\"['align_prior_matrix','pitch','speaker_id','gst_ref_audio']\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fcc2c5",
   "metadata": {},
   "source": [
    "3. **Speaker Verification:** As explained in this [paper](https://arxiv.org/abs/1806.04558), speaker embeddings can be extracted from Speaker Verification models like [Titanet](https://arxiv.org/abs/2110.04410) and used to condition TTS models like FastPitch. To use this in NeMo:\n",
    "    - Extract speaker embeddings from Titanet and save them.\n",
    "    - Add `speaker_embedding` to the list of `sup_data_types`.\n",
    "    - Add `+model.train_ds.dataset.speaker_embedding_path=<path/to/speaker/embeddings/from/Titanet>` to the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a0cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract speaker embeddings from speaker verification model (Titanet)\n",
    "\n",
    "def add_label_and_embeddingid(file_name):\n",
    "    \"\"\"\n",
    "    For extracting speaker embeddings from Titanet, the manifest needs to have a `label` field which is the same as speaker id,\n",
    "    so we will add this field to the manifest. We will also add another field `embedding_id` to the manifest, which denotes\n",
    "    the ID of the embedding in the embedding file that corresponds to the particular audio, and is needed when using this\n",
    "    embedding in FastPitch.\n",
    "    \"\"\"\n",
    "    manifest = list(json_reader(file_name))\n",
    "    for i, m in enumerate(manifest):\n",
    "        m['label'] = m['speaker']\n",
    "        m['embedding_id'] = i\n",
    "    json_writer(file_name, manifest)\n",
    "\n",
    "\n",
    "import nemo.collections.asr as nemo_asr\n",
    "\n",
    "!mkdir -p speaker_embeddings\n",
    "\n",
    "# For best performance, check that sampling rate of SV model matches that of TTS model\n",
    "verification_model = nemo_asr.models.EncDecSpeakerLabelModel.from_pretrained('titanet_large')\n",
    "verification_model.eval().cuda()\n",
    "\n",
    "stages = ['train', 'val']\n",
    "\n",
    "for stage in stages:\n",
    "    manifest_path = f\"./{stage}_manifest.json\"\n",
    "    add_label_and_embeddingid(manifest_path)\n",
    "    embedding_path = f\"./speaker_embeddings/{stage}_titanet_embedding.npy\"\n",
    "    embs, _, _, _ = nemo_asr.models.EncDecSpeakerLabelModel.get_batch_embeddings(verification_model,\n",
    "                                                                             manifest_filepath=manifest_path,\n",
    "                                                                             batch_size=32, \n",
    "                                                                             sample_rate=44100, \n",
    "                                                                             device='cuda')\n",
    "    np.save(embedding_path, embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8795e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If Titanet embeddings are used to represent speakers\n",
    "sup_data_types=\"['align_prior_matrix','pitch','speaker_embedding']\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb7f2c0",
   "metadata": {},
   "source": [
    "All three above methods can be used together as well (see implementation in `nemo.collections.tts.modules.submodules.SpeakerEncoder`). In this case `sup_data_types` should look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If Lookup table, GST speaker embedding, Titanet embeddings are used together\n",
    "sup_data_types=\"['align_prior_matrix','pitch','speaker_id','gst_ref_audio','speaker_embedding']\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0278f366",
   "metadata": {},
   "source": [
    "### Training Adapters\n",
    "\n",
    "Now we are ready for training the adapters! Let's try to adapt new speakers using adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c21581",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(python fastpitch_finetune_adapters.py \\\n",
    "    --config-name=fastpitch_speaker_adaptation.yaml \\\n",
    "    sample_rate=44100 \\\n",
    "    train_dataset=\"./train_manifest.json\" \\\n",
    "    validation_datasets=\"./val_manifest.json\" \\\n",
    "    sup_data_types={sup_data_types} \\\n",
    "    sup_data_path={fastpitch_sup_data_path} \\\n",
    "    +init_from_pretrained_model=\"tts_en_fastpitch_multispeaker\" \\\n",
    "    pitch_mean={pitch_mean} \\\n",
    "    pitch_std={pitch_std} \\\n",
    "    pitch_fmin={pitch_min} \\\n",
    "    pitch_fmax={pitch_max} \\\n",
    "    phoneme_dict_path=tts_dataset_files/cmudict-0.7b_nv22.10 \\\n",
    "    heteronyms_path=tts_dataset_files/heteronyms-052722 \\\n",
    "    whitelist_path=tts_dataset_files/lj_speech.tsv \\\n",
    "    model.speaker_embedding_dim=192 \\\n",
    "    model.n_speakers=12800 \\\n",
    "    model.adapter.add_weight_speaker=True \\\n",
    "    +model.adapter.add_weight_speaker_list=\"{speakers}\" \\\n",
    "    model.train_ds.dataloader_params.batch_size=24 \\\n",
    "    model.validation_ds.dataloader_params.batch_size=24 \\\n",
    "    model.train_ds.dataloader_params.num_workers=8 \\\n",
    "    model.validation_ds.dataloader_params.num_workers=8 \\\n",
    "    +model.train_ds.dataset.speaker_embedding_path=./speaker_embeddings/train_titanet_embedding.npy \\\n",
    "    +model.validation_ds.dataset.speaker_embedding_path=./speaker_embeddings/val_titanet_embedding.npy \\\n",
    "    model.optim.lr=1e-5 \\\n",
    "    ~model.optim.sched \\\n",
    "    model.optim.name=adamw \\\n",
    "    model.optim.weight_decay=0.0 \\\n",
    "    +model.text_tokenizer.add_blank_at=True \\\n",
    "    trainer.check_val_every_n_epoch=10 \\\n",
    "    trainer.max_epochs=50 \\\n",
    "    trainer.log_every_n_steps=1 \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.precision=32 \\\n",
    "    exp_manager.exp_dir=\"nemo_experiments\" \\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37dbd82",
   "metadata": {},
   "source": [
    "From the model summary it is evident that the number of trainable parameters is ~7% of the total number of parameters. Hence, the training is so fast, as we train only the additional parameters introduced by the adapters keeping the pretrained model frozen.\n",
    "\n",
    "Let's look at some of the options in the training command:\n",
    "\n",
    "`+init_from_pretrained_model` : FastPitch will load this pretrained checkpoint, and add adapters on top of this model.\n",
    "\n",
    "`model.n_speakers` : In the above pretrained model, there are already 12800 number of speaker embeddings, hence this parameter needs to match that number for loading the weights correctly.\n",
    "\n",
    "`~model.optim.sched` : Since this is a finetuning activity for only a few steps, we do not need a scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178418b8",
   "metadata": {},
   "source": [
    "Let's look at some fields in `fastpitch_speaker_adaptation.yaml`\n",
    "\n",
    "`adapter` : This is a new field inside `model` struct, this struct contains all adapter related configs.\n",
    "\n",
    "`adapter_name` : Name of the adapter.\n",
    "\n",
    "`adapter_module_name` : Names of modules where adapter has to be added/enabled.\n",
    "\n",
    "`adapter_state_dict_name` : Name of the state dict to be saved with adapter weights only.\n",
    "\n",
    "`add_random_speaker` : Since adapters are used to add new speakers, this flag tells if a random speaker embedding should be used to initialize the new speaker's embedding or some other pretrained speaker embedding needs to be used as initial value of the new speaker's embedding.\n",
    "\n",
    "`add_weight_speaker` : New/unseen speaker's embeddings are not present in the pretrained model, so if this option is set to true then the unseen speaker is represented by a weighted mean of the pretrained speakers.\n",
    "\n",
    "`add_weight_speaker_list` : User can choose the pretrained speakers to be used to calculate weighted mean in case `add_weight_speaker` is set to true.\n",
    "\n",
    "`speaker_encoder` : This is a new field inside `model` struct, this struct contains all speaker encoder related configs. The implementation can be found in `nemo.collections.tts.modules.submodules.SpeakerEncoder`. This module can combine GST (global style token) based speaker embeddings, speaker embeddings from speaker verificaiton models and lookup table speaker embeddings. This struct has sub-structs related to GST, Speaker Verification based embeddings and Lookup table based embeddings corresponding to the three parameters `SpeakerEncoder` class expects - gst_module, sv_projection_module and lookup_emb_projection_module.\n",
    "\n",
    " - `speaker_encoder.gst_module` : This struct is the GST related struct inside of `speaker_encoder`. If you are not using GST based speaker embedding then please remove it by adding `~model.speaker_encoder.gst_module` to the above command.\n",
    "\n",
    " - `speaker_encoder.sv_projection_module` : This struct is the Speaker verification related struct inside of `speaker_encoder`. If you are not using speaker verification based speaker embedding then please remove it by adding `~model.speaker_encoder.sv_projection_module` to the above command. However, if you are using speaker verification based speaker embedding then add the speaker_embedding_path for each dataset to the above command like:\n",
    "```\n",
    "+model.train_ds.dataset.speaker_embedding_path=speaker_embeddings/train_titanet_embedding.npy \\\n",
    "+model.validation_ds.dataset.speaker_embedding_path=./speaker_embeddings/val_titanet_embedding.npy \\\n",
    "```\n",
    "\n",
    "\n",
    "Look at `nemo.collections.common.parts.adapter_modules` to see the type of adapters available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb97532",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Inference using already trained adapter can be done using the following steps:\n",
    "\n",
    "- Update the config of the pretrained model to support adapters. This step replaces the module, where adapters need to be used, with the registered adapter class for that particular module.\n",
    "\n",
    "- Load the pretrained model.\n",
    "\n",
    "- Load the adapter weights using `load_adapters` method.\n",
    "\n",
    "- Generate spectrogram using this model and convert it to spectrogram using a vocoder.\n",
    "\n",
    "Let's go through these steps one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f3d147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Let's load validation manifest for inference\n",
    "val_manifest = list(json_reader('val_manifest.json'))\n",
    "# Select randomly an audio sample for the given speaker to use as reference audio for GST module or speaker verification module.\n",
    "ref = random.sample(val_manifest, 1)[0]\n",
    "# For illustration purposes we will infer on 2 samples only.\n",
    "val_manifest = random.sample(val_manifest, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f091f",
   "metadata": {},
   "source": [
    "#### Setup adapters for inference\n",
    "\n",
    "Let's prepare config file to use adapter modules in FastPitch instead of just the FastPitch model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4256d81d",
   "metadata": {},
   "source": [
    "Load adapter checkpoint. The checkpoint produced in the above training was saved in the folder `./nemo_experiments/FastPitch/{version}/checkpoints/` where version is based on the timestamp during training.\n",
    "\n",
    "After your training of adapters is done, check the folder `./nemo_experiments/FastPitch/`, you will find a folder with a timestamp as it's name. This folder name is the `version`, replace the version in the below cell with the version that your experiment generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f200e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.common.parts import adapter_modules\n",
    "from nemo.collections.tts.modules.submodules import WeightedSpeakerEmbedding\n",
    "from nemo.collections.tts.models import FastPitchModel, HifiGanModel\n",
    "\n",
    "spec_model_ckpt = \"./nemo_experiments/FastPitch/<version_folder_name>/checkpoints/<last_checkpoint.ckpt>\"\n",
    "\n",
    "spec_model = FastPitchModel.load_from_checkpoint(spec_model_ckpt, strict=False)\n",
    "state_dict = torch.load(spec_model_ckpt)['state_dict']\n",
    "has_adapter = any(['adapter' in k for k in state_dict.keys()])\n",
    "if has_adapter:\n",
    "    adapter_cfg = adapter_modules.LinearAdapterConfig(\n",
    "        in_features=spec_model.cfg.output_fft.d_model,  # conformer specific model dim. Every layer emits this dim at its output.\n",
    "        dim=256,  # the bottleneck dimension of the adapter\n",
    "        activation='swish',  # activation used in bottleneck block\n",
    "        norm_position='pre',  # whether to use LayerNorm at the beginning or the end of the adapter\n",
    "    )\n",
    "    spec_model.add_adapter(name='encoder+decoder+duration_predictor+pitch_predictor+aligner:adapter', cfg=adapter_cfg)\n",
    "    spec_model.set_enabled_adapters(enabled=False)\n",
    "    spec_model.set_enabled_adapters('adapter', enabled=True)\n",
    "    spec_model.unfreeze_enabled_adapters()\n",
    "\n",
    "spec_model.fastpitch.speaker_emb = WeightedSpeakerEmbedding(pretrained_embedding=spec_model.fastpitch.speaker_emb, speaker_list=speakers)\n",
    "spec_model.load_state_dict(state_dict)\n",
    "spec_model.eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532f3ac9",
   "metadata": {},
   "source": [
    "Since the above model uses GST based speaker representation, we will need the spectrogram and spectrogram length of the GST reference audio we selected previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bba0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "def load_wav(audio_file, target_sr=None):\n",
    "    '''\n",
    "    This method loads wav audio file at a given target sampling rate.\n",
    "    '''\n",
    "    with sf.SoundFile(audio_file, 'r') as f:\n",
    "        samples = f.read(dtype='float32')\n",
    "        sample_rate = f.samplerate\n",
    "    if target_sr is not None and sample_rate != target_sr:\n",
    "        samples = librosa.core.resample(samples, orig_sr=sample_rate, target_sr=target_sr)\n",
    "    return samples.transpose()\n",
    "\n",
    "\n",
    "device = spec_model.device\n",
    "gst_audio = load_wav(ref[\"audio_filepath\"], 44100)\n",
    "gst_audio = torch.from_numpy(gst_audio).unsqueeze(0).to(device)\n",
    "gst_audio_len = torch.tensor(gst_audio.shape[1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "gst_ref_spec, gst_ref_spec_lens = spec_model.preprocessor(input_signal=gst_audio, length=gst_audio_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5eeede",
   "metadata": {},
   "source": [
    "Extract reference speaker embedding from precalculated Titanet based speaker embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af60f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_embeddings = np.load(\"./speaker_embeddings/val_titanet_embedding.npy\")\n",
    "sv_speaker_emb = torch.tensor(sv_embeddings[ref['embedding_id']].reshape(-1, 1)).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78438ab5",
   "metadata": {},
   "source": [
    "To convert spectrogram to waveform we will need a **vocoder**. Let's load HiFiGAN vocoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d87f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vocoder = \"tts_en_hifitts_hifigan_ft_fastpitch\"\n",
    "vocoder = HifiGanModel.from_pretrained(pretrained_vocoder)\n",
    "vocoder.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0238fd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(spec_gen_model, vocoder_model, str_input, gst_ref_spec=None, gst_ref_spec_lens=None, speaker_embedding=None):\n",
    "    \"\"\"\n",
    "    Synthesizes spectrogram and audio from a text string given a spectrogram synthesis and vocoder model.\n",
    "    \n",
    "    Args:\n",
    "        spec_gen_model: Spectrogram generator model (FastPitch in our case)\n",
    "        vocoder_model: Vocoder model (HiFiGAN in our case)\n",
    "        str_input: Text input for the synthesis\n",
    "        gst_ref_spec: Reference spectrogram from same speaker for GST input\n",
    "        gst_ref_spec_lens: Reference spectrogram length\n",
    "    \n",
    "    Returns:\n",
    "        spectrogram and waveform of the synthesized audio.\n",
    "    \"\"\"\n",
    "    spec_gen_model.cuda().eval()\n",
    "    with torch.no_grad():\n",
    "        parsed = spec_gen_model.parse(str_input)\n",
    "        spectrogram = spec_gen_model.generate_spectrogram(\n",
    "            tokens=parsed, \n",
    "            gst_ref_spec=gst_ref_spec,\n",
    "            gst_ref_spec_lens=gst_ref_spec_lens,\n",
    "            speaker_embedding=speaker_embedding,\n",
    "        )\n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "        \n",
    "    if spectrogram is not None:\n",
    "        if isinstance(spectrogram, torch.Tensor):\n",
    "            spectrogram = spectrogram.to('cpu').numpy()\n",
    "        if len(spectrogram.shape) == 3:\n",
    "            spectrogram = spectrogram[0]\n",
    "    else:\n",
    "        raise Exception(\"None value was generated for spectrogram\")\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return spectrogram, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8370e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val_record in val_manifest:\n",
    "    str_input = val_record['text_normalized']\n",
    "    speaker_id = val_record['speaker']\n",
    "    _, audio_adapter = infer(spec_model, vocoder, str_input, gst_ref_spec, gst_ref_spec_lens, sv_speaker_emb.T)\n",
    "    print(f\"text: {str_input}\")\n",
    "    print(\"Original Audio\")\n",
    "    ipd.display(ipd.Audio(val_record['audio_filepath'], rate=44100))\n",
    "    print(\"Generated Audio with Adapter\")\n",
    "    ipd.display(ipd.Audio(audio_adapter, rate=44100))\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f713aa",
   "metadata": {},
   "source": [
    "**Note:** The quality of the generated audio may not be very good because for demo purposes we trained on <5mins of data, it is recommended to use 15-30mins of new user data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de9e89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
