{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9942568e",
   "metadata": {},
   "source": [
    "# FastPitch SpeakerAdaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5962b16",
   "metadata": {},
   "source": [
    "This notebook is designed to provide a guide on how to run FastPitch Speaker Adaptation Pipeline. If you are not familiar with Adapters please go through the following [tutorial](https://github.com/NVIDIA/NeMo/blob/main/tutorials/02_NeMo_Adapters.ipynb). This tutorial contains the following sections:\n",
    "\n",
    "- Fine-tune FastPitch: fine-tune pre-trained multi-speaker FastPitch for a new speaker\n",
    "- Inference: generate speech from adapted FastPitch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcede511",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "> Copyright 2022 NVIDIA. All Rights Reserved.\n",
    "> \n",
    "> Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "> \n",
    "> http://www.apache.org/licenses/LICENSE-2.0\n",
    "> \n",
    "> Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can either run this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies# .\n",
    "\"\"\"\n",
    "BRANCH = 'main'\n",
    "# # If you're using Colab and not running locally, uncomment and run this cell.\n",
    "# !apt-get install sox libsndfile1 ffmpeg\n",
    "# !pip install wget unidecode pynini==2.1.4 scipy==1.7.3\n",
    "# !python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ebe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nemo\n",
    "import torch\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a86d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "from nemo.collections.asr.parts.preprocessing.segment import AudioSegment\n",
    "from nemo.collections.tts.torch.g2ps import EnglishG2p\n",
    "from nemo.collections.tts.torch.data import TTSDataset\n",
    "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "from nemo.collections.tts.torch.tts_tokenizers import EnglishPhonemesTokenizer, EnglishCharsTokenizer\n",
    "\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36315863",
   "metadata": {},
   "source": [
    "## Fine-tune FastPitch using Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad821327",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Download a small dataset to demonstrate speaker adaptation using adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b124fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset download\n",
    "!wget https://nemo-public.s3.us-east-2.amazonaws.com/6097_5_mins.tar.gz  # Contains 10MB of data\n",
    "!tar -xzf 6097_5_mins.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9978223",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 ./6097_5_mins/manifest.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a13f7",
   "metadata": {},
   "source": [
    "For speaker adaptation the manifest must contain speaker ID (`speaker` field). Our downloaded manifest does not contain this field so we will add it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_reader(filename):\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "            \n",
    "\n",
    "def json_writer(file, json_objects):\n",
    "    with open(file, \"w\") as f:\n",
    "        for jsonobj in json_objects:\n",
    "            jsonstr = json.dumps(jsonobj)\n",
    "            f.write(jsonstr + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71630c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = list(json_reader('./6097_5_mins/manifest.json'))\n",
    "for m in manifest:\n",
    "    m['speaker'] = 500\n",
    "json_writer('./6097_5_mins/manifest.json', manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ac9ee",
   "metadata": {},
   "source": [
    "Split the data into train and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a4894",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./6097_5_mins/manifest.json | tail -n 2 > ./val_manifest.json\n",
    "!cat ./6097_5_mins/manifest.json | head -n -2 > ./train_manifest.json\n",
    "!ln -s ./6097_5_mins/audio audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaa32df",
   "metadata": {},
   "source": [
    "Download all additional files for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b28c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional files\n",
    "!mkdir -p tts_dataset_files && cd tts_dataset_files \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/scripts/tts_dataset_files/cmudict-0.7b_nv22.10 \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/scripts/tts_dataset_files/heteronyms-052722 \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/nemo_text_processing/text_normalization/en/data/whitelist/lj_speech.tsv \\\n",
    "&& cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026c254f",
   "metadata": {},
   "source": [
    "We use the `examples/tts/fastpitch_finetune_adapters.py` script to finetune the adapters with the `fastpitch_speaker_adaptation.yaml` configuration. So, let's download these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eca3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/fastpitch_finetune_adapters.py\n",
    "\n",
    "!mkdir -p conf \\\n",
    "&& cd conf \\\n",
    "&& wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/conf/fastpitch_speaker_adaptation.yaml \\\n",
    "&& cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c5ab4",
   "metadata": {},
   "source": [
    "### Generate Supplementary Data\n",
    "\n",
    "It is recommended to precompute supplementary data like `pitch` and `alignment_matrix` before training.\n",
    "Let's define the normalizer and tokenizer for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a98a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text normalizer\n",
    "text_normalizer = Normalizer(\n",
    "    lang=\"en\", \n",
    "    input_case=\"cased\", \n",
    "    whitelist=\"tts_dataset_files/lj_speech.tsv\"\n",
    ")\n",
    "\n",
    "text_normalizer_call_kwargs = {\n",
    "    \"punct_pre_process\": True,\n",
    "    \"punct_post_process\": True\n",
    "}\n",
    "\n",
    "# Text tokenizer\n",
    "# Grapheme-to-phoneme module\n",
    "g2p = EnglishG2p(\n",
    "    phoneme_dict=\"tts_dataset_files/cmudict-0.7b_nv22.10\",\n",
    "    heteronyms=\"tts_dataset_files/heteronyms-052722\"\n",
    ")\n",
    "\n",
    "# Text tokenizer\n",
    "text_tokenizer = EnglishPhonemesTokenizer(\n",
    "    punct=True,\n",
    "    stresses=True,\n",
    "    chars=True,\n",
    "    apostrophe=True,\n",
    "    pad_with_space=True,\n",
    "    g2p=g2p,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209511a",
   "metadata": {},
   "source": [
    "Defining a method that would compute and save the supplementary data. This method would return pitch statistics for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf62d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_calculate_supplementary_data(manifest_dir, sup_data_path, sup_data_types, text_tokenizer, text_normalizer, \n",
    "                                     text_normalizer_call_kwargs, \n",
    "                                     batch_size=32, \n",
    "                                     sample_rate=44100):\n",
    "    stages = [\"train\", \"val\"]\n",
    "    stage2dl = {}\n",
    "    for stage in stages:\n",
    "        ds = TTSDataset(\n",
    "            manifest_filepath=os.path.join(manifest_dir, f\"{stage}_manifest.json\"),\n",
    "            sample_rate=sample_rate,\n",
    "            sup_data_path=sup_data_path,\n",
    "            sup_data_types=sup_data_types,\n",
    "            n_fft=2048,\n",
    "            win_length=2048,\n",
    "            hop_length=512,\n",
    "            window=\"hann\",\n",
    "            n_mels=80,\n",
    "            lowfreq=0,\n",
    "            highfreq=22050,\n",
    "            text_tokenizer=text_tokenizer,\n",
    "            text_normalizer=text_normalizer,\n",
    "            text_normalizer_call_kwargs=text_normalizer_call_kwargs\n",
    "\n",
    "        ) \n",
    "        stage2dl[stage] = torch.utils.data.DataLoader(ds, batch_size=batch_size, collate_fn=ds._collate_fn, num_workers=12, pin_memory=True)\n",
    "    \n",
    "    # iteration over dataloaders\n",
    "    pitch_mean, pitch_std, pitch_min, pitch_max = None, None, None, None\n",
    "    for stage, dl in stage2dl.items():\n",
    "        pitch_list = []\n",
    "        for batch in tqdm(dl, total=len(dl)):\n",
    "            tokens, tokens_lengths, audios, audio_lengths, attn_prior, pitches, pitches_lengths = batch\n",
    "            pitch_list.append(pitches[pitches != 0])\n",
    "            \n",
    "        if stage == \"train\":\n",
    "            pitch_tensor = torch.cat(pitch_list)\n",
    "            pitch_mean, pitch_std = pitch_tensor.mean().item(), pitch_tensor.std().item()\n",
    "            pitch_min, pitch_max = pitch_tensor.min().item(), pitch_tensor.max().item()\n",
    "            \n",
    "    return pitch_mean, pitch_std, pitch_min, pitch_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b4b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastpitch_sup_data_path = \"fastpitch_sup_data_folder\"\n",
    "manifest_dir = \"./\"\n",
    "sup_data_types = [\"align_prior_matrix\", \"pitch\"]\n",
    "\n",
    "# Precompute the supplementary data like pitch, alignment matrix.\n",
    "pitch_mean, pitch_std, pitch_min, pitch_max = pre_calculate_supplementary_data(\n",
    "    manifest_dir, fastpitch_sup_data_path, sup_data_types, text_tokenizer, text_normalizer, text_normalizer_call_kwargs\n",
    ")\n",
    "\n",
    "pitch_mean, pitch_std, pitch_min, pitch_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96729c8",
   "metadata": {},
   "source": [
    "New speakers are adapted on a pretrained multi-speaker model, in our example we will use [tts_en_fastpitch_multispeaker](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/tts_en_multispeaker_fastpitchhifigan) model. We will need the trained speaker IDs of the pretrained model, in our pretrained model the speaker IDs go from 1 to 20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16a1fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = [spkr for spkr in range(1, 21)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ca0611",
   "metadata": {},
   "source": [
    "### Training Adapters\n",
    "\n",
    "Now we are ready for training the adapters! Let's try to adapt new speakers using adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1a8665",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(python fastpitch_finetune_adapters.py \\\n",
    "    --config-name=fastpitch_speaker_adaptation.yaml \\\n",
    "    sample_rate=44100 \\\n",
    "    train_dataset=\"./train_manifest.json\" \\\n",
    "    validation_datasets=\"./val_manifest.json\" \\\n",
    "    sup_data_types=\"['align_prior_matrix','pitch','speaker_id','gst_ref_audio']\" \\\n",
    "    sup_data_path={fastpitch_sup_data_path} \\\n",
    "    +init_from_pretrained_model=\"tts_en_fastpitch_multispeaker\" \\\n",
    "    pitch_mean={pitch_mean} \\\n",
    "    pitch_std={pitch_std} \\\n",
    "    pitch_fmin={pitch_min} \\\n",
    "    pitch_fmax={pitch_max} \\\n",
    "    model.n_speakers=12800 \\\n",
    "    phoneme_dict_path=tts_dataset_files/cmudict-0.7b_nv22.10 \\\n",
    "    heteronyms_path=tts_dataset_files/heteronyms-052722 \\\n",
    "    whitelist_path=tts_dataset_files/lj_speech.tsv \\\n",
    "    model.adapter.add_weight_speaker=True \\\n",
    "    +model.adapter.add_weight_speaker_list=\"{speakers}\" \\\n",
    "    model.train_ds.dataloader_params.batch_size=24 \\\n",
    "    model.validation_ds.dataloader_params.batch_size=24 \\\n",
    "    model.train_ds.dataloader_params.num_workers=8 \\\n",
    "    model.validation_ds.dataloader_params.num_workers=8 \\\n",
    "    model.optim.lr=1e-5 \\\n",
    "    ~model.optim.sched \\\n",
    "    model.optim.name=adamw \\\n",
    "    model.optim.weight_decay=0.0 \\\n",
    "    +model.text_tokenizer.add_blank_at=True \\\n",
    "    trainer.check_val_every_n_epoch=10 \\\n",
    "    trainer.max_epochs=100 \\\n",
    "    trainer.log_every_n_steps=1 \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.precision=32 \\\n",
    "    exp_manager.exp_dir=\"nemo_experiments\" \\\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d917ad6",
   "metadata": {},
   "source": [
    "From the model summary it is evident that the number of trainable parameters is ~7% of the total number of parameters. Hence, the training is so fast, as we train only the additional parameters introduced by the adapters keeping the pretrained model frozen.\n",
    "\n",
    "Let's look at some of the options in the training command:\n",
    "\n",
    "`+init_from_pretrained_model` : FastPitch will load this pretrained checkpoint, and add adapters on top of this model.\n",
    "\n",
    "`model.n_speakers` : In the above pretrained model, there are already 12800 number of speaker embeddings, hence this parameter needs to match that number for loading the weights correctly.\n",
    "\n",
    "`~model.optim.sched` : Since this is a finetuning activity for only a few steps, we do not need a scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7acb1cb",
   "metadata": {},
   "source": [
    "Let's look at some fields in `fastpitch_speaker_adaptation.yaml`\n",
    "\n",
    "`adapter` : This is a new field inside `model` struct, this struct contains all adapter related configs.\n",
    "\n",
    "`adapter_name` : Name of the adapter.\n",
    "\n",
    "`adapter_module_name` : Names of modules where adapter has to be added/enabled.\n",
    "\n",
    "`adapter_state_dict_name` : Name of the state dict to be saved with adapter weights only.\n",
    "\n",
    "`add_random_speaker` : Since adapters are used to add new speakers, this flag tells if a random speaker embedding should be used to initialize the new speaker's embedding or some other pretrained speaker embedding needs to be used as initial value of the new speaker's embedding.\n",
    "\n",
    "`add_weight_speaker` : New/unseen speaker's embeddings are not present in the pretrained model, so if this option is set to true then the unseen speaker is represented by a weighted mean of the pretrained speakers.\n",
    "\n",
    "`add_weight_speaker_list` : User can choose the pretrained speakers to be used to calculate weighted mean in case `add_weight_speaker` is set to true.\n",
    "\n",
    "`global_style_token` : This is a new field inside `model` struct, this struct contains all global style token related configs. [Global Style Tokens](https://arxiv.org/pdf/1803.09017.pdf) are used in addition to lookup table speaker embeddings to represent speaker embeddings.\n",
    "If user doesn't want to use global style token to represent speakers in addition to lookup table speaker embeddings, they can remove it by including `~model.global_style_token` in the commandline. The global style token module implementation can be found in `nemo.collections.tts.modules.submodules.GlobalStyleToken`.\n",
    "\n",
    "Look at `nemo.collections.common.parts.adapter_modules` to see the type of adapters available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23651249",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Inference using already trained adapter can be done using the following steps:\n",
    "\n",
    "- Update the config of the pretrained model to support adapters. This step replaces the module, where adapters need to be used, with the registered adapter class for that particular module.\n",
    "\n",
    "- Load the pretrained model.\n",
    "\n",
    "- Load the adapter weights using `load_adapters` method.\n",
    "\n",
    "- Generate spectrogram using this model and convert it to spectrogram using a vocoder.\n",
    "\n",
    "Let's go through these steps one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b67395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Let's load validation manifest for inference\n",
    "val_manifest = list(json_reader('val_manifest.json'))\n",
    "# Select randomly an audio sample for the given speaker to use as reference audio for GST module.\n",
    "ref = random.sample(val_manifest, 1)[0]\n",
    "# For illustration purposes we will infer on 2 samples only.\n",
    "val_manifest = random.sample(val_manifest, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d737f9c",
   "metadata": {},
   "source": [
    "#### Setup adapters for inference\n",
    "\n",
    "Let's prepare config file to use adapter modules in FastPitch instead of just the FastPitch model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b327a47",
   "metadata": {},
   "source": [
    "Load adapter checkpoint. The checkpoint produced in the above training was saved in the folder `./nemo_experiments/FastPitch/{version}/checkpoints/` where version is based on the timestamp during training.\n",
    "\n",
    "After your training of adapters is done, check the folder `./nemo_experiments/FastPitch/`, you will find a folder with a timestamp as it's name. This folder name is the `version`, replace the version in the below cell with the version that your experiment generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2ff7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.common.parts import adapter_modules\n",
    "from nemo.collections.tts.modules.submodules import WeightedSpeakerEmbedding\n",
    "from nemo.collections.tts.models import FastPitchModel, HifiGanModel\n",
    "\n",
    "spec_model_ckpt = \"./nemo_experiments/FastPitch/<version_folder_name>/checkpoints/<last_checkpoint.ckpt>\"\n",
    "\n",
    "spec_model = FastPitchModel.load_from_checkpoint(spec_model_ckpt, strict=False)\n",
    "state_dict = torch.load(spec_model_ckpt)['state_dict']\n",
    "has_adapter = any(['adapter' in k for k in state_dict.keys()])\n",
    "if has_adapter:\n",
    "    adapter_cfg = adapter_modules.LinearAdapterConfig(\n",
    "        in_features=spec_model.cfg.output_fft.d_model,  # conformer specific model dim. Every layer emits this dim at its output.\n",
    "        dim=256,  # the bottleneck dimension of the adapter\n",
    "        activation='swish',  # activation used in bottleneck block\n",
    "        norm_position='pre',  # whether to use LayerNorm at the beginning or the end of the adapter\n",
    "    )\n",
    "    spec_model.add_adapter(name='encoder+decoder+duration_predictor+pitch_predictor+aligner:adapter', cfg=adapter_cfg)\n",
    "    spec_model.set_enabled_adapters(enabled=False)\n",
    "    spec_model.set_enabled_adapters('adapter', enabled=True)\n",
    "    spec_model.unfreeze_enabled_adapters()\n",
    "\n",
    "spec_model.fastpitch.speaker_emb = WeightedSpeakerEmbedding(pretrained_embedding=spec_model.fastpitch.speaker_emb, speaker_list=speakers)\n",
    "spec_model.load_state_dict(state_dict)\n",
    "spec_model.eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3e4476",
   "metadata": {},
   "source": [
    "Since the above model uses GST based speaker representation, we will need the spectrogram and spectrogram length of the GST reference audio we selected previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85e5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "def load_wav(audio_file, target_sr=None):\n",
    "    '''\n",
    "    This method loads wav audio file at a given target sampling rate.\n",
    "    '''\n",
    "    with sf.SoundFile(audio_file, 'r') as f:\n",
    "        samples = f.read(dtype='float32')\n",
    "        sample_rate = f.samplerate\n",
    "    if target_sr is not None and sample_rate != target_sr:\n",
    "        samples = librosa.core.resample(samples, orig_sr=sample_rate, target_sr=target_sr)\n",
    "    return samples.transpose()\n",
    "\n",
    "\n",
    "device = spec_model.device\n",
    "gst_audio = load_wav(ref[\"audio_filepath\"], 44100)\n",
    "gst_audio = torch.from_numpy(gst_audio).unsqueeze(0).to(device)\n",
    "gst_audio_len = torch.tensor(gst_audio.shape[1], dtype=torch.long, device=device).unsqueeze(0)\n",
    "gst_ref_spec, gst_ref_spec_lens = spec_model.preprocessor(input_signal=gst_audio, length=gst_audio_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19660c26",
   "metadata": {},
   "source": [
    "To convert spectrogram to waveform we will need a **vocoder**. Let's load HiFiGAN vocoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f926ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vocoder = \"tts_en_hifitts_hifigan_ft_fastpitch\"\n",
    "vocoder = HifiGanModel.from_pretrained(pretrained_vocoder)\n",
    "vocoder.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce195746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(spec_gen_model, vocoder_model, str_input, gst_ref_spec, gst_ref_spec_lens):\n",
    "    \"\"\"\n",
    "    Synthesizes spectrogram and audio from a text string given a spectrogram synthesis and vocoder model.\n",
    "    \n",
    "    Args:\n",
    "        spec_gen_model: Spectrogram generator model (FastPitch in our case)\n",
    "        vocoder_model: Vocoder model (HiFiGAN in our case)\n",
    "        str_input: Text input for the synthesis\n",
    "        gst_ref_spec: Reference spectrogram from same speaker for GST input\n",
    "        gst_ref_spec_lens: Reference spectrogram length\n",
    "    \n",
    "    Returns:\n",
    "        spectrogram and waveform of the synthesized audio.\n",
    "    \"\"\"\n",
    "    spec_gen_model.cuda().eval()\n",
    "    with torch.no_grad():\n",
    "        parsed = spec_gen_model.parse(str_input)\n",
    "        spectrogram = spec_gen_model.generate_spectrogram(\n",
    "            tokens=parsed, \n",
    "            gst_ref_spec=gst_ref_spec,\n",
    "            gst_ref_spec_lens=gst_ref_spec_lens,\n",
    "        )\n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "        \n",
    "    if spectrogram is not None:\n",
    "        if isinstance(spectrogram, torch.Tensor):\n",
    "            spectrogram = spectrogram.to('cpu').numpy()\n",
    "        if len(spectrogram.shape) == 3:\n",
    "            spectrogram = spectrogram[0]\n",
    "    else:\n",
    "        raise Exception(\"None value was generated for spectrogram\")\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return spectrogram, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a421c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val_record in val_manifest:\n",
    "    str_input = val_record['text_normalized']\n",
    "    speaker_id = val_record['speaker']\n",
    "    _, audio_adapter = infer(spec_model, vocoder, str_input, gst_ref_spec, gst_ref_spec_lens)\n",
    "    print(f\"text: {str_input}\")\n",
    "    print(\"Original Audio\")\n",
    "    ipd.display(ipd.Audio(val_record['audio_filepath'], rate=44100))\n",
    "    print(\"Generated Audio with Adapter\")\n",
    "    ipd.display(ipd.Audio(audio_adapter, rate=44100))\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8557fe81",
   "metadata": {},
   "source": [
    "**Note:** The quality of the generated audio may not be very good because for demo purposes we trained on <5mins of data, it is recommended to use 15-30mins of new user data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1569e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
