{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77cbae37",
   "metadata": {},
   "source": [
    "# FastPitch MultiSpeaker Pretraining\n",
    "\n",
    "This notebook is designed to provide a guide on how to run FastPitch MultiSpeaker Pretraining Pipeline. It contains the following sections:\n",
    "1. **Pre-train FastPitch on multi-speaker data**: pre-train a multi-speaker FastPitch\n",
    "* Dataset Preparation: download dataset and extract manifest files.\n",
    "* Preprocessing: add absolute audio paths and normalized texts in manifest, calculate pitch stats, extract speaker embedding from pre-trained speaker-verification (SV) model.\n",
    "* Training: pre-train multispeaker FastPitch with additional four components including 1) Looked-up speaker embedding 2) Pre-trained SV speaker embedding 3) Global Style Tokens 4) Conditional Layer Normalization.\n",
    "* Transform pretrained checkpoint to adapter-compatible model: transform original checkpoint configs to adapter-compatible configs.\n",
    "2. **Fine-tune HiFiGAN on multi-speaker data**: fine-tune a vocoder for the pre-trained multi-speaker FastPitch\n",
    "* Dataset Preparation: extract mel-spectrograms from pre-trained FastPitch.\n",
    "* Training: fine-tune HiFiGAN with pre-trained multi-speaker data.\n",
    "3. **Inference**: generate speech from adpated FastPitch\n",
    "* Load Model: load pre-trained multi-speaker FastPitch.\n",
    "* Output Audio: generate audio files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b96a9af",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "> Copyright 2023 NVIDIA. All Rights Reserved.\n",
    "> \n",
    "> Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "> you may not use this file except in compliance with the License.\n",
    "> You may obtain a copy of the License at\n",
    "> \n",
    ">     http://www.apache.org/licenses/LICENSE-2.0\n",
    "> \n",
    "> Unless required by applicable law or agreed to in writing, software\n",
    "> distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "> See the License for the specific language governing permissions and\n",
    "> limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4d5d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can either run this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies# .\n",
    "\"\"\"\n",
    "BRANCH = 'main'\n",
    "# # If you're using Colab and not running locally, uncomment and run this cell.\n",
    "# !apt-get install sox libsndfile1 ffmpeg\n",
    "# !pip install wget unidecode pynini==2.1.4 scipy==1.7.3\n",
    "# !python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d0430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 44100\n",
    "# Store all python script\n",
    "codedir = 'NeMoTTS' \n",
    "# Store all manifest and audios\n",
    "datadir = 'NeMoTTS_dataset'\n",
    "# Store all related text-normalized files\n",
    "normdir = 'NeMoTTS_normalize_files'\n",
    "# Store all supplementary files\n",
    "suppdir = \"NeMoTTS_sup_data\"\n",
    "# Store all config files\n",
    "confdir = \"NeMoTTS_conf\"\n",
    "# Store all training logs\n",
    "logsdir = \"NeMoTTS_logs\"\n",
    "# Store all mel-spectrograms for vocoder training\n",
    "melsdir = \"NeMoTTS_mels\"\n",
    "# Store all generated audios\n",
    "gensdir = \"NeMoTTS_gens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e4a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login #PASTE_WANDB_APIKEY_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c3bc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nemo\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a0dcd5",
   "metadata": {},
   "source": [
    "# 1. Pre-train FastPitch on multi-speaker data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6620a91",
   "metadata": {},
   "source": [
    "## a. Dataset Preparation\n",
    "For our tutorial, we use the subset of Hi-Fi Multi-Speaker English TTS (Hi-Fi TTS) dataset with 10 speakers. The audios have 44100 kHz sampling rate. You can read more about dataset [here](https://arxiv.org/abs/2104.01497)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2342499",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {datadir} && wget https://hifitts-subset.s3.amazonaws.com/hifitts_subset.tar.gz && tar zxf hifitts_subset.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21ff292",
   "metadata": {},
   "outputs": [],
   "source": [
    "manidir = f\"{datadir}/hifitts_subset\"\n",
    "!ls {manidir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5afa0a6",
   "metadata": {},
   "source": [
    "For simplicity, we use original dev set as training set and original test set as validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fbc28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manifest = os.path.abspath(os.path.join(manidir, 'train.json'))\n",
    "valid_manifest = os.path.abspath(os.path.join(manidir, 'dev.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb09c98",
   "metadata": {},
   "source": [
    "## b. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f71c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional files\n",
    "!mkdir -p {normdir} && cd {normdir} \\\n",
    "&& wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/scripts/tts_dataset_files/cmudict-0.7b_nv22.10 \\\n",
    "&& wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/scripts/tts_dataset_files/heteronyms-052722 \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7369e8",
   "metadata": {},
   "source": [
    "### Add absoluate audio path in manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd39e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_reader(filename):\n",
    "    lines = []\n",
    "    with open(filename) as f:\n",
    "        for line in f: lines.append(json.loads(line))\n",
    "    return lines\n",
    "\n",
    "def json_writer(manifest, filename):\n",
    "    with open(filename, 'w') as fout:\n",
    "        for m in manifest: fout.write(json.dumps(m) + '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf91e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas = json_reader(train_manifest)\n",
    "for m in train_datas: m['audio_filepath'] = os.path.abspath(os.path.join(manidir, m['audio_filepath']))\n",
    "json_writer(train_datas, train_manifest)\n",
    "\n",
    "valid_datas = json_reader(valid_manifest)\n",
    "for m in valid_datas: m['audio_filepath'] = os.path.abspath(os.path.join(manidir, m['audio_filepath']))\n",
    "json_writer(valid_datas, valid_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab52830",
   "metadata": {},
   "source": [
    "### Calibrate speaker id to start from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec78ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas = json_reader(train_manifest)\n",
    "speaker2id = {s: _id for _id, s in enumerate(set([m['speaker'] for m in train_datas]))}\n",
    "for m in train_datas: m['old_speaker'], m['speaker'] = m['speaker'], speaker2id[m['speaker']]\n",
    "json_writer(train_datas, train_manifest)\n",
    "\n",
    "valid_datas = json_reader(valid_manifest)\n",
    "for m in valid_datas: m['old_speaker'], m['speaker'] = m['speaker'], speaker2id[m['speaker']]\n",
    "json_writer(valid_datas, valid_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c976f510",
   "metadata": {},
   "source": [
    "### Extract speaker embedding from pre-trained speaker-verification model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef115d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd {codedir} && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/scripts/dataset_processing/tts/add_pretrained_speaker_embedding.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0b77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd {codedir} && python add_pretrained_speaker_embedding.py --manifest-path={train_manifest} --feature-dir={os.path.abspath(suppdir)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd {codedir} && python add_pretrained_speaker_embedding.py --manifest-path={valid_manifest} --feature-dir={os.path.abspath(suppdir)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fafa0e",
   "metadata": {},
   "source": [
    "### Add normalized text in manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeeb3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd {codedir} && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/scripts/dataset_processing/tts/add_normalized_text.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ba0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd {codedir} && python add_normalized_text.py --src {train_manifest} --dst {train_manifest}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074c3a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd {codedir} && python add_normalized_text.py --src {valid_manifest} --dst {valid_manifest}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e22544",
   "metadata": {},
   "source": [
    "### Calculate Pitch Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea89e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from nemo.collections.asr.parts.preprocessing.features import WaveformFeaturizer\n",
    "from nemo.collections.tts.parts.utils.tts_dataset_utils import get_base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ee00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitch(sample):    \n",
    "    rel_audio_path = Path(sample[\"audio_filepath\"]).relative_to(base_data_dir).with_suffix(\"\")\n",
    "    rel_audio_path_as_text_id = str(rel_audio_path).replace(\"/\", \"_\")\n",
    "    pitch_filepath = os.path.join(pitch_dir, f\"{rel_audio_path_as_text_id}.pt\")\n",
    "    \n",
    "    if os.path.exists(pitch_filepath):\n",
    "        pitch = torch.load(pitch_filepath).numpy()\n",
    "\n",
    "    else:\n",
    "        features = wave_model.process(\n",
    "            sample[\"audio_filepath\"]\n",
    "        )\n",
    "        voiced_tuple = librosa.pyin(\n",
    "            features.numpy(),\n",
    "            fmin=librosa.note_to_hz('C2'),\n",
    "            fmax=librosa.note_to_hz('C7'),\n",
    "            frame_length=2048,\n",
    "            sr=44100,\n",
    "            fill_na=0.0,\n",
    "        )\n",
    "        pitch = voiced_tuple[0]\n",
    "        torch.save(torch.from_numpy(pitch).float(), pitch_filepath)\n",
    "    \n",
    "    return pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3691c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_model = WaveformFeaturizer(sample_rate=sample_rate)\n",
    "pitch_dir = os.path.join(suppdir, 'pitch')\n",
    "os.makedirs(pitch_dir, exist_ok=True)\n",
    "\n",
    "train_pitchs = []\n",
    "train_datas = json_reader(train_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in train_datas])\n",
    "for m in tqdm(train_datas): train_pitchs.append(get_pitch(m))\n",
    "    \n",
    "valid_datas = json_reader(valid_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in valid_datas])\n",
    "for m in tqdm(valid_datas): get_pitch(m)\n",
    "\n",
    "train_pitchs = np.concatenate(train_pitchs)\n",
    "pitch_mean = float(np.mean(train_pitchs))\n",
    "pitch_std = float(np.std(train_pitchs))\n",
    "\n",
    "with open(os.path.join(manidir, 'pitch_stats.json'), 'w') as f:\n",
    "    json.dump({'pitch':[pitch_mean, pitch_std]}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9e04c0",
   "metadata": {},
   "source": [
    "## c. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34c1849",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {confdir} && cd {confdir} \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/conf/fastpitch_speaker_adaptation.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8be28b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {codedir} \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/fastpitch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8610a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally 200 epochs\n",
    "\n",
    "!(python {codedir}/fastpitch.py \\\n",
    "  --config-path={os.path.abspath(confdir)} \\\n",
    "  --config-name=fastpitch_speaker_adaptation.yaml \\\n",
    "  sample_rate={sample_rate} \\\n",
    "  train_dataset={train_manifest} \\\n",
    "  validation_datasets={valid_manifest} \\\n",
    "  sup_data_types=\"['align_prior_matrix', 'pitch', 'speaker_id','reference_audio','reference_speaker_embedding']\" \\\n",
    "  sup_data_path={suppdir} \\\n",
    "  +init_from_pretrained_model=\"tts_en_fastpitch\" \\\n",
    "  pitch_mean={pitch_mean} \\\n",
    "  pitch_std={pitch_std} \\\n",
    "  phoneme_dict_path={normdir}/cmudict-0.7b_nv22.10 \\\n",
    "  heteronyms_path={normdir}/heteronyms-052722 \\\n",
    "  model.n_speakers=10 \\\n",
    "  model.speaker_emb_condition_prosody=True \\\n",
    "  model.speaker_emb_condition_decoder=True \\\n",
    "  model.speaker_emb_condition_aligner=True \\\n",
    "  model.speaker_emb_condition_layernm=True \\\n",
    "  model.speaker_embedding_dim=192 \\\n",
    "  model.train_ds.dataloader_params.batch_size=8 \\\n",
    "  model.validation_ds.dataloader_params.batch_size=8 \\\n",
    "  model.train_ds.dataloader_params.num_workers=8 \\\n",
    "  model.validation_ds.dataloader_params.num_workers=8 \\\n",
    "  model.train_ds.dataset.max_duration=20 \\\n",
    "  model.validation_ds.dataset.max_duration=20 \\\n",
    "  model.validation_ds.dataset.min_duration=0.1 \\\n",
    "  +model.text_tokenizer.add_blank_at=True \\\n",
    "  exp_manager.exp_dir={logsdir} \\\n",
    "  +exp_manager.create_wandb_logger=True \\\n",
    "  +exp_manager.wandb_logger_kwargs.name=\"tutorial-FastPitch-pretrain-multispeaker\" \\\n",
    "  +exp_manager.wandb_logger_kwargs.project=\"NeMo\" \\\n",
    "  trainer.max_epochs=5 \\\n",
    "  trainer.check_val_every_n_epoch=1 \\\n",
    "  trainer.log_every_n_steps=1 \\\n",
    "  trainer.devices=-1 \\\n",
    "  trainer.strategy=ddp \\\n",
    "  trainer.precision=32 \\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c806dac7",
   "metadata": {},
   "source": [
    "## d. Transform pretrained checkpoint to adapter-compatible model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa268368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.core import adapter_mixins\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d82301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_config_to_support_adapter(config) -> DictConfig:\n",
    "    with open_dict(config):\n",
    "        enc_adapter_metadata = adapter_mixins.get_registered_adapter(config.input_fft._target_)\n",
    "        if enc_adapter_metadata is not None:\n",
    "            config.input_fft._target_ = enc_adapter_metadata.adapter_class_path\n",
    "\n",
    "        dec_adapter_metadata = adapter_mixins.get_registered_adapter(config.output_fft._target_)\n",
    "        if dec_adapter_metadata is not None:\n",
    "            config.output_fft._target_ = dec_adapter_metadata.adapter_class_path\n",
    "\n",
    "        pitch_predictor_adapter_metadata = adapter_mixins.get_registered_adapter(config.pitch_predictor._target_)\n",
    "        if pitch_predictor_adapter_metadata is not None:\n",
    "            config.pitch_predictor._target_ = pitch_predictor_adapter_metadata.adapter_class_path\n",
    "\n",
    "        duration_predictor_adapter_metadata = adapter_mixins.get_registered_adapter(config.duration_predictor._target_)\n",
    "        if duration_predictor_adapter_metadata is not None:\n",
    "            config.duration_predictor._target_ = duration_predictor_adapter_metadata.adapter_class_path\n",
    "\n",
    "        aligner_adapter_metadata = adapter_mixins.get_registered_adapter(config.alignment_module._target_)\n",
    "        if aligner_adapter_metadata is not None:\n",
    "            config.alignment_module._target_ = aligner_adapter_metadata.adapter_class_path\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b17f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_PRETRAINED_FASTPITCH_CHECKPOINT = \"\"\n",
    "state = torch.load(YOUR_PRETRAINED_FASTPITCH_CHECKPOINT)\n",
    "state['hyper_parameters']['cfg'] = update_model_config_to_support_adapter(state['hyper_parameters']['cfg'])\n",
    "torch.save(state, YOUR_PRETRAINED_FASTPITCH_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff976aa",
   "metadata": {},
   "source": [
    "# 2. Fine-tune HiFiGAN on multi-speaker data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d13a66a",
   "metadata": {},
   "source": [
    "## a. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ec3c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.parts.utils.tts_dataset_utils import BetaBinomialInterpolator\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bbe51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_spectrogram(index, manifest, speaker_to_index, mel_dir, pitch_dir, speaker_embedding_dir, base_data_dir):\n",
    "    \n",
    "    record = manifest[index]\n",
    "    audio_file = record[\"audio_filepath\"]\n",
    "    \n",
    "    if '.wav' in audio_file:\n",
    "        save_path = os.path.abspath(os.path.join(mel_dir, audio_file.split(\"/\")[-1].replace(\".wav\", \".npy\")))\n",
    "    \n",
    "    if '.flac' in audio_file:\n",
    "        save_path = os.path.abspath(os.path.join(mel_dir, audio_file.split(\"/\")[-1].replace(\".flac\", \".npy\")))\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        return save_path\n",
    "    \n",
    "    if \"normalized_text\" in record:\n",
    "        text = spec_model.parse(record[\"normalized_text\"], normalize=False)\n",
    "    else:\n",
    "        text = spec_model.parse(record['text'])\n",
    "        \n",
    "    text_len = torch.tensor(text.shape[-1], dtype=torch.long, device=spec_model.device).unsqueeze(0)\n",
    "    \n",
    "    audio = wave_model.process(audio_file).unsqueeze(0).to(device=spec_model.device)\n",
    "    audio_len = torch.tensor(audio.shape[1]).long().unsqueeze(0).to(device=spec_model.device)\n",
    "    spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len) \n",
    "    \n",
    "    rel_audio_path = Path(audio_file).relative_to(base_data_dir).with_suffix(\"\")\n",
    "    rel_audio_path_as_text_id = str(rel_audio_path).replace(\"/\", \"_\")\n",
    "    pitch_filepath = os.path.join(pitch_dir, f\"{rel_audio_path_as_text_id}.pt\")\n",
    "    pitch = torch.load(pitch_filepath)\n",
    "    pitch -= pitch_mean\n",
    "    pitch[pitch == -pitch_mean] = 0.0\n",
    "    pitch /= pitch_std\n",
    "    pitch = pitch.unsqueeze(0).to(device=spec_model.device)\n",
    "    attn_prior = torch.from_numpy(beta_binomial_interpolator(spect_len.item(), text_len.item())).unsqueeze(0).to(spec_model.device)\n",
    "    \n",
    "    speaker = torch.tensor([record['speaker']]).to(spec_model.device)\n",
    "    \n",
    "    reference_pool = speaker_to_index[record[\"speaker\"]] - set([index]) if len(speaker_to_index[record[\"speaker\"]]) > 1 else speaker_to_index[record[\"speaker\"]]\n",
    "    reference_sample = manifest[random.sample(reference_pool, 1)[0]]\n",
    "    reference_audio = wave_model.process(reference_sample[\"audio_filepath\"]).unsqueeze(0).to(device=spec_model.device)\n",
    "    reference_audio_length = torch.tensor(reference_audio.shape[1]).long().unsqueeze(0).to(device=spec_model.device)\n",
    "    reference_spec, reference_spec_len = spec_model.preprocessor(input_signal=reference_audio, length=reference_audio_length)  \n",
    "    \n",
    "    reference_pool = speaker_to_index[record[\"speaker\"]] - set([index]) if len(speaker_to_index[record[\"speaker\"]]) > 1 else speaker_to_index[record[\"speaker\"]]\n",
    "    reference_sample = manifest[random.sample(reference_pool, 1)[0]]\n",
    "    rel_audio_path = Path(reference_sample[\"audio_filepath\"]).relative_to(base_data_dir).with_suffix(\"\")\n",
    "    rel_audio_path_as_text_id = str(rel_audio_path).replace(\"/\", \"_\")\n",
    "    speaker_emb_filepath = os.path.join(speaker_embedding_dir, f\"{rel_audio_path_as_text_id}.pt\")\n",
    "    speaker_emb = torch.load(speaker_emb_filepath).unsqueeze(0).to(spec_model.device)\n",
    "\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        spectrogram = spec_model.forward(\n",
    "          text=text, \n",
    "          input_lens=text_len,\n",
    "          pitch=pitch,\n",
    "          spec=spect, \n",
    "          mel_lens=spect_len, \n",
    "          attn_prior=attn_prior,\n",
    "          speaker=speaker,\n",
    "          reference_spec=reference_spec,\n",
    "          reference_spec_lens=reference_spec_len,\n",
    "          reference_speaker_embedding=speaker_emb,\n",
    "        )[0]\n",
    "    \n",
    "    spec = spectrogram[0].to('cpu').numpy()\n",
    "    np.save(save_path, spec)\n",
    "    return save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2757eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_model = FastPitchModel.load_from_checkpoint(YOUR_PRETRAINED_FASTPITCH_CHECKPOINT)\n",
    "spec_model.eval().cuda()\n",
    "beta_binomial_interpolator = BetaBinomialInterpolator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d0e93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(melsdir, exist_ok=True)\n",
    "speaker_embedding_dir = os.path.join(suppdir, 'reference_speaker_embedding')\n",
    "\n",
    "# Train\n",
    "train_datas = json_reader(train_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in train_datas])\n",
    "\n",
    "speaker_to_index = defaultdict(list)\n",
    "for i, d in enumerate(train_datas): speaker_to_index[d.get('speaker', None)].append(i)\n",
    "speaker_to_index = {k: set(v) for k, v in speaker_to_index.items()}\n",
    "\n",
    "for i, record in enumerate(tqdm(train_datas)):\n",
    "    record[\"mel_filepath\"] =  gen_spectrogram(i, train_datas, speaker_to_index, melsdir, pitch_dir, speaker_embedding_dir, base_data_dir)\n",
    "\n",
    "json_writer(train_datas, train_manifest)\n",
    "\n",
    "\n",
    "# Valid\n",
    "valid_datas = json_reader(valid_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in valid_datas])\n",
    "\n",
    "speaker_to_index = defaultdict(list)\n",
    "for i, d in enumerate(valid_datas): speaker_to_index[d.get('speaker', None)].append(i)\n",
    "speaker_to_index = {k: set(v) for k, v in speaker_to_index.items()}\n",
    "\n",
    "for i, record in enumerate(tqdm(valid_datas)):\n",
    "    record[\"mel_filepath\"] =  gen_spectrogram(i, valid_datas, speaker_to_index, melsdir, pitch_dir, speaker_embedding_dir, base_data_dir)\n",
    "\n",
    "json_writer(valid_datas, valid_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f22e57b",
   "metadata": {},
   "source": [
    "## b. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df12593",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {confdir} \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/conf/hifigan/hifigan_44100.yaml\n",
    "!cd {confdir} \\\n",
    "&& mkdir -p model/train_ds && cd model/train_ds && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/conf/hifigan/model/train_ds/train_ds_finetune.yaml \n",
    "!cd {confdir} \\\n",
    "&& mkdir -p model/validation_ds && cd model/validation_ds && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/conf/hifigan/model/validation_ds/val_ds_finetune.yaml\n",
    "!cd {confdir} \\\n",
    "&& mkdir -p model/generator && cd model/generator && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/conf/hifigan/model/generator/v1_44100.yaml\n",
    "!cd {codedir} \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/hifigan_finetune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d69ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally 100 epochs\n",
    "\n",
    "!(python {codedir}/hifigan_finetune.py \\\n",
    "--config-path={os.path.abspath(confdir)} \\\n",
    "--config-name=hifigan_44100.yaml \\\n",
    "train_dataset={train_manifest} \\\n",
    "validation_datasets={valid_manifest} \\\n",
    "+init_from_pretrained_model=\"tts_en_hifitts_hifigan_ft_fastpitch\" \\\n",
    "model.train_ds.dataloader_params.batch_size=32 \\\n",
    "model.optim.lr=0.0001 \\\n",
    "+trainer.max_epochs=20 \\\n",
    "trainer.check_val_every_n_epoch=20 \\\n",
    "model/train_ds=train_ds_finetune \\\n",
    "model/validation_ds=val_ds_finetune \\\n",
    "trainer.devices=-1 \\\n",
    "trainer.strategy='ddp' \\\n",
    "trainer.precision=16 \\\n",
    "exp_manager.exp_dir={logsdir} \\\n",
    "exp_manager.create_wandb_logger=True \\\n",
    "exp_manager.wandb_logger_kwargs.name=\"tutorial-HiFiGAN-finetune-multispeaker\" \\\n",
    "exp_manager.wandb_logger_kwargs.project=\"NeMo\" \\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bf5eca",
   "metadata": {},
   "source": [
    "# 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import HifiGanModel\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd6e5e5",
   "metadata": {},
   "source": [
    "## a. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d237e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_model = WaveformFeaturizer(sample_rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9287c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastPitch\n",
    "spec_model = FastPitchModel.load_from_checkpoint(YOUR_PRETRAINED_FASTPITCH_CHECKPOINT, strict=False)\n",
    "spec_model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbc9d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HiFiGAN\n",
    "YOUR_FINETUNED_HIFIGAN_ON_MULTISPEAKER_CHECKPOINT = \"\"\n",
    "vocoder_model = HifiGanModel.load_from_checkpoint(checkpoint_path=YOUR_FINETUNED_HIFIGAN_ON_MULTISPEAKER_CHECKPOINT)\n",
    "vocoder_model.eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81468fc4",
   "metadata": {},
   "source": [
    "## b. Output Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3000b4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt_spectrogram(audio_path, wave_model, spec_gen_model):\n",
    "    features = wave_model.process(audio_path, trim=False)\n",
    "    audio, audio_length = features, torch.tensor(features.shape[0]).long()\n",
    "    audio = audio.unsqueeze(0).to(device=spec_gen_model.device)\n",
    "    audio_length = audio_length.unsqueeze(0).to(device=spec_gen_model.device)\n",
    "    with torch.no_grad():\n",
    "        spectrogram, spec_len = spec_gen_model.preprocessor(input_signal=audio, length=audio_length)\n",
    "    return spectrogram, spec_len\n",
    "\n",
    "def gen_spectrogram(text, spec_gen_model, speaker, reference_spec, reference_spec_lens):\n",
    "    parsed = spec_gen_model.parse(text)\n",
    "    speaker = torch.tensor([speaker]).long().to(device=spec_gen_model.device)\n",
    "    with torch.no_grad():    \n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed, \n",
    "                                                          speaker=speaker, \n",
    "                                                          reference_spec=reference_spec, \n",
    "                                                          reference_spec_lens=reference_spec_lens)\n",
    "\n",
    "    return spectrogram\n",
    "  \n",
    "def synth_audio(vocoder_model, spectrogram):    \n",
    "    with torch.no_grad():  \n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb54dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(gensdir, exist_ok=True)\n",
    "\n",
    "# Reference Audio\n",
    "with open(train_manifest, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        reference_record = json.loads(line)\n",
    "        break\n",
    "        \n",
    "# Validatation Audio\n",
    "num_val = 10\n",
    "val_records = []\n",
    "with open(valid_manifest, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        val_records.append(json.loads(line))\n",
    "        if len(val_records) >= num_val:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(gensdir, f'manifest.json'), 'w') as f_manifest: \n",
    "    for i, val_record in enumerate(val_records):\n",
    "        f_manifest.write(json.dumps(val_record) + '\\n')\n",
    "        \n",
    "        audio = ipd.Audio(val_record['audio_filepath'], rate=sample_rate)\n",
    "        with open(os.path.join(gensdir, f'{i}-GT.wav'), 'wb') as f: f.write(audio.data)\n",
    "\n",
    "        reference_spec, reference_spec_lens = gt_spectrogram(reference_record['audio_filepath'], wave_model, spec_model)\n",
    "        reference_spec = reference_spec.to(spec_model.device)\n",
    "        spec_pred = gen_spectrogram(val_record['text'], spec_model,\n",
    "                                    speaker=val_record['speaker'], \n",
    "                                    reference_spec=reference_spec, reference_spec_lens=reference_spec_lens)\n",
    "\n",
    "        audio = synth_audio(vocoder_model, spec_pred)\n",
    "        audio = ipd.Audio(audio, rate=sample_rate)\n",
    "        with open(os.path.join(gensdir, f'{i}-Gen.wav'), 'wb') as f: f.write(audio.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0055de33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
