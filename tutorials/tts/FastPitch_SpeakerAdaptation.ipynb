{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7bf3531",
   "metadata": {},
   "source": [
    "# FastPitch SpeakerAdaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3565f10b",
   "metadata": {},
   "source": [
    "This notebook is designed to provide a guide on how to run FastPitch Speaker Adaptation Pipeline. If you are not familiar with Adapters please go through the following [tutorial](https://github.com/NVIDIA/NeMo/blob/main/tutorials/02_NeMo_Adapters.ipynb). This tutorial contains the following sections:\n",
    "\n",
    "- Fine-tune FastPitch: fine-tune pre-trained multi-speaker FastPitch for a new speaker\n",
    "- Inference: generate speech from adapted FastPitch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9571bb",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "> Copyright 2022 NVIDIA. All Rights Reserved.\n",
    "> \n",
    "> Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "> \n",
    "> http://www.apache.org/licenses/LICENSE-2.0\n",
    "> \n",
    "> Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37121c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can either run this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies# .\n",
    "\"\"\"\n",
    "BRANCH = 'main'\n",
    "# # If you're using Colab and not running locally, uncomment and run this cell.\n",
    "# !apt-get install sox libsndfile1 ffmpeg\n",
    "# !pip install wget unidecode pynini==2.1.4 scipy==1.7.3\n",
    "# !python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf02d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nemo\n",
    "import torch\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a971e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.torch.g2ps import EnglishG2p\n",
    "from nemo.collections.tts.torch.data import TTSDataset\n",
    "from nemo_text_processing.text_normalization.normalize import Normalizer\n",
    "from nemo.collections.tts.torch.tts_tokenizers import EnglishPhonemesTokenizer, EnglishCharsTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e0e2b7",
   "metadata": {},
   "source": [
    "## Fine-tune FastPitch using Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e98f71",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Download a small dataset to demonstrate speaker adaptation using adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598e3fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset download\n",
    "!wget https://nemo-public.s3.us-east-2.amazonaws.com/6097_5_mins.tar.gz  # Contains 10MB of data\n",
    "!tar -xzf 6097_5_mins.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827cf38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 ./6097_5_mins/manifest.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbedd7d",
   "metadata": {},
   "source": [
    "For speaker adaptation the manifest must contain speaker ID (`speaker` field). Our downloaded manifest does not contain this field so we will add it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2af68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_reader(filename):\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line)\n",
    "            \n",
    "\n",
    "def json_writer(file, json_objects):\n",
    "    with open(file, \"w\") as f:\n",
    "        for jsonobj in json_objects:\n",
    "            jsonstr = json.dumps(jsonobj)\n",
    "            f.write(jsonstr + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674819cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = list(json_reader('./6097_5_mins/manifest.json'))\n",
    "for m in manifest:\n",
    "    m['speaker'] = 500\n",
    "json_writer('./6097_5_mins/manifest.json', manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7434e88a",
   "metadata": {},
   "source": [
    "Split the data into train and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e233b7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./6097_5_mins/manifest.json | tail -n 2 > ./val_manifest.json\n",
    "!cat ./6097_5_mins/manifest.json | head -n -2 > ./train_manifest.json\n",
    "!ln -s ./6097_5_mins/audio audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5026bf3",
   "metadata": {},
   "source": [
    "Download all additional files for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d034747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional files\n",
    "!mkdir -p tts_dataset_files && cd tts_dataset_files \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/scripts/tts_dataset_files/cmudict-0.7b_nv22.08 \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/scripts/tts_dataset_files/heteronyms-052722 \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/nemo_text_processing/text_normalization/en/data/whitelist/lj_speech.tsv \\\n",
    "&& cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15020eea",
   "metadata": {},
   "source": [
    "We use the `examples/tts/fastpitch_finetune_adapters.py` script to finetune the adapters with the `fastpitch_speaker_adaptation.yaml` configuration. So, let's download these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfebc4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/fastpitch_finetune_adapters.py\n",
    "\n",
    "!mkdir -p conf \\\n",
    "&& cd conf \\\n",
    "&& wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/conf/fastpitch_speaker_adaptation.yaml \\\n",
    "&& cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be671a0",
   "metadata": {},
   "source": [
    "### Generate Supplementary Data\n",
    "\n",
    "It is recommended to precompute supplementary data like `pitch` and `alignment_matrix` before training.\n",
    "Let's define the normalizer and tokenizer for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68fc9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text normalizer\n",
    "text_normalizer = Normalizer(\n",
    "    lang=\"en\", \n",
    "    input_case=\"cased\", \n",
    "    whitelist=\"tts_dataset_files/lj_speech.tsv\"\n",
    ")\n",
    "\n",
    "text_normalizer_call_kwargs = {\n",
    "    \"punct_pre_process\": True,\n",
    "    \"punct_post_process\": True\n",
    "}\n",
    "\n",
    "# Text tokenizer\n",
    "# Grapheme-to-phoneme module\n",
    "g2p = EnglishG2p(\n",
    "    phoneme_dict=\"tts_dataset_files/cmudict-0.7b_nv22.08\",\n",
    "    heteronyms=\"tts_dataset_files/heteronyms-052722\"\n",
    ")\n",
    "\n",
    "# Text tokenizer\n",
    "text_tokenizer = EnglishPhonemesTokenizer(\n",
    "    punct=True,\n",
    "    stresses=True,\n",
    "    chars=True,\n",
    "    apostrophe=True,\n",
    "    pad_with_space=True,\n",
    "    g2p=g2p,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6438e467",
   "metadata": {},
   "source": [
    "Defining a method that would compute and save the supplementary data. This method would return pitch statistics for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c5b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_calculate_supplementary_data(manifest_dir, sup_data_path, sup_data_types, text_tokenizer, text_normalizer, \n",
    "                                     text_normalizer_call_kwargs, \n",
    "                                     batch_size=32, \n",
    "                                     sample_rate=22050):\n",
    "    # init train and val dataloaders\n",
    "    stages = [\"train\", \"val\"]\n",
    "    stage2dl = {}\n",
    "    for stage in stages:\n",
    "        ds = TTSDataset(\n",
    "            manifest_filepath=os.path.join(manifest_dir, f\"{stage}_manifest.json\"),\n",
    "            sample_rate=sample_rate,\n",
    "            sup_data_path=sup_data_path,\n",
    "            sup_data_types=sup_data_types,\n",
    "            n_fft=1024,\n",
    "            win_length=1024,\n",
    "            hop_length=256,\n",
    "            window=\"hann\",\n",
    "            n_mels=80,\n",
    "            lowfreq=0,\n",
    "            highfreq=8000,\n",
    "            text_tokenizer=text_tokenizer,\n",
    "            text_normalizer=text_normalizer,\n",
    "            text_normalizer_call_kwargs=text_normalizer_call_kwargs\n",
    "\n",
    "        ) \n",
    "        stage2dl[stage] = torch.utils.data.DataLoader(ds, batch_size=batch_size, collate_fn=ds._collate_fn, num_workers=12, pin_memory=True)\n",
    "    \n",
    "    # iteration over dataloaders\n",
    "    pitch_mean, pitch_std, pitch_min, pitch_max = None, None, None, None\n",
    "    for stage, dl in stage2dl.items():\n",
    "        pitch_list = []\n",
    "        for batch in tqdm(dl, total=len(dl)):\n",
    "            tokens, tokens_lengths, audios, audio_lengths, attn_prior, pitches, pitches_lengths = batch\n",
    "            pitch_list.append(pitches[pitches != 0])\n",
    "            \n",
    "        if stage == \"train\":\n",
    "            pitch_tensor = torch.cat(pitch_list)\n",
    "            pitch_mean, pitch_std = pitch_tensor.mean().item(), pitch_tensor.std().item()\n",
    "            pitch_min, pitch_max = pitch_tensor.min().item(), pitch_tensor.max().item()\n",
    "            \n",
    "    return pitch_mean, pitch_std, pitch_min, pitch_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07defea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastpitch_sup_data_path = \"fastpitch_sup_data_folder\"\n",
    "manifest_dir = \"./\"\n",
    "sup_data_types = [\"align_prior_matrix\", \"pitch\"]\n",
    "\n",
    "pitch_mean, pitch_std, pitch_min, pitch_max = pre_calculate_supplementary_data(\n",
    "    manifest_dir, fastpitch_sup_data_path, sup_data_types, text_tokenizer, text_normalizer, text_normalizer_call_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35330749",
   "metadata": {},
   "source": [
    "### Training Adapters\n",
    "\n",
    "Now we are ready for training the adapters! Let's try to adapt new speakers using adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f77f9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !(python fastpitch_finetune_adapters.py \\\n",
    "#     --config-name=fastpitch_speaker_adaptation.yaml \\\n",
    "\n",
    "!(python /workspace/NeMo/examples/tts/fastpitch_finetune_adapters.py \\\n",
    "    --config-path=/workspace/NeMo/examples/tts/conf \\\n",
    "    --config-name=fastpitch_speaker_adaptation.yaml \\\n",
    "    sample_rate=44100 \\\n",
    "    train_dataset=./train_manifest.json \\\n",
    "    validation_datasets=./val_manifest.json \\\n",
    "    sup_data_types=\"['align_prior_matrix', 'pitch', 'speaker_id']\" \\\n",
    "    sup_data_path={fastpitch_sup_data_path} \\\n",
    "    +init_from_pretrained_model=\"tts_en_fastpitch_multispeaker\" \\\n",
    "    pitch_mean={pitch_mean} \\\n",
    "    pitch_std={pitch_std} \\\n",
    "    pitch_fmin={pitch_min} \\\n",
    "    pitch_fmax={pitch_max} \\\n",
    "    model.n_speakers=12800 \\\n",
    "    phoneme_dict_path=tts_dataset_files/cmudict-0.7b_nv22.08 \\\n",
    "    heteronyms_path=tts_dataset_files/heteronyms-052722 \\\n",
    "    whitelist_path=tts_dataset_files/lj_speech.tsv \\\n",
    "    model.train_ds.dataloader_params.batch_size=24 \\\n",
    "    model.validation_ds.dataloader_params.batch_size=24 \\\n",
    "    model.train_ds.dataloader_params.num_workers=8 \\\n",
    "    model.validation_ds.dataloader_params.num_workers=8 \\\n",
    "    model.optim.lr=2e-4 \\\n",
    "    ~model.optim.sched \\\n",
    "    model.optim.name=adamw \\\n",
    "    model.optim.weight_decay=0.0 \\\n",
    "    +model.text_tokenizer.add_blank_at=True \\\n",
    "    trainer.check_val_every_n_epoch=10 \\\n",
    "    trainer.max_epochs=100 \\\n",
    "    trainer.log_every_n_steps=1 \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.precision=32 \\\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b893d6",
   "metadata": {},
   "source": [
    "Let's look at some of the options in the training command:\n",
    "\n",
    "`+init_from_pretrained_model` : FastPitch will load this pretrained checkpoint, and add adapters on top of this model.\n",
    "\n",
    "`model.n_speakers` : In the above pretrained model, there are already 12800 number of speaker embeddings, hence this parameter needs to match that number for loading the weights correctly.\n",
    "\n",
    "`~model.optim.sched` : Since this is a finetuning activity for only a few steps, we do not need a scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b3f92",
   "metadata": {},
   "source": [
    "Let's look at some fields in `fastpitch_speaker_adaptation.yaml`\n",
    "\n",
    "`adapter` : This is a new field inside `model` struct, this struct contains all adapter related configs.\n",
    "\n",
    "`adapter_name` : Name of the adapter.\n",
    "\n",
    "`adapter_module_name` : Names of modules where adapter has to be added/enabled.\n",
    "\n",
    "`adapter_state_dict_name` : Name of the state dict to be saved with adapter weights only.\n",
    "\n",
    "`add_random_speaker` : Since adapters are used to add new speakers, this flag tells if a random speaker embedding should be used to initialize the new speaker's embedding or some other pretrained speaker embedding needs to be used as initial value of the new speaker's embedding.\n",
    "\n",
    "Look at `nemo.collections.common.parts.adapter_modules` to see the type of adapters available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a1cb49",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Inference using already trained adapter can be done using the following steps:\n",
    "\n",
    "- Update the config of the pretrained model to support adapters. This step replaces the module, where adapters need to be used, with the registered adapter class for that particular module.\n",
    "\n",
    "- Load the pretrained model.\n",
    "\n",
    "- Load the adapter weights using `load_adapters` method.\n",
    "\n",
    "- Generate spectrogram using this model and convert it to spectrogram using a vocoder.\n",
    "\n",
    "Let's go through these steps one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0d76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import FastPitchModel, HifiGanModel\n",
    "from nemo.core import adapter_mixins\n",
    "from omegaconf import DictConfig, open_dict\n",
    "\n",
    "def update_model_config_to_support_adapter(config) -> DictConfig:\n",
    "    with open_dict(config):\n",
    "        enc_adapter_metadata = adapter_mixins.get_registered_adapter(config.input_fft._target_)\n",
    "        if enc_adapter_metadata is not None:\n",
    "            config.input_fft._target_ = enc_adapter_metadata.adapter_class_path\n",
    "\n",
    "        dec_adapter_metadata = adapter_mixins.get_registered_adapter(config.output_fft._target_)\n",
    "        if dec_adapter_metadata is not None:\n",
    "            config.output_fft._target_ = dec_adapter_metadata.adapter_class_path\n",
    "\n",
    "        pitch_predictor_adapter_metadata = adapter_mixins.get_registered_adapter(config.pitch_predictor._target_)\n",
    "        if pitch_predictor_adapter_metadata is not None:\n",
    "            config.pitch_predictor._target_ = pitch_predictor_adapter_metadata.adapter_class_path\n",
    "\n",
    "        duration_predictor_adapter_metadata = adapter_mixins.get_registered_adapter(config.duration_predictor._target_)\n",
    "        if duration_predictor_adapter_metadata is not None:\n",
    "            config.duration_predictor._target_ = duration_predictor_adapter_metadata.adapter_class_path\n",
    "\n",
    "        aligner_adapter_metadata = adapter_mixins.get_registered_adapter(config.alignment_module._target_)\n",
    "        if aligner_adapter_metadata is not None:\n",
    "            config.alignment_module._target_ = aligner_adapter_metadata.adapter_class_path\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe6f94",
   "metadata": {},
   "source": [
    "Let's prepare config file to use adapter modules in FastPitch instead of just the FastPitch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a34f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = \"tts_en_fastpitch_multispeaker\"\n",
    "\n",
    "# Get the model config from the pretrained checkpoint. \n",
    "model_cfg = FastPitchModel.from_pretrained(pretrained_model, return_config=True)\n",
    "# Update the config to support adapters.\n",
    "model_cfg = update_model_config_to_support_adapter(model_cfg)\n",
    "# Use the updated config to load the pretrained checkpoint. This would allow loading the pretrained \\\n",
    "# checkpoint along with adapters.\n",
    "spec_gen_model = FastPitchModel.from_pretrained(pretrained_model, override_config_path=model_cfg)\n",
    "spec_gen_model.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad0ccc",
   "metadata": {},
   "source": [
    "Load adapter checkpoint. The checkpoint produced in the above training was saved in the folder `./nemo_experiments/FastPitch/{version}/checkpoints/adapters` where version is based on the timestamp during training.\n",
    "\n",
    "After your training of adapters is done, check the folder `./nemo_experiments/FastPitch/`, you will find a folder with a timestamp as it's name. This folder name is the `version`, replace the version in the below cell with the version that your experiment generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dac610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace `version` with the version in your experiment\n",
    "version = \"2022-10-12_19-29-21\"\n",
    "adapter_checkpoint_path = f\"nemo_experiments/FastPitch/{version}/checkpoints/adapters.pt\"\n",
    "spec_gen_model.load_adapters(adapter_checkpoint_path, name=None, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60920dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_gen_model.freeze()\n",
    "spec_gen_model.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0a74b9",
   "metadata": {},
   "source": [
    "To convert spectrogram to waveform we will need a vocoder. Let's load HiFiGAN vocoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f368b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_vocoder = \"tts_en_hifitts_hifigan_ft_fastpitch\"\n",
    "vocoder = HifiGanModel.from_pretrained(pretrained_vocoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172ec840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(spec_gen_model, vocoder_model, str_input, speaker=None):\n",
    "    \"\"\"\n",
    "    Synthesizes spectrogram and audio from a text string given a spectrogram synthesis and vocoder model.\n",
    "    \n",
    "    Args:\n",
    "        spec_gen_model: Spectrogram generator model (FastPitch in our case)\n",
    "        vocoder_model: Vocoder model (HiFiGAN in our case)\n",
    "        str_input: Text input for the synthesis\n",
    "        speaker: Speaker ID\n",
    "    \n",
    "    Returns:\n",
    "        spectrogram and waveform of the synthesized audio.\n",
    "    \"\"\"\n",
    "    spec_gen_model.cuda().eval()\n",
    "    with torch.no_grad():\n",
    "        parsed = spec_gen_model.parse(str_input)\n",
    "        if speaker is not None:\n",
    "            speaker = torch.tensor([speaker]).long().to(device=spec_gen_model.device)\n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed, speaker=speaker)\n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "        \n",
    "    if spectrogram is not None:\n",
    "        if isinstance(spectrogram, torch.Tensor):\n",
    "            spectrogram = spectrogram.to('cpu').numpy()\n",
    "        if len(spectrogram.shape) == 3:\n",
    "            spectrogram = spectrogram[0]\n",
    "    else:\n",
    "        raise Exception(\"None value was generated for spectrogram\")\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return spectrogram, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764995ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load validation manifest for inference\n",
    "val_manifest = list(json_reader('./val_manifest.json'))\n",
    "for val_record in val_manifest:\n",
    "    str_input = val_record['text_normalized']\n",
    "    speaker_id = val_record['speaker']\n",
    "    _, audio_adapter = infer(spec_gen_model, vocoder, str_input, speaker=speaker_id)\n",
    "    \n",
    "    print(\"Original Audio\")\n",
    "    ipd.display(ipd.Audio(val_record['audio_filepath'], rate=44100))\n",
    "    print(\"Generated Audio with Adapter\")\n",
    "    ipd.display(ipd.Audio(audio_adapter, rate=44100))\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a38daeb",
   "metadata": {},
   "source": [
    "**Note:** The quality of the generated audio may not be very good because for demo purposes we trained on <5mins of data, it is recommended to use 15-30mins of new user data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6749069e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
