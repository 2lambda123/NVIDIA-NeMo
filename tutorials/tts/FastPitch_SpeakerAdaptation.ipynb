{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "107c396f",
   "metadata": {},
   "source": [
    "# FastPitch SpeakerAdaptation\n",
    "\n",
    "This notebook is designed to provide a guide on how to run FastPitch Speaker Adaptation Pipeline. It contains the following sections:\n",
    "1. **Pre-train FastPitch on multi-speaker data**: pre-train a multi-speaker FastPitch\n",
    "* Dataset Preparation: download dataset and extract manifest files.\n",
    "* Preprocessing: add absolute audio paths and normalized texts in manifest, calculate pitch stats, extract speaker embedding from pre-trained speaker-verification (SV) model.\n",
    "* Training: pre-train multispeaker FastPitch with additional four components including 1) Looked-up speaker embedding 2) Pre-trained SV speaker embedding 3) Global Style Tokens 4) Conditional Layer Normalization.\n",
    "* Transform pretrained checkpoint to adapter-compatible model: transform original checkpoint configs to adapter-compatible configs.\n",
    "2. **Fine-tune HiFiGAN on multi-speaker data**: fine-tune a vocoder for the pre-trained multi-speaker FastPitch\n",
    "* Dataset Preparation: extract mel-spectrograms from pre-trained FastPitch.\n",
    "* Training: fine-tune HiFiGAN with pre-trained multi-speaker data.\n",
    "3. **Fine-tune FastPitch on adaptation data**: fine-tune pre-trained multi-speaker FastPitch for a new speaker\n",
    "* Dataset Preparation: download dataset and extract manifest files. (duration more than 15 mins)\n",
    "* Preprocessing: add absolute audio paths and normalized texts in manifest, calculate pitch stats, extract speaker embedding from pre-trained speaker-verification (SV) model.\n",
    "* Training: fine-tune frozen multispeaker FastPitch with trainable adapters and weighted looked-up speaker embedding.\n",
    "4. **Fine-tune HiFiGAN on adaptation data**: fine-tune a vocoder for the fine-tuned multi-speaker FastPitch\n",
    "* Dataset Preparation: extract mel-spectrograms from fine-tuned FastPitch.\n",
    "* Training: fine-tune HiFiGAN with fine-tuned adaptation data.\n",
    "5. **Inference**: generate speech from adpated FastPitch\n",
    "* Load Model: load pre-trained multi-speaker FastPitch with fine-tuned adapters and weighted looked-up speaker embedding.\n",
    "* Output Audio: generate audio files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6c04a",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "> Copyright 2023 NVIDIA. All Rights Reserved.\n",
    "> \n",
    "> Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "> you may not use this file except in compliance with the License.\n",
    "> You may obtain a copy of the License at\n",
    "> \n",
    ">     http://www.apache.org/licenses/LICENSE-2.0\n",
    "> \n",
    "> Unless required by applicable law or agreed to in writing, software\n",
    "> distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "> See the License for the specific language governing permissions and\n",
    "> limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can either run this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies# .\n",
    "\"\"\"\n",
    "BRANCH = 'main'\n",
    "# # If you're using Colab and not running locally, uncomment and run this cell.\n",
    "# !apt-get install sox libsndfile1 ffmpeg\n",
    "# !pip install wget unidecode pynini==2.1.4 scipy==1.7.3\n",
    "# !python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1017d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all python script\n",
    "codedir = 'NeMoTTS' \n",
    "# Store all manifest and audios\n",
    "datadir = 'NeMoTTS_dataset'\n",
    "# Store all related text-normalized files\n",
    "normdir = 'NeMoTTS_normalize_files'\n",
    "# Store all supplementary files\n",
    "suppdir = \"NeMoTTS_sup_data\"\n",
    "# Store all config files\n",
    "confdir = \"NeMoTTS_conf\"\n",
    "# Store all training logs\n",
    "logsdir = \"NeMoTTS_logs\"\n",
    "# Store all mel-spectrograms for vocoder training\n",
    "melsdir = \"NeMoTTS_mels\"\n",
    "# Store all generated audios\n",
    "gensdir = \"NeMoTTS_gens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b20759",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login #paste_wandb_apikey_here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae4f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nemo\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f28d556",
   "metadata": {},
   "source": [
    "# 1. Pre-train FastPitch on multi-speaker data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8c06db",
   "metadata": {},
   "source": [
    "## a. Dataset Preparation\n",
    "For our tutorial, we use Hi-Fi Multi-Speaker English TTS (Hi-Fi TTS) dataset with 10 speakers. The audios have 44100 kHz sampling rate. You can read more about dataset [here](https://arxiv.org/abs/2104.01497)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60131ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir {codedir} && cd {codedir} && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/scripts/dataset_processing/tts/hifitts/get_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6970120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(mkdir {datadir} && \\\n",
    "  cd {codedir} && \\\n",
    "  python get_data.py \\\n",
    "        --data-root ../{datadir}/ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f7227",
   "metadata": {},
   "outputs": [],
   "source": [
    "manidir = f\"{datadir}/hi_fi_tts_v0\"\n",
    "!ls {manidir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2d0f0b",
   "metadata": {},
   "source": [
    "For simplicity, we use original dev set as training set and original test set as validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f403706",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manifest = os.path.abspath(os.path.join(manidir, 'dev_manifest.json'))\n",
    "valid_manifest = os.path.abspath(os.path.join(manidir, 'test_manifest.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a40855",
   "metadata": {},
   "source": [
    "## b. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4646cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional files\n",
    "!mkdir -p {normdir} && cd {normdir} \\\n",
    "&& wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/scripts/tts_dataset_files/cmudict-0.7b_nv22.10 \\\n",
    "&& wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/scripts/tts_dataset_files/heteronyms-052722 \\\n",
    "&& wget "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49fcc47",
   "metadata": {},
   "source": [
    "### Add absoluate audio path in manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d298e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_reader(filename):\n",
    "    lines = []\n",
    "    with open(filename) as f:\n",
    "        for line in f: lines.append(json.loads(line))\n",
    "    return lines\n",
    "\n",
    "def json_writer(manifest, filename):\n",
    "    with open(filename, 'w') as fout:\n",
    "        for m in manifest: fout.write(json.dumps(m) + '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061cc8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas = json_reader(train_manifest)\n",
    "for m in train_datas: m['audio_filepath'] = os.path.abspath(os.path.join(manidir, m['audio_filepath']))\n",
    "json_writer(train_datas, train_manifest)\n",
    "\n",
    "valid_datas = json_reader(valid_manifest)\n",
    "for m in valid_datas: m['audio_filepath'] = os.path.abspath(os.path.join(manidir, m['audio_filepath']))\n",
    "json_writer(valid_datas, valid_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168447c7",
   "metadata": {},
   "source": [
    "### Add normalized text in manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b85ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd {codedir} && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/scripts/dataset_processing/add_normalized_text.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd76f7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd {codedir} && python add_normalized_text.py --src {train_manifest} --dst {train_manifest}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438cc1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd {codedir} && python add_normalized_text.py --src {valid_manifest} --dst {valid_manifest}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89219bb",
   "metadata": {},
   "source": [
    "### Calculate Pitch Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from pathlib import Path\n",
    "from nemo.collections.asr.parts.preprocessing.features import WaveformFeaturizer\n",
    "from nemo.collections.tts.parts.utils.tts_dataset_utils import get_base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa01e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitch(sample):    \n",
    "    rel_audio_path = Path(sample[\"audio_filepath\"]).relative_to(base_data_dir).with_suffix(\"\")\n",
    "    rel_audio_path_as_text_id = str(rel_audio_path).replace(\"/\", \"_\")\n",
    "    pitch_filepath = os.path.join(pitch_dir, f\"{rel_audio_path_as_text_id}.pt\")\n",
    "    \n",
    "    if os.path.exists(pitch_filepath):\n",
    "        pitch = torch.load(pitch_filepath).numpy()\n",
    "\n",
    "    else:\n",
    "        features = wave_model.process(\n",
    "            sample[\"audio_filepath\"]\n",
    "        )\n",
    "        voiced_tuple = librosa.pyin(\n",
    "            features.numpy(),\n",
    "            fmin=librosa.note_to_hz('C2'),\n",
    "            fmax=librosa.note_to_hz('C7'),\n",
    "            frame_length=2048,\n",
    "            sr=44100,\n",
    "            fill_na=0.0,\n",
    "        )\n",
    "        pitch = voiced_tuple[0]\n",
    "        torch.save(torch.from_numpy(pitch).float(), pitch_filepath)\n",
    "    \n",
    "    return pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe0d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_model = WaveformFeaturizer(sample_rate=44100)\n",
    "pitch_dir = os.path.join(suppdir, 'pitch')\n",
    "os.makedirs(pitch_dir, exist_ok=True)\n",
    "\n",
    "train_pitchs = []\n",
    "train_datas = json_reader(train_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in train_datas])\n",
    "for m in tqdm(train_datas): train_pitchs.append(get_pitch(m))\n",
    "    \n",
    "valid_datas = json_reader(valid_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in valid_datas])\n",
    "for m in tqdm(valid_datas): get_pitch(m)\n",
    "\n",
    "train_pitchs = np.concatenate(train_pitchs)\n",
    "pitch_mean = float(np.mean(train_pitchs))\n",
    "pitch_std = float(np.std(train_pitchs))\n",
    "\n",
    "with open(os.path.join(manidir, 'pitch_stats.json'), 'w') as f:\n",
    "    json.dump({'pitch':[pitch_mean, pitch_std]}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273b8e99",
   "metadata": {},
   "source": [
    "### Extract speaker embedding from pre-trained speaker-verification model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94045cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_model = nemo_asr.models.EncDecSpeakerLabelModel.from_pretrained('titanet_large')\n",
    "verification_model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6bb07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas = json_reader(train_manifest)\n",
    "speaker2id = {s: _id for _id, s in enumerate(set([m['speaker'] for m in train_datas]))}\n",
    "\n",
    "for m in train_datas: \n",
    "    old_id, new_id = m['speaker'], speaker2id[m['speaker']]\n",
    "    m['old_speaker'] = old_id\n",
    "    m['speaker'] = new_id\n",
    "    m['label'] = new_id\n",
    "    \n",
    "json_writer(train_datas, train_manifest)\n",
    "\n",
    "valid_datas = json_reader(valid_manifest)\n",
    "for m in valid_datas: \n",
    "    old_id, new_id = m['speaker'], speaker2id[m['speaker']]\n",
    "    m['old_speaker'] = old_id\n",
    "    m['speaker'] = new_id\n",
    "    m['label'] = new_id\n",
    "    \n",
    "json_writer(valid_datas, valid_manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bea985",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = os.path.join(suppdir, 'speaker_embedding')\n",
    "os.makedirs(embedding_path, exist_ok=True)\n",
    "\n",
    "# Train\n",
    "embs, *_ = verification_model.batch_inference(manifest_filepath=train_manifest,\n",
    "                                              batch_size=64, \n",
    "                                              sample_rate=16000, \n",
    "                                              device='cuda')\n",
    "np.save(os.path.join(embedding_path, 'train.npy'), embs)\n",
    "\n",
    "# Valid\n",
    "embs, *_ = verification_model.batch_inference(manifest_filepath=valid_manifest,\n",
    "                                              batch_size=64, \n",
    "                                              sample_rate=16000, \n",
    "                                              device='cuda')\n",
    "np.save(os.path.join(embedding_path, 'valid.npy'), embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59155ee2",
   "metadata": {},
   "source": [
    "## c. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7f9647",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {confdir} && cd {confdir} \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/conf/fastpitch_speaker_adaptation.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf28638",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {codedir} \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/fastpitch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9883d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally 200 epochs\n",
    "\n",
    "!(python {codedir}/fastpitch.py \\\n",
    "  --config-path={os.path.abspath(confdir)} \\\n",
    "  --config-name=fastpitch_speaker_adaptation.yaml \\\n",
    "  sample_rate=44100 \\\n",
    "  train_dataset={train_manifest} \\\n",
    "  validation_datasets={valid_manifest} \\\n",
    "  sup_data_types=\"['align_prior_matrix', 'pitch', 'speaker_id','gst_ref_audio','speaker_embedding']\" \\\n",
    "  sup_data_path={suppdir} \\\n",
    "  +init_from_pretrained_model=\"tts_en_fastpitch\" \\\n",
    "  pitch_mean={pitch_mean} \\\n",
    "  pitch_std={pitch_std} \\\n",
    "  phoneme_dict_path={normdir}/cmudict-0.7b_nv22.10 \\\n",
    "  heteronyms_path={normdir}/heteronyms-052722 \\\n",
    "  model.n_speakers=10 \\\n",
    "  model.speaker_emb_condition_prosody=True \\\n",
    "  model.speaker_emb_condition_decoder=True \\\n",
    "  model.speaker_emb_condition_aligner=True \\\n",
    "  model.speaker_emb_condition_layernm=True \\\n",
    "  model.speaker_embedding_dim=192 \\\n",
    "  model.train_ds.dataloader_params.batch_size=8 \\\n",
    "  model.validation_ds.dataloader_params.batch_size=8 \\\n",
    "  model.train_ds.dataloader_params.num_workers=8 \\\n",
    "  model.validation_ds.dataloader_params.num_workers=8 \\\n",
    "  model.train_ds.dataset.max_duration=20 \\\n",
    "  model.validation_ds.dataset.max_duration=20 \\\n",
    "  model.validation_ds.dataset.min_duration=0.1 \\\n",
    "  +model.text_tokenizer.add_blank_at=True \\\n",
    "  +model.train_ds.dataset.speaker_embedding_path={suppdir}/speaker_embedding/train.npy \\\n",
    "  +model.validation_ds.dataset.speaker_embedding_path={suppdir}/speaker_embedding/valid.npy \\\n",
    "  exp_manager.exp_dir={logsdir} \\\n",
    "  +exp_manager.create_wandb_logger=True \\\n",
    "  +exp_manager.wandb_logger_kwargs.name=\"tutorial-FastPitch-pretrain-multispeaker\" \\\n",
    "  +exp_manager.wandb_logger_kwargs.project=\"NeMo\" \\\n",
    "  trainer.check_val_every_n_epoch=1 \\\n",
    "  trainer.log_every_n_steps=1 \\\n",
    "  trainer.max_epochs=5 \\\n",
    "  trainer.devices=-1 \\\n",
    "  trainer.strategy=ddp \\\n",
    "  trainer.precision=32 \\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d553515",
   "metadata": {},
   "source": [
    "## d. Transform pretrained checkpoint to adapter-compatible model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04461ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.core import adapter_mixins\n",
    "from nemo.collections.tts.modules.submodules import WeightedSpeakerEmbedding\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f2d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model_config_to_support_adapter(config) -> DictConfig:\n",
    "    with open_dict(config):\n",
    "        enc_adapter_metadata = adapter_mixins.get_registered_adapter(config.input_fft._target_)\n",
    "        if enc_adapter_metadata is not None:\n",
    "            config.input_fft._target_ = enc_adapter_metadata.adapter_class_path\n",
    "\n",
    "        dec_adapter_metadata = adapter_mixins.get_registered_adapter(config.output_fft._target_)\n",
    "        if dec_adapter_metadata is not None:\n",
    "            config.output_fft._target_ = dec_adapter_metadata.adapter_class_path\n",
    "\n",
    "        pitch_predictor_adapter_metadata = adapter_mixins.get_registered_adapter(config.pitch_predictor._target_)\n",
    "        if pitch_predictor_adapter_metadata is not None:\n",
    "            config.pitch_predictor._target_ = pitch_predictor_adapter_metadata.adapter_class_path\n",
    "\n",
    "        duration_predictor_adapter_metadata = adapter_mixins.get_registered_adapter(config.duration_predictor._target_)\n",
    "        if duration_predictor_adapter_metadata is not None:\n",
    "            config.duration_predictor._target_ = duration_predictor_adapter_metadata.adapter_class_path\n",
    "\n",
    "        aligner_adapter_metadata = adapter_mixins.get_registered_adapter(config.alignment_module._target_)\n",
    "        if aligner_adapter_metadata is not None:\n",
    "            config.alignment_module._target_ = aligner_adapter_metadata.adapter_class_path\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecc5c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = \"YOUR_PRETRAINED_FASTPITCH_CHECKPOINT\"\n",
    "state = torch.load(ckpt)\n",
    "state['hyper_parameters']['cfg'] = update_model_config_to_support_adapter(state['hyper_parameters']['cfg'])\n",
    "torch.save(state, ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5150aeba",
   "metadata": {},
   "source": [
    "# 2. Fine-tune HiFiGAN on multi-speaker data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ff2ea",
   "metadata": {},
   "source": [
    "## a. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591085ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.parts.utils.tts_dataset_utils import BetaBinomialInterpolator\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f308c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_spectrogram(index, manifest, speaker_embedding, speaker_to_index, mel_dir, pitch_dir, base_data_dir):\n",
    "    \n",
    "    record = manifest[index]\n",
    "    audio_file = record[\"audio_filepath\"]\n",
    "    \n",
    "    if '.wav' in audio_file:\n",
    "        save_path = os.path.abspath(os.path.join(mel_dir, audio_file.split(\"/\")[-1].replace(\".wav\", \".npy\")))\n",
    "    \n",
    "    if '.flac' in audio_file:\n",
    "        save_path = os.path.abspath(os.path.join(mel_dir, audio_file.split(\"/\")[-1].replace(\".flac\", \".npy\")))\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        return save_path\n",
    "    \n",
    "    if \"normalized_text\" in record:\n",
    "        text = spec_model.parse(record[\"normalized_text\"], normalize=False)\n",
    "    else:\n",
    "        text = spec_model.parse(record['text'])\n",
    "        \n",
    "    text_len = torch.tensor(text.shape[-1], dtype=torch.long, device=spec_model.device).unsqueeze(0)\n",
    "    \n",
    "    audio = wave_model.process(audio_file).unsqueeze(0).to(device=spec_model.device)\n",
    "    audio_len = torch.tensor(audio.shape[1]).long().unsqueeze(0).to(device=spec_model.device)\n",
    "    spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len) \n",
    "    \n",
    "    rel_audio_path = Path(audio_file).relative_to(base_data_dir).with_suffix(\"\")\n",
    "    rel_audio_path_as_text_id = str(rel_audio_path).replace(\"/\", \"_\")\n",
    "    pitch_filepath = os.path.join(pitch_dir, f\"{rel_audio_path_as_text_id}.pt\")\n",
    "    pitch = torch.load(pitch_filepath)\n",
    "    pitch -= pitch_mean\n",
    "    pitch[pitch == -pitch_mean] = 0.0\n",
    "    pitch /= pitch_std\n",
    "    pitch = pitch.unsqueeze(0).to(device=spec_model.device)\n",
    "    attn_prior = torch.from_numpy(beta_binomial_interpolator(spect_len.item(), text_len.item())).unsqueeze(0).to(spec_model.device)\n",
    "    \n",
    "    speaker = torch.tensor([record['speaker']]).to(spec_model.device)\n",
    "    \n",
    "    ref_pool = speaker_to_index[record[\"speaker\"]] - set([index]) if len(speaker_to_index[record[\"speaker\"]]) > 1 else speaker_to_index[record[\"speaker\"]]\n",
    "    ref_sample = manifest[random.sample(ref_pool, 1)[0]]\n",
    "    gst_ref_audio = wave_model.process(ref_sample[\"audio_filepath\"]).unsqueeze(0).to(device=spec_model.device)\n",
    "    gst_ref_audio_length = torch.tensor(gst_ref_audio.shape[1]).long().unsqueeze(0).to(device=spec_model.device)\n",
    "    gst_ref_spec, gst_ref_spec_len = spec_model.preprocessor(input_signal=gst_ref_audio, length=gst_ref_audio_length)  \n",
    "    \n",
    "    ref_pool = speaker_to_index[record[\"speaker\"]] - set([index]) if len(speaker_to_index[record[\"speaker\"]]) > 1 else speaker_to_index[record[\"speaker\"]]\n",
    "    ref_sample_index = random.sample(ref_pool, 1)[0]\n",
    "    speaker_emb = torch.from_numpy(speaker_embedding[ref_sample_index]).unsqueeze(0).to(spec_model.device)\n",
    "\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        spectrogram = spec_model.forward(\n",
    "          text=text, \n",
    "          input_lens=text_len,\n",
    "          pitch=pitch,\n",
    "          spec=spect, \n",
    "          mel_lens=spect_len, \n",
    "          attn_prior=attn_prior,\n",
    "          speaker=speaker,\n",
    "          gst_ref_spec=gst_ref_spec,\n",
    "          gst_ref_spec_lens=gst_ref_spec_len,\n",
    "          speaker_embedding=speaker_emb,\n",
    "        )[0]\n",
    "    \n",
    "    spec = spectrogram[0].to('cpu').numpy()\n",
    "    np.save(save_path, spec)\n",
    "    return save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fdcce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_model = FastPitchModel.load_from_checkpoint(\"YOUR_PRETRAINED_FASTPITCH_CHECKPOINT\")\n",
    "spec_model.eval().cuda()\n",
    "wave_model = WaveformFeaturizer(sample_rate=44100)\n",
    "beta_binomial_interpolator = BetaBinomialInterpolator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22d681",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(melsdir, exist_ok=True)\n",
    "\n",
    "# Train\n",
    "train_datas = json_reader(train_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in train_datas])\n",
    "train_speaker_embedding = np.load(f'{suppdir}/speaker_embedding/train.npy')\n",
    "\n",
    "speaker_to_index = defaultdict(list)\n",
    "for i, d in enumerate(train_datas): speaker_to_index[d.get('speaker', None)].append(i)\n",
    "speaker_to_index = {k: set(v) for k, v in speaker_to_index.items()}\n",
    "\n",
    "for i, record in enumerate(tqdm(train_datas)):\n",
    "    record[\"mel_filepath\"] =  gen_spectrogram(i, train_datas, train_speaker_embedding, speaker_to_index, melsdir, pitch_dir, base_data_dir)\n",
    "\n",
    "json_writer(train_datas, train_manifest)\n",
    "\n",
    "\n",
    "# Valid\n",
    "valid_datas = json_reader(valid_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in valid_datas])\n",
    "valid_speaker_embedding = np.load(f'{suppdir}/speaker_embedding/valid.npy')\n",
    "\n",
    "speaker_to_index = defaultdict(list)\n",
    "for i, d in enumerate(valid_datas): speaker_to_index[d.get('speaker', None)].append(i)\n",
    "speaker_to_index = {k: set(v) for k, v in speaker_to_index.items()}\n",
    "\n",
    "for i, record in enumerate(tqdm(valid_datas)):\n",
    "    record[\"mel_filepath\"] =  gen_spectrogram(i, valid_datas, valid_speaker_embedding, speaker_to_index, melsdir, pitch_dir, base_data_dir)\n",
    "\n",
    "json_writer(valid_datas, valid_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7f859c",
   "metadata": {},
   "source": [
    "## b. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22037f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {confdir} \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/conf/hifigan/hifigan_44100.yaml\n",
    "!cd {confdir} \\\n",
    "&& mkdir -p model/train_ds && cd model/train_ds && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/conf/hifigan/model/train_ds/train_ds_finetune.yaml \n",
    "!cd {confdir} \\\n",
    "&& mkdir -p model/validation_ds && cd model/validation_ds && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/conf/hifigan/model/validation_ds/val_ds_finetune.yaml\n",
    "!cd {confdir} \\\n",
    "&& mkdir -p model/generator && cd model/generator && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/conf/hifigan/model/generator/v1_44100.yaml\n",
    "!cd {codedir} \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/hifigan_finetune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9684875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally 100 epochs\n",
    "\n",
    "!(python {codedir}/hifigan_finetune.py \\\n",
    "--config-path={os.path.abspath(confdir)} \\\n",
    "--config-name=hifigan_44100.yaml \\\n",
    "train_dataset={train_manifest} \\\n",
    "validation_datasets={valid_manifest} \\\n",
    "+init_from_pretrained_model=\"tts_en_hifitts_hifigan_ft_fastpitch\" \\\n",
    "model.train_ds.dataloader_params.batch_size=32 \\\n",
    "model.optim.lr=0.0001 \\\n",
    "+trainer.max_epochs=100 \\\n",
    "trainer.check_val_every_n_epoch=20 \\\n",
    "model/train_ds=train_ds_finetune \\\n",
    "model/validation_ds=val_ds_finetune \\\n",
    "trainer.devices=-1 \\\n",
    "trainer.strategy='ddp' \\\n",
    "trainer.precision=16 \\\n",
    "exp_manager.exp_dir={logsdir} \\\n",
    "exp_manager.create_wandb_logger=True \\\n",
    "exp_manager.wandb_logger_kwargs.name=\"tutorial-HiFiGAN-finetune-multispeaker\" \\\n",
    "exp_manager.wandb_logger_kwargs.project=\"NeMo\" \\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132b83b8",
   "metadata": {},
   "source": [
    "# 3. Fine-tune FastPitch on adaptation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35df6a4d",
   "metadata": {},
   "source": [
    "## a. Data Preparation\n",
    "For our tutorial, we use a new target speaker from LJSpeech dataset. Usually, the audios should have total duration more than 15 mintues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57771148",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd {codedir} \\\n",
    "&& wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/scripts/dataset_processing/tts/ljspeech/get_data.py -O get_data_ljspeech.py \\\n",
    "&& wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/scripts/dataset_processing/tts/ljspeech/lj_speech.tsv \\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc93ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd {codedir} && python get_data_ljspeech.py \\\n",
    "        --data-root ../{datadir}/ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592ffc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "manidir = f\"{datadir}/LJSpeech-1.1\"\n",
    "!ls {manidir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd45a8",
   "metadata": {},
   "source": [
    "For simplicity, we use original dev set as training set and original test set as validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270fa946",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manifest = os.path.abspath(os.path.join(manidir, 'val_manifest.json'))\n",
    "valid_manifest = os.path.abspath(os.path.join(manidir, 'test_manifest.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226bdbbf",
   "metadata": {},
   "source": [
    "## b. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c013400",
   "metadata": {},
   "source": [
    "### Add absolute file path in manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81621201",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas = json_reader(train_manifest)\n",
    "for m in train_datas: m['audio_filepath'] = os.path.abspath(m['audio_filepath'])\n",
    "json_writer(train_datas, train_manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a3308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datas = json_reader(valid_manifest)\n",
    "for m in valid_datas: m['audio_filepath'] = os.path.abspath(m['audio_filepath'])\n",
    "json_writer(valid_datas, valid_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6363f0f",
   "metadata": {},
   "source": [
    "### Calculate Pitch Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25575f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_model = WaveformFeaturizer(sample_rate=44100)\n",
    "pitch_dir = os.path.join(suppdir, 'pitch')\n",
    "os.makedirs(pitch_dir, exist_ok=True)\n",
    "\n",
    "train_pitchs = []\n",
    "train_datas = json_reader(train_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in train_datas])\n",
    "for m in tqdm(train_datas): train_pitchs.append(get_pitch(m))\n",
    "    \n",
    "valid_datas = json_reader(valid_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in valid_datas])\n",
    "for m in tqdm(valid_datas): get_pitch(m)\n",
    "\n",
    "train_pitchs = np.concatenate(train_pitchs)\n",
    "pitch_mean = float(np.mean(train_pitchs))\n",
    "pitch_std = float(np.std(train_pitchs))\n",
    "\n",
    "with open(os.path.join(manidir, 'pitch_stats.json'), 'w') as f:\n",
    "    json.dump({'pitch':[pitch_mean, pitch_std]}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80f5f28",
   "metadata": {},
   "source": [
    "### Extract speaker embedding from pre-trained speaker-verification model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f0b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas = json_reader(train_manifest)\n",
    "for m in train_datas: \n",
    "    if 'speaker' in m:\n",
    "        m['old_speaker'] = m['speaker']\n",
    "    m['speaker'] = 0\n",
    "    m['label'] = 0\n",
    "    \n",
    "json_writer(train_datas, train_manifest)\n",
    "\n",
    "valid_datas = json_reader(valid_manifest)\n",
    "for m in valid_datas: \n",
    "    if 'speaker' in m:\n",
    "        m['old_speaker'] = m['speaker']\n",
    "    m['speaker'] = 0\n",
    "    m['label'] = 0\n",
    "    \n",
    "json_writer(valid_datas, valid_manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f341cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = os.path.join(suppdir, 'speaker_embedding_ft')\n",
    "os.makedirs(embedding_path, exist_ok=True)\n",
    "\n",
    "# Train\n",
    "embs, *_ = verification_model.batch_inference(manifest_filepath=train_manifest,\n",
    "                                              batch_size=32, \n",
    "                                              sample_rate=16000, \n",
    "                                              device='cuda')\n",
    "np.save(os.path.join(embedding_path, 'train.npy'), embs)\n",
    "\n",
    "# Valid\n",
    "embs, *_ = verification_model.batch_inference(manifest_filepath=valid_manifest,\n",
    "                                              batch_size=32, \n",
    "                                              sample_rate=16000, \n",
    "                                              device='cuda')\n",
    "np.save(os.path.join(embedding_path, 'valid.npy'), embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444ef79e",
   "metadata": {},
   "source": [
    "## c. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b27aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {codedir} \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/fastpitch_finetune_adapters.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e830701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally 100 epochs\n",
    "\n",
    "!(python {codedir}/fastpitch_finetune_adapters.py \\\n",
    "--config-path={os.path.abspath(confdir)} \\\n",
    "--config-name=fastpitch_speaker_adaptation.yaml \\\n",
    "sample_rate=44100 \\\n",
    "train_dataset={train_manifest} \\\n",
    "validation_datasets={valid_manifest} \\\n",
    "sup_data_types=\"['align_prior_matrix', 'pitch', 'speaker_id', 'gst_ref_audio','speaker_embedding']\" \\\n",
    "sup_data_path={suppdir} \\\n",
    "+init_from_ptl_ckpt=\"YOUR_PRETRAINED_FASTPITCH_CHECKPOINT\" \\\n",
    "pitch_mean={pitch_mean} \\\n",
    "pitch_std={pitch_std} \\\n",
    "model.n_speakers=118 \\\n",
    "model.speaker_emb_condition_prosody=True \\\n",
    "model.speaker_emb_condition_decoder=True \\\n",
    "model.speaker_emb_condition_aligner=True \\\n",
    "model.speaker_emb_condition_layernm=True \\\n",
    "model.speaker_embedding_dim=192 \\\n",
    "model.adapter.add_weight_speaker=True \\\n",
    "+model.text_tokenizer.add_blank_at=True \\\n",
    "phoneme_dict_path={normdir}/cmudict-0.7b_nv22.10 \\\n",
    "heteronyms_path={normdir}/heteronyms-052722 \\\n",
    "model.train_ds.dataloader_params.batch_size=8 \\\n",
    "model.validation_ds.dataloader_params.batch_size=8 \\\n",
    "model.train_ds.dataloader_params.num_workers=8 \\\n",
    "model.validation_ds.dataloader_params.num_workers=8 \\\n",
    "+model.train_ds.dataset.speaker_embedding_path={suppdir}/speaker_embedding_ft/train.npy \\\n",
    "+model.validation_ds.dataset.speaker_embedding_path={suppdir}/speaker_embedding_ft/valid.npy \\\n",
    "model.optim.lr=2e-4 \\\n",
    "~model.optim.sched \\\n",
    "model.optim.name=adam \\\n",
    "model.optim.weight_decay=0.0 \\\n",
    "trainer.check_val_every_n_epoch=100 \\\n",
    "trainer.max_epochs=100 \\\n",
    "trainer.log_every_n_steps=1 \\\n",
    "trainer.devices=1 \\\n",
    "trainer.strategy=ddp \\\n",
    "trainer.precision=32 \\\n",
    "exp_manager.exp_dir={logsdir} \\\n",
    "+exp_manager.create_wandb_logger=True \\\n",
    "+exp_manager.wandb_logger_kwargs.name=\"tutorial-FastPitch-finetune-adaptation\" \\\n",
    "+exp_manager.wandb_logger_kwargs.project=\"NeMo\" \\\n",
    "+exp_manager.checkpoint_callback_params.save_top_k=-1 \\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a8b69",
   "metadata": {},
   "source": [
    "# 4. Fine-tune HiFiGAN on adaptation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6a74b6",
   "metadata": {},
   "source": [
    "## a. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ca8906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained FastPitch Weights\n",
    "spec_model = FastPitchModel.load_from_checkpoint(\"YOUR_PRETRAINED_FASTPITCH_CHECKPOINT\", strict=False)\n",
    "\n",
    "YOUR_FINETUNED_FASTPITCH_CHECKPOINT = \"YOUR_FINETUNED_FASTPITCH_CHECKPOINT\"\n",
    "\n",
    "# Load Adapter Weights\n",
    "spec_model.load_adapters(f'{logsdir}/FastPitch/{YOUR_FINETUNED_FASTPITCH_CHECKPOINT}/checkpoints/adapters.pt')\n",
    "spec_model.freeze()\n",
    "spec_model.unfreeze_enabled_adapters()\n",
    "\n",
    "# Load Weighted Speaker Embedding Weights\n",
    "wemb = WeightedSpeakerEmbedding(pretrained_embedding=spec_model.fastpitch.speaker_emb)\n",
    "wemb.embedding_weight.data = torch.load(f'{logsdir}/FastPitch/{YOUR_FINETUNED_FASTPITCH_CHECKPOINT}/checkpoints/wemb.pt')\n",
    "wemb.embedding_weight.requires_grad = False\n",
    "spec_model.fastpitch.speaker_emb = wemb\n",
    "spec_model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559ba325",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_model = WaveformFeaturizer(sample_rate=44100)\n",
    "beta_binomial_interpolator = BetaBinomialInterpolator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54d4d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(melsdir, exist_ok=True)\n",
    "\n",
    "# Train\n",
    "train_datas = json_reader(train_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in train_datas])\n",
    "train_speaker_embedding = np.load(f'{suppdir}/speaker_embedding_ft/train.npy')\n",
    "speaker_to_index = defaultdict(list)\n",
    "for i, d in enumerate(train_datas): speaker_to_index[d.get('speaker', None)].append(i)\n",
    "speaker_to_index = {k: set(v) for k, v in speaker_to_index.items()}\n",
    "\n",
    "for i, record in enumerate(tqdm(train_datas)):\n",
    "    record[\"mel_filepath\"] =  gen_spectrogram(i, train_datas, train_speaker_embedding, speaker_to_index, melsdir, pitch_dir, base_data_dir)\n",
    "\n",
    "json_writer(train_datas, train_manifest)\n",
    "\n",
    "\n",
    "# Valid\n",
    "valid_datas = json_reader(valid_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in valid_datas])\n",
    "valid_speaker_embedding = np.load(f'{suppdir}/speaker_embedding_ft/valid.npy')\n",
    "speaker_to_index = defaultdict(list)\n",
    "for i, d in enumerate(valid_datas): speaker_to_index[d.get('speaker', None)].append(i)\n",
    "speaker_to_index = {k: set(v) for k, v in speaker_to_index.items()}\n",
    "\n",
    "for i, record in enumerate(tqdm(valid_datas)):\n",
    "    record[\"mel_filepath\"] =  gen_spectrogram(i, valid_datas, valid_speaker_embedding, speaker_to_index, melsdir, pitch_dir, base_data_dir)\n",
    "\n",
    "json_writer(valid_datas, valid_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f7d3f7",
   "metadata": {},
   "source": [
    "## b. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845223e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally 500 epochs \n",
    "!(python {codedir}/hifigan_finetune.py \\\n",
    "--config-path={os.path.abspath(confdir)} \\\n",
    "--config-name=hifigan_44100.yaml \\\n",
    "train_dataset={train_manifest} \\\n",
    "validation_datasets={valid_manifest} \\\n",
    "+init_from_ptl_ckpt=\"YOUR_FINETUNED_HIFIGAN_ON_MULTISPEAKER_CHECKPOINT\" \\\n",
    "model.train_ds.dataloader_params.batch_size=32 \\\n",
    "model.optim.lr=0.0001 \\\n",
    "+trainer.max_epochs=500 \\\n",
    "trainer.check_val_every_n_epoch=20 \\\n",
    "model/train_ds=train_ds_finetune \\\n",
    "model/validation_ds=val_ds_finetune \\\n",
    "trainer.devices=1 \\\n",
    "trainer.strategy='ddp' \\\n",
    "trainer.precision=16 \\\n",
    "exp_manager.exp_dir={logsdir} \\\n",
    "exp_manager.create_wandb_logger=True \\\n",
    "exp_manager.wandb_logger_kwargs.name=\"tutorial-HiFiGAN\" \\\n",
    "exp_manager.wandb_logger_kwargs.project=\"NeMo\" \\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea65a0",
   "metadata": {},
   "source": [
    "# 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d43da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import HifiGanModel\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f395a1b",
   "metadata": {},
   "source": [
    "## a. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f060cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waveform\n",
    "sample_rate = 44100\n",
    "wave_model = WaveformFeaturizer(sample_rate=sample_rate)\n",
    "\n",
    "# FastPitch\n",
    "spec_model = FastPitchModel.load_from_checkpoint(last_ckpt, strict=False)\n",
    "spec_model.load_adapters(f'{logsdir}/FastPitch/{YOUR_FINETUNED_FASTPITCH_CHECKPOINT}/checkpoints/adapters.pt')\n",
    "spec_model.freeze()\n",
    "spec_model.unfreeze_enabled_adapters()\n",
    "wemb = WeightedSpeakerEmbedding(pretrained_embedding=spec_model.fastpitch.speaker_emb)\n",
    "wemb.embedding_weight.data = torch.load(f'{logsdir}/FastPitch/{YOUR_FINETUNED_FASTPITCH_CHECKPOINT}/checkpoints/wemb.pt')\n",
    "wemb.embedding_weight.requires_grad = False\n",
    "spec_model.fastpitch.speaker_emb = wemb\n",
    "spec_model.eval().cuda()\n",
    "\n",
    "# HiFiGAN\n",
    "vocoder_model = HifiGanModel.load_from_checkpoint(checkpoint_path=f'YOUR_FINETUNED_HIFIGAN_ON_ADAPTATION_CHECKPOINT')\n",
    "vocoder_model.eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3820d784",
   "metadata": {},
   "source": [
    "## b. Output Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cc3b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt_spectrogram(audio_path, wave_model, spec_gen_model):\n",
    "    features = wave_model.process(audio_path, trim=False)\n",
    "    audio, audio_length = features, torch.tensor(features.shape[0]).long()\n",
    "    audio = audio.unsqueeze(0).to(device=spec_gen_model.device)\n",
    "    audio_length = audio_length.unsqueeze(0).to(device=spec_gen_model.device)\n",
    "    with torch.no_grad():\n",
    "        spectrogram, spec_len = spec_gen_model.preprocessor(input_signal=audio, length=audio_length)\n",
    "    return spectrogram, spec_len\n",
    "\n",
    "def gen_spectrogram(text, spec_gen_model, speaker, spec_ref, spec_ref_lens, speaker_embedding):\n",
    "    parsed = spec_gen_model.parse(text)\n",
    "    speaker = torch.tensor([speaker]).long().to(device=spec_gen_model.device)\n",
    "    with torch.no_grad():    \n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed, \n",
    "                                                          speaker=speaker, \n",
    "                                                          speaker_embedding=speaker_embedding,\n",
    "                                                          gst_ref_spec=spec_ref, \n",
    "                                                          gst_ref_spec_lens=spec_ref_lens)\n",
    "\n",
    "    return spectrogram\n",
    "  \n",
    "def sync_audio(vocoder_model, spectrogram):    \n",
    "    with torch.no_grad():  \n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b24dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(gensdir, exist_ok=True)\n",
    "\n",
    "# Reference Audio\n",
    "with open(train_manifest, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        ref_record = json.loads(line)\n",
    "        break\n",
    "        \n",
    "# Validatation Audio\n",
    "num_val = 10\n",
    "val_records = []\n",
    "with open(valid_manifest, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        val_records.append(json.loads(line))\n",
    "        if len(val_records) >= num_val:\n",
    "            break\n",
    "            \n",
    "# Speaker Embedding\n",
    "speaker_embeddings = np.load(f'{suppdir}/speaker_embedding_ft/train.npy')\n",
    "speaker_embedding  = np.mean(speaker_embeddings, axis=0)\n",
    "speaker_embedding  = torch.from_numpy(speaker_embedding).unsqueeze(0).to(spec_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a6eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, val_record in enumerate(val_records):\n",
    "    with open(os.path.join(gensdir, f'{i}-text.txt'), 'w') as f: f.write(val_record['text'])\n",
    "\n",
    "    audio = ipd.Audio(val_record['audio_filepath'], rate=sample_rate)\n",
    "    with open(os.path.join(gensdir, f'{i}-GT.wav'), 'wb') as f: f.write(audio.data)\n",
    "\n",
    "    spec_ref, spec_ref_len = gt_spectrogram(ref_record['audio_filepath'], wave_model, spec_model)\n",
    "    spec_ref = spec_ref.to(spec_model.device)\n",
    "    spec_pred = gen_spectrogram(val_record['text'], spec_model,\n",
    "                                speaker=val_record['speaker'], \n",
    "                                speaker_embedding=speaker_embedding,\n",
    "                                spec_ref=spec_ref, spec_ref_lens=spec_ref_len)\n",
    "    \n",
    "    audio = sync_audio(vocoder_model, spec_pred)\n",
    "    audio = ipd.Audio(audio, rate=sample_rate)\n",
    "    with open(os.path.join(gensdir, f'{i}-Gen.wav'), 'wb') as f: f.write(audio.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b88ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
