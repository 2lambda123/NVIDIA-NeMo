{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b78928",
   "metadata": {},
   "source": [
    "# FastPitch Adapter Finetuning\n",
    "\n",
    "This notebook is designed to provide a guide on how to run FastPitch Adapter Finetuning Pipeline. It contains the following sections:\n",
    "1. **Fine-tune FastPitch on adaptation data**: fine-tune pre-trained multi-speaker FastPitch for a new speaker\n",
    "* Dataset Preparation: download dataset and extract manifest files. (duration more than 15 mins)\n",
    "* Preprocessing: add absolute audio paths and normalized texts in manifest, calculate pitch stats, extract speaker embedding from pre-trained speaker-verification (SV) model.\n",
    "* Training: fine-tune frozen multispeaker FastPitch with trainable adapters and weighted looked-up speaker embedding.\n",
    "2. **Fine-tune HiFiGAN on adaptation data**: fine-tune a vocoder for the fine-tuned multi-speaker FastPitch\n",
    "* Dataset Preparation: extract mel-spectrograms from fine-tuned FastPitch.\n",
    "* Training: fine-tune HiFiGAN with fine-tuned adaptation data.\n",
    "3. **Inference**: generate speech from adpated FastPitch\n",
    "* Load Model: load pre-trained multi-speaker FastPitch with fine-tuned adapters and weighted looked-up speaker embedding.\n",
    "* Output Audio: generate audio files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dc8eb4",
   "metadata": {},
   "source": [
    "# License\n",
    "\n",
    "> Copyright 2023 NVIDIA. All Rights Reserved.\n",
    "> \n",
    "> Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "> you may not use this file except in compliance with the License.\n",
    "> You may obtain a copy of the License at\n",
    "> \n",
    ">     http://www.apache.org/licenses/LICENSE-2.0\n",
    "> \n",
    "> Unless required by applicable law or agreed to in writing, software\n",
    "> distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "> See the License for the specific language governing permissions and\n",
    "> limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbd6690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can either run this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies# .\n",
    "\"\"\"\n",
    "BRANCH = 'main'\n",
    "# # If you're using Colab and not running locally, uncomment and run this cell.\n",
    "# !apt-get install sox libsndfile1 ffmpeg\n",
    "# !pip install wget unidecode pynini==2.1.4 scipy==1.7.3\n",
    "# !python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc54ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 44100\n",
    "# Store all python script\n",
    "codedir = 'NeMoTTS' \n",
    "# Store all manifest and audios\n",
    "datadir = 'NeMoTTS_dataset'\n",
    "# Store all related text-normalized files\n",
    "normdir = 'NeMoTTS_normalize_files'\n",
    "# Store all supplementary files\n",
    "suppdir = \"NeMoTTS_sup_data\"\n",
    "# Store all config files\n",
    "confdir = \"NeMoTTS_conf\"\n",
    "# Store all training logs\n",
    "logsdir = \"NeMoTTS_logs\"\n",
    "# Store all mel-spectrograms for vocoder training\n",
    "melsdir = \"NeMoTTS_mels\"\n",
    "# Store all generated audios\n",
    "gensdir = \"NeMoTTS_gens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c83c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_PRETRAINED_FASTPITCH_CHECKPOINT = \"\"\n",
    "YOUR_FINETUNED_HIFIGAN_ON_MULTISPEAKER_CHECKPOINT = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a381425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login #PASTE_WANDB_APIKEY_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bc865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nemo\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd9340",
   "metadata": {},
   "source": [
    "# 1. Fine-tune FastPitch on adaptation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fc5f3d",
   "metadata": {},
   "source": [
    "## a. Data Preparation\n",
    "For our tutorial, we use small part of VCTK dataset with a new target speaker (p267). Usually, the audios should have total duration more than 15 mintues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58db8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {datadir} && wget https://vctk-subset.s3.amazonaws.com/vctk_subset.tar.gz && tar zxf vctk_subset.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacb2433",
   "metadata": {},
   "outputs": [],
   "source": [
    "manidir = f\"{datadir}/vctk_subset\"\n",
    "!ls {manidir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628e8347",
   "metadata": {},
   "source": [
    "For simplicity, we use original dev set as training set and original test set as validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a474d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_manifest = os.path.abspath(os.path.join(manidir, 'train.json'))\n",
    "valid_manifest = os.path.abspath(os.path.join(manidir, 'dev.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0064712d",
   "metadata": {},
   "source": [
    "## b. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ed88b",
   "metadata": {},
   "source": [
    "### Add absolute file path in manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25390731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_reader(filename):\n",
    "    lines = []\n",
    "    with open(filename) as f:\n",
    "        for line in f: lines.append(json.loads(line))\n",
    "    return lines\n",
    "\n",
    "def json_writer(manifest, filename):\n",
    "    with open(filename, 'w') as fout:\n",
    "        for m in manifest: fout.write(json.dumps(m) + '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f367beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas = json_reader(train_manifest)\n",
    "for m in train_datas: m['audio_filepath'] = os.path.abspath(os.path.join(manidir, m['audio_filepath']))\n",
    "json_writer(train_datas, train_manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde8f034",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datas = json_reader(valid_manifest)\n",
    "for m in valid_datas: m['audio_filepath'] = os.path.abspath(os.path.join(manidir, m['audio_filepath']))\n",
    "json_writer(valid_datas, valid_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e745736",
   "metadata": {},
   "source": [
    "### Calibrate speaker id to start from 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d108dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datas = json_reader(train_manifest)\n",
    "for m in train_datas: m['old_speaker'], m['speaker'] = m['speaker'], 0\n",
    "json_writer(train_datas, train_manifest)\n",
    "\n",
    "valid_datas = json_reader(valid_manifest)\n",
    "for m in valid_datas: m['old_speaker'], m['speaker'] = m['speaker'], 0\n",
    "json_writer(valid_datas, valid_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f53777d",
   "metadata": {},
   "source": [
    "### Extract speaker embedding from pre-trained speaker-verification model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1b73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd {codedir} && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/scripts/dataset_processing/tts/add_pretrained_speaker_embedding.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99770e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd {codedir} && python add_pretrained_speaker_embedding.py --manifest-path={train_manifest} --feature-dir={os.path.abspath(suppdir)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892ae4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd {codedir} && python add_pretrained_speaker_embedding.py --manifest-path={valid_manifest} --feature-dir={os.path.abspath(suppdir)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72094343",
   "metadata": {},
   "source": [
    "### Add normalized text in manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df27f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd {codedir} && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/scripts/dataset_processing/add_normalized_text.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64406f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd {codedir} && python add_normalized_text.py --src {train_manifest} --dst {train_manifest}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f05128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!(cd {codedir} && python add_normalized_text.py --src {valid_manifest} --dst {valid_manifest}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2910ab0c",
   "metadata": {},
   "source": [
    "### Calculate Pitch Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f2e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from nemo.collections.asr.parts.preprocessing.features import WaveformFeaturizer\n",
    "from nemo.collections.tts.parts.utils.tts_dataset_utils import get_base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e154af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitch(sample):    \n",
    "    rel_audio_path = Path(sample[\"audio_filepath\"]).relative_to(base_data_dir).with_suffix(\"\")\n",
    "    rel_audio_path_as_text_id = str(rel_audio_path).replace(\"/\", \"_\")\n",
    "    pitch_filepath = os.path.join(pitch_dir, f\"{rel_audio_path_as_text_id}.pt\")\n",
    "    \n",
    "    if os.path.exists(pitch_filepath):\n",
    "        pitch = torch.load(pitch_filepath).numpy()\n",
    "\n",
    "    else:\n",
    "        features = wave_model.process(\n",
    "            sample[\"audio_filepath\"]\n",
    "        )\n",
    "        voiced_tuple = librosa.pyin(\n",
    "            features.numpy(),\n",
    "            fmin=librosa.note_to_hz('C2'),\n",
    "            fmax=librosa.note_to_hz('C7'),\n",
    "            frame_length=2048,\n",
    "            sr=44100,\n",
    "            fill_na=0.0,\n",
    "        )\n",
    "        pitch = voiced_tuple[0]\n",
    "        torch.save(torch.from_numpy(pitch).float(), pitch_filepath)\n",
    "    \n",
    "    return pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe00f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_model = WaveformFeaturizer(sample_rate=sample_rate)\n",
    "pitch_dir = os.path.join(suppdir, 'pitch')\n",
    "os.makedirs(pitch_dir, exist_ok=True)\n",
    "\n",
    "train_pitchs = []\n",
    "train_datas = json_reader(train_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in train_datas])\n",
    "for m in tqdm(train_datas): train_pitchs.append(get_pitch(m))\n",
    "    \n",
    "valid_datas = json_reader(valid_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in valid_datas])\n",
    "for m in tqdm(valid_datas): get_pitch(m)\n",
    "\n",
    "train_pitchs = np.concatenate(train_pitchs)\n",
    "pitch_mean = float(np.mean(train_pitchs))\n",
    "pitch_std = float(np.std(train_pitchs))\n",
    "\n",
    "with open(os.path.join(manidir, 'pitch_stats.json'), 'w') as f:\n",
    "    json.dump({'pitch':[pitch_mean, pitch_std]}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d70e1c4",
   "metadata": {},
   "source": [
    "## c. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fadc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {codedir} \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/fastpitch_finetune_adapters.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d265445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally 100 epochs\n",
    "\n",
    "!(python {codedir}/fastpitch_finetune_adapters.py \\\n",
    "--config-path={os.path.abspath(confdir)} \\\n",
    "--config-name=fastpitch_speaker_adaptation.yaml \\\n",
    "sample_rate=44100 \\\n",
    "train_dataset={train_manifest} \\\n",
    "validation_datasets={valid_manifest} \\\n",
    "sup_data_types=\"['align_prior_matrix', 'pitch', 'speaker_id', 'reference_audio']\" \\\n",
    "sup_data_path={suppdir} \\\n",
    "+init_from_ptl_ckpt={YOUR_PRETRAINED_FASTPITCH_CHECKPOINT} \\\n",
    "pitch_mean={pitch_mean} \\\n",
    "pitch_std={pitch_std} \\\n",
    "model.n_speakers=10 \\\n",
    "model.speaker_emb_condition_prosody=True \\\n",
    "model.speaker_emb_condition_decoder=True \\\n",
    "model.speaker_emb_condition_aligner=True \\\n",
    "model.speaker_emb_condition_layernm=True \\\n",
    "model.speaker_embedding_dim=192 \\\n",
    "model.adapter.add_weight_speaker=True \\\n",
    "+model.text_tokenizer.add_blank_at=True \\\n",
    "phoneme_dict_path={normdir}/cmudict-0.7b_nv22.10 \\\n",
    "heteronyms_path={normdir}/heteronyms-052722 \\\n",
    "model.train_ds.dataloader_params.batch_size=8 \\\n",
    "model.validation_ds.dataloader_params.batch_size=8 \\\n",
    "model.train_ds.dataloader_params.num_workers=8 \\\n",
    "model.validation_ds.dataloader_params.num_workers=8 \\\n",
    "model.optim.lr=2e-4 \\\n",
    "~model.optim.sched \\\n",
    "model.optim.name=adam \\\n",
    "model.optim.weight_decay=0.0 \\\n",
    "exp_manager.exp_dir={logsdir} \\\n",
    "+exp_manager.create_wandb_logger=True \\\n",
    "+exp_manager.wandb_logger_kwargs.name=\"tutorial-FastPitch-finetune-adaptation\" \\\n",
    "+exp_manager.wandb_logger_kwargs.project=\"NeMo\" \\\n",
    "+exp_manager.checkpoint_callback_params.save_top_k=-1 \\\n",
    "trainer.max_epochs=10 \\\n",
    "trainer.check_val_every_n_epoch=10 \\\n",
    "trainer.log_every_n_steps=1 \\\n",
    "trainer.devices=1 \\\n",
    "trainer.strategy=ddp \\\n",
    "trainer.precision=32 \\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9dbca5",
   "metadata": {},
   "source": [
    "# 4. Fine-tune HiFiGAN on adaptation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef460ca",
   "metadata": {},
   "source": [
    "## a. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93219b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.parts.utils.tts_dataset_utils import BetaBinomialInterpolator\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from nemo.collections.tts.modules.submodules import WeightedSpeakerEmbedding\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ad8dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_spectrogram(index, manifest, speaker_to_index, mel_dir, pitch_dir, base_data_dir):\n",
    "    \n",
    "    record = manifest[index]\n",
    "    audio_file = record[\"audio_filepath\"]\n",
    "    \n",
    "    if '.wav' in audio_file:\n",
    "        save_path = os.path.abspath(os.path.join(mel_dir, audio_file.split(\"/\")[-1].replace(\".wav\", \".npy\")))\n",
    "    \n",
    "    if '.flac' in audio_file:\n",
    "        save_path = os.path.abspath(os.path.join(mel_dir, audio_file.split(\"/\")[-1].replace(\".flac\", \".npy\")))\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        return save_path\n",
    "    \n",
    "    if \"normalized_text\" in record:\n",
    "        text = spec_model.parse(record[\"normalized_text\"], normalize=False)\n",
    "    else:\n",
    "        text = spec_model.parse(record['text'])\n",
    "        \n",
    "    text_len = torch.tensor(text.shape[-1], dtype=torch.long, device=spec_model.device).unsqueeze(0)\n",
    "    \n",
    "    audio = wave_model.process(audio_file).unsqueeze(0).to(device=spec_model.device)\n",
    "    audio_len = torch.tensor(audio.shape[1]).long().unsqueeze(0).to(device=spec_model.device)\n",
    "    spect, spect_len = spec_model.preprocessor(input_signal=audio, length=audio_len) \n",
    "    \n",
    "    rel_audio_path = Path(audio_file).relative_to(base_data_dir).with_suffix(\"\")\n",
    "    rel_audio_path_as_text_id = str(rel_audio_path).replace(\"/\", \"_\")\n",
    "    pitch_filepath = os.path.join(pitch_dir, f\"{rel_audio_path_as_text_id}.pt\")\n",
    "    pitch = torch.load(pitch_filepath)\n",
    "    pitch -= pitch_mean\n",
    "    pitch[pitch == -pitch_mean] = 0.0\n",
    "    pitch /= pitch_std\n",
    "    pitch = pitch.unsqueeze(0).to(device=spec_model.device)\n",
    "    attn_prior = torch.from_numpy(beta_binomial_interpolator(spect_len.item(), text_len.item())).unsqueeze(0).to(spec_model.device)\n",
    "    \n",
    "    speaker = torch.tensor([record['speaker']]).to(spec_model.device)\n",
    "    \n",
    "    reference_pool = speaker_to_index[record[\"speaker\"]] - set([index]) if len(speaker_to_index[record[\"speaker\"]]) > 1 else speaker_to_index[record[\"speaker\"]]\n",
    "    reference_sample = manifest[random.sample(reference_pool, 1)[0]]\n",
    "    reference_audio = wave_model.process(reference_sample[\"audio_filepath\"]).unsqueeze(0).to(device=spec_model.device)\n",
    "    reference_audio_length = torch.tensor(reference_audio.shape[1]).long().unsqueeze(0).to(device=spec_model.device)\n",
    "    reference_spec, reference_spec_len = spec_model.preprocessor(input_signal=reference_audio, length=reference_audio_length)  \n",
    "    \n",
    "        \n",
    "    with torch.no_grad():\n",
    "        spectrogram = spec_model.forward(\n",
    "          text=text, \n",
    "          input_lens=text_len,\n",
    "          pitch=pitch,\n",
    "          spec=spect, \n",
    "          mel_lens=spect_len, \n",
    "          attn_prior=attn_prior,\n",
    "          speaker=speaker,\n",
    "          reference_spec=reference_spec,\n",
    "          reference_spec_lens=reference_spec_len,\n",
    "        )[0]\n",
    "    \n",
    "    spec = spectrogram[0].to('cpu').numpy()\n",
    "    np.save(save_path, spec)\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed2ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_FINETUNED_FASTPITCH_CHECKPOINT_FOLDER = \"/home/chsieh/NeMo/NeMoTTS_logs/FastPitch/2023-04-10_21-09-10\"\n",
    "\n",
    "# Pretrained FastPitch Weights\n",
    "spec_model = FastPitchModel.load_from_checkpoint(YOUR_PRETRAINED_FASTPITCH_CHECKPOINT, strict=False)\n",
    "\n",
    "# Load Adapter Weights\n",
    "spec_model.load_adapters(f'{YOUR_FINETUNED_FASTPITCH_CHECKPOINT_FOLDER}/checkpoints/adapters.pt')\n",
    "spec_model.freeze()\n",
    "spec_model.unfreeze_enabled_adapters()\n",
    "\n",
    "# Load Weighted Speaker Embedding Weights\n",
    "wemb = WeightedSpeakerEmbedding(pretrained_embedding=spec_model.fastpitch.speaker_emb)\n",
    "wemb.embedding_weight.data = torch.load(f'{YOUR_FINETUNED_FASTPITCH_CHECKPOINT_FOLDER}/checkpoints/wemb.pt')\n",
    "wemb.embedding_weight.requires_grad = False\n",
    "spec_model.fastpitch.speaker_emb = wemb\n",
    "spec_model.eval().cuda()\n",
    "\n",
    "beta_binomial_interpolator = BetaBinomialInterpolator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11267b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(melsdir, exist_ok=True)\n",
    "\n",
    "# Train\n",
    "train_datas = json_reader(train_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in train_datas])\n",
    "speaker_to_index = defaultdict(list)\n",
    "for i, d in enumerate(train_datas): speaker_to_index[d.get('speaker', None)].append(i)\n",
    "speaker_to_index = {k: set(v) for k, v in speaker_to_index.items()}\n",
    "\n",
    "for i, record in enumerate(tqdm(train_datas)):\n",
    "    record[\"mel_filepath\"] =  gen_spectrogram(i, train_datas, speaker_to_index, melsdir, pitch_dir, base_data_dir)\n",
    "\n",
    "json_writer(train_datas, train_manifest)\n",
    "\n",
    "\n",
    "# Valid\n",
    "valid_datas = json_reader(valid_manifest)\n",
    "base_data_dir = get_base_dir([item[\"audio_filepath\"] for item in valid_datas])\n",
    "speaker_to_index = defaultdict(list)\n",
    "for i, d in enumerate(valid_datas): speaker_to_index[d.get('speaker', None)].append(i)\n",
    "speaker_to_index = {k: set(v) for k, v in speaker_to_index.items()}\n",
    "\n",
    "for i, record in enumerate(tqdm(valid_datas)):\n",
    "    record[\"mel_filepath\"] =  gen_spectrogram(i, valid_datas, speaker_to_index, melsdir, pitch_dir, base_data_dir)\n",
    "\n",
    "json_writer(valid_datas, valid_manifest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f9d2ec",
   "metadata": {},
   "source": [
    "## b. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861164cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd {confdir} \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/conf/hifigan/hifigan_44100.yaml\n",
    "!cd {confdir} \\\n",
    "&& mkdir -p model/train_ds && cd model/train_ds && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/conf/hifigan/model/train_ds/train_ds_finetune.yaml \n",
    "!cd {confdir} \\\n",
    "&& mkdir -p model/validation_ds && cd model/validation_ds && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/conf/hifigan/model/validation_ds/val_ds_finetune.yaml\n",
    "!cd {confdir} \\\n",
    "&& mkdir -p model/generator && cd model/generator && wget https://raw.githubusercontent.com/nvidia/NeMo/$BRANCH/examples/tts/conf/hifigan/model/generator/v1_44100.yaml\n",
    "!cd {codedir} \\\n",
    "&& wget https://raw.githubusercontent.com/NVIDIA/NeMo/$BRANCH/examples/tts/hifigan_finetune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43072a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally 500 epochs \n",
    "!(python {codedir}/hifigan_finetune.py \\\n",
    "--config-path={os.path.abspath(confdir)} \\\n",
    "--config-name=hifigan_44100.yaml \\\n",
    "train_dataset={train_manifest} \\\n",
    "validation_datasets={valid_manifest} \\\n",
    "+init_from_ptl_ckpt={YOUR_FINETUNED_HIFIGAN_ON_MULTISPEAKER_CHECKPOINT} \\\n",
    "model.train_ds.dataloader_params.batch_size=32 \\\n",
    "model.optim.lr=0.0001 \\\n",
    "+trainer.max_epochs=20 \\\n",
    "trainer.check_val_every_n_epoch=20 \\\n",
    "model/train_ds=train_ds_finetune \\\n",
    "model/validation_ds=val_ds_finetune \\\n",
    "trainer.devices=1 \\\n",
    "trainer.strategy='ddp' \\\n",
    "trainer.precision=16 \\\n",
    "exp_manager.exp_dir={logsdir} \\\n",
    "exp_manager.create_wandb_logger=True \\\n",
    "exp_manager.wandb_logger_kwargs.name=\"tutorial-HiFiGAN\" \\\n",
    "exp_manager.wandb_logger_kwargs.project=\"NeMo\" \\\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a8b3d",
   "metadata": {},
   "source": [
    "# 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25efbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.tts.models import HifiGanModel\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abde8e3",
   "metadata": {},
   "source": [
    "## a. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb17da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_model = WaveformFeaturizer(sample_rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e3e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastPitch\n",
    "spec_model = FastPitchModel.load_from_checkpoint(YOUR_PRETRAINED_FASTPITCH_CHECKPOINT, strict=False)\n",
    "spec_model.load_adapters(f'{YOUR_FINETUNED_FASTPITCH_CHECKPOINT_FOLDER}/checkpoints/adapters.pt')\n",
    "spec_model.freeze()\n",
    "spec_model.unfreeze_enabled_adapters()\n",
    "wemb = WeightedSpeakerEmbedding(pretrained_embedding=spec_model.fastpitch.speaker_emb)\n",
    "wemb.embedding_weight.data = torch.load(f'{YOUR_FINETUNED_FASTPITCH_CHECKPOINT_FOLDER}/checkpoints/wemb.pt')\n",
    "wemb.embedding_weight.requires_grad = False\n",
    "spec_model.fastpitch.speaker_emb = wemb\n",
    "spec_model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa1985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HiFiGAN\n",
    "YOUR_FINETUNED_HIFIGAN_ON_ADAPTATION_CHECKPOINT = \"\"\n",
    "vocoder_model = HifiGanModel.load_from_checkpoint(checkpoint_path=YOUR_FINETUNED_HIFIGAN_ON_ADAPTATION_CHECKPOINT)\n",
    "vocoder_model.eval().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ce3eb",
   "metadata": {},
   "source": [
    "## b. Output Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302fe2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gt_spectrogram(audio_path, wave_model, spec_gen_model):\n",
    "    features = wave_model.process(audio_path, trim=False)\n",
    "    audio, audio_length = features, torch.tensor(features.shape[0]).long()\n",
    "    audio = audio.unsqueeze(0).to(device=spec_gen_model.device)\n",
    "    audio_length = audio_length.unsqueeze(0).to(device=spec_gen_model.device)\n",
    "    with torch.no_grad():\n",
    "        spectrogram, spec_len = spec_gen_model.preprocessor(input_signal=audio, length=audio_length)\n",
    "    return spectrogram, spec_len\n",
    "\n",
    "def gen_spectrogram(text, spec_gen_model, speaker, reference_spec, reference_spec_lens):\n",
    "    parsed = spec_gen_model.parse(text)\n",
    "    speaker = torch.tensor([speaker]).long().to(device=spec_gen_model.device)\n",
    "    with torch.no_grad():    \n",
    "        spectrogram = spec_gen_model.generate_spectrogram(tokens=parsed, \n",
    "                                                          speaker=speaker, \n",
    "                                                          reference_spec=reference_spec, \n",
    "                                                          reference_spec_lens=reference_spec_lens)\n",
    "\n",
    "    return spectrogram\n",
    "  \n",
    "def synth_audio(vocoder_model, spectrogram):    \n",
    "    with torch.no_grad():  \n",
    "        audio = vocoder_model.convert_spectrogram_to_audio(spec=spectrogram)\n",
    "    if isinstance(audio, torch.Tensor):\n",
    "        audio = audio.to('cpu').numpy()\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabf8001",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(gensdir, exist_ok=True)\n",
    "\n",
    "# Reference Audio\n",
    "with open(train_manifest, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        reference_record = json.loads(line)\n",
    "        break\n",
    "        \n",
    "# Validatation Audio\n",
    "num_val = 10\n",
    "val_records = []\n",
    "with open(valid_manifest, \"r\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        val_records.append(json.loads(line))\n",
    "        if len(val_records) >= num_val:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(gensdir, f'manifest.json'), 'w') as f_manifest: \n",
    "    for i, val_record in enumerate(val_records):\n",
    "        f_manifest.write(json.dumps(val_record) + '\\n')\n",
    "        \n",
    "        audio = ipd.Audio(val_record['audio_filepath'], rate=sample_rate)\n",
    "        with open(os.path.join(gensdir, f'{i}-GT.wav'), 'wb') as f: f.write(audio.data)\n",
    "\n",
    "        reference_spec, reference_spec_lens = gt_spectrogram(reference_record['audio_filepath'], wave_model, spec_model)\n",
    "        reference_spec = reference_spec.to(spec_model.device)\n",
    "        spec_pred = gen_spectrogram(val_record['text'], spec_model,\n",
    "                                    speaker=val_record['speaker'], \n",
    "                                    reference_spec=reference_spec, reference_spec_lens=reference_spec_lens)\n",
    "\n",
    "        audio = synth_audio(vocoder_model, spec_pred)\n",
    "        audio = ipd.Audio(audio, rate=sample_rate)\n",
    "        with open(os.path.join(gensdir, f'{i}-Gen.wav'), 'wb') as f: f.write(audio.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94941b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
