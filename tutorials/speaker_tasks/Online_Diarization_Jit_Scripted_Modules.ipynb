{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d3d4d3",
   "metadata": {},
   "source": [
    "# Speaker Clustering: Torch Scripted Module "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a331b6",
   "metadata": {},
   "source": [
    "Provide the NeMo path to `NEMO_BRANCH_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45d90716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check NeMo PATH: ['/home/taejinp/projects/_streaming_mulspk_asr/NeMo/nemo']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# NEMO_BRANCH_PATH = '/your/path/to/diar_torch/NeMo/'\n",
    "NEMO_BRANCH_PATH = '/home/taejinp/projects/_streaming_mulspk_asr/NeMo/'\n",
    "sys.path.insert(0, NEMO_BRANCH_PATH)\n",
    "import nemo\n",
    "print(\"Check NeMo PATH:\", nemo.__path__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b45939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc61cfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-02-10 16:57:53 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
      "[NeMo W 2023-02-10 16:57:53 experimental:27] Module <class 'nemo.collections.asr.models.audio_to_audio_model.AudioToAudioModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-02-10 16:57:54 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-02-10 16:57:54 nemo_logging:349] /home/taejinp/anaconda3/lib/python3.9/site-packages/torch/jit/annotations.py:309: UserWarning: TorchScript will treat type annotations of Tensor dtype-specific subtypes as if they are normal Tensors. dtype constraints are not enforced in compilation either.\n",
      "      warnings.warn(\"TorchScript will treat type annotations of Tensor \"\n",
      "    \n",
      "[NeMo W 2023-02-10 16:57:54 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_audio.BaseAudioDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-02-10 16:57:54 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_audio.AudioToTargetDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-02-10 16:57:54 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_audio.AudioToTargetWithReferenceDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-02-10 16:57:54 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_audio.AudioToTargetWithEmbeddingDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-02-10 16:57:54 experimental:27] Module <class 'nemo.collections.asr.models.enhancement_models.EncMaskDecAudioToAudioModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check NeMo PATH: ['/home/taejinp/projects/_streaming_mulspk_asr/NeMo/nemo']\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.asr.parts.utils.online_clustering import OnlineSpeakerClustering\n",
    "from nemo.collections.asr.parts.utils.speaker_utils import OnlineSegmentor\n",
    "import nemo\n",
    "print(\"Check NeMo PATH:\", nemo.__path__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cce91cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_segmentor = OnlineSegmentor(sample_rate=16000)\n",
    "\n",
    "# Export and save torch jit script module.\n",
    "online_segmentor = torch.jit.script(online_segmentor)\n",
    "torch.jit.save(online_segmentor, 'online_segmentor.pt')\n",
    "online_segmentor = torch.jit.load('online_segmentor.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3827e182",
   "metadata": {},
   "source": [
    "Setup a toy audio signal to check if online segmentor is working without an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf38fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_secs = 7\n",
    "sample_rate = 16000\n",
    "chunk_len = 1.0\n",
    "buffer_pad_half = 2.0\n",
    "buffer_len_sec = 5.0\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "signal_source = torch.rand(sample_rate*(2*n_secs),).to(device)\n",
    "online_segmentor = OnlineSegmentor(sample_rate=sample_rate)\n",
    "\n",
    "\n",
    "segment_raw_audio = []\n",
    "segment_range_ts = []\n",
    "segment_indexes = []\n",
    "\n",
    "window = 0.5\n",
    "shift = 0.25\n",
    "\n",
    "# \"\"\"\n",
    "# Frame is in the middle of the buffer.\n",
    "# |___Buffer___[___________]____________|\n",
    "# |____________[   Frame   ]____________|\n",
    "# | <- buffer start\n",
    "# |____________| <- frame start\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f5c82b",
   "metadata": {},
   "source": [
    "We assumee that we have 5 second buffer.\n",
    "in the middle of that buffer, we have 1 second frame (also called chunk in NeMo)\n",
    "In the very first segmentation, \n",
    "self.buffer_start = 0\n",
    "self.buffer_end = 5\n",
    "self.frame_start = 2\n",
    "\n",
    "And these values will be increased by 1 second per step.\n",
    "This 5 second buffer behaves like a Queue: \n",
    "new segment comes from the right and we delete the leftmost 1 second. \n",
    "\n",
    "Now let's run a loop that simulates incoming `audio_buffer` from the source signal `signal_source`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78eded99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= index 0 =====\n",
      "audio buffer shape torch.Size([80000]) 80000\n",
      "frame_start: 2.0 buffer_start: 0.0 buffer_end: 5.0\n",
      "segment ranges time stamps [[0.0, 0.5], [0.25, 0.75], [0.5, 1.0], [0.75, 1.25], [1.0, 1.5], [1.25, 1.75], [1.5, 2.0]]\n",
      "======= index 1 =====\n",
      "audio buffer shape torch.Size([80000]) 80000\n",
      "frame_start: 3.0 buffer_start: 1.0 buffer_end: 6.0\n",
      "segment ranges time stamps [[0.0, 0.5], [0.25, 0.75], [0.5, 1.0], [0.75, 1.25], [1.0, 1.5], [1.25, 1.75], [1.5, 2.0], [3.0, 3.5], [3.25, 3.75], [3.5, 4.0], [3.75, 4.25], [4.0, 4.5], [4.25, 4.75], [4.5, 5.0]]\n",
      "======= index 2 =====\n",
      "audio buffer shape torch.Size([80000]) 80000\n",
      "frame_start: 4.0 buffer_start: 2.0 buffer_end: 7.0\n",
      "segment ranges time stamps [[0.0, 0.5], [0.25, 0.75], [0.5, 1.0], [0.75, 1.25], [1.0, 1.5], [1.25, 1.75], [1.5, 2.0], [3.0, 3.5], [3.25, 3.75], [3.5, 4.0], [3.75, 4.25], [4.0, 4.5], [4.25, 4.75], [4.5, 5.0], [4.75, 5.25], [5.0, 5.5], [5.25, 5.75], [5.5, 6.0]]\n",
      "======= index 3 =====\n",
      "audio buffer shape torch.Size([80000]) 80000\n",
      "frame_start: 5.0 buffer_start: 3.0 buffer_end: 8.0\n",
      "segment ranges time stamps [[0.0, 0.5], [0.25, 0.75], [0.5, 1.0], [0.75, 1.25], [1.0, 1.5], [1.25, 1.75], [1.5, 2.0], [3.0, 3.5], [3.25, 3.75], [3.5, 4.0], [3.75, 4.25], [4.0, 4.5], [4.25, 4.75], [4.5, 5.0], [4.75, 5.25], [5.0, 5.5], [5.25, 5.75], [5.5, 6.0], [5.75, 6.25], [6.0, 6.5], [6.25, 6.75], [6.5, 7.0]]\n",
      "======= index 4 =====\n",
      "audio buffer shape torch.Size([80000]) 80000\n",
      "frame_start: 6.0 buffer_start: 4.0 buffer_end: 9.0\n",
      "segment ranges time stamps [[0.0, 0.5], [0.25, 0.75], [0.5, 1.0], [0.75, 1.25], [1.0, 1.5], [1.25, 1.75], [1.5, 2.0], [3.0, 3.5], [3.25, 3.75], [3.5, 4.0], [3.75, 4.25], [4.0, 4.5], [4.25, 4.75], [4.5, 5.0], [4.75, 5.25], [5.0, 5.5], [5.25, 5.75], [5.5, 6.0], [5.75, 6.25], [6.0, 6.5], [6.25, 6.75], [6.5, 7.0], [6.75, 7.25], [7.0, 7.5], [7.25, 7.75], [7.5, 8.0]]\n",
      "======= index 5 =====\n",
      "audio buffer shape torch.Size([80000]) 80000\n",
      "frame_start: 7.0 buffer_start: 5.0 buffer_end: 10.0\n",
      "segment ranges time stamps [[0.0, 0.5], [0.25, 0.75], [0.5, 1.0], [0.75, 1.25], [1.0, 1.5], [1.25, 1.75], [1.5, 2.0], [3.0, 3.5], [3.25, 3.75], [3.5, 4.0], [3.75, 4.25], [4.0, 4.5], [4.25, 4.75], [4.5, 5.0], [4.75, 5.25], [5.0, 5.5], [5.25, 5.75], [5.5, 6.0], [5.75, 6.25], [6.0, 6.5], [6.25, 6.75], [6.5, 7.0], [6.75, 7.25], [7.0, 7.5], [7.25, 7.75], [7.5, 8.0], [7.75, 8.25], [8.0, 8.5], [8.25, 8.75], [8.5, 9.0]]\n",
      "======= index 6 =====\n",
      "audio buffer shape torch.Size([80000]) 80000\n",
      "frame_start: 8.0 buffer_start: 6.0 buffer_end: 11.0\n",
      "segment ranges time stamps [[0.0, 0.5], [0.25, 0.75], [0.5, 1.0], [0.75, 1.25], [1.0, 1.5], [1.25, 1.75], [1.5, 2.0], [3.0, 3.5], [3.25, 3.75], [3.5, 4.0], [3.75, 4.25], [4.0, 4.5], [4.25, 4.75], [4.5, 5.0], [4.75, 5.25], [5.0, 5.5], [5.25, 5.75], [5.5, 6.0], [5.75, 6.25], [6.0, 6.5], [6.25, 6.75], [6.5, 7.0], [6.75, 7.25], [7.0, 7.5], [7.25, 7.75], [7.5, 8.0], [7.75, 8.25], [8.0, 8.5], [8.25, 8.75], [8.5, 9.0], [8.75, 9.25], [9.0, 9.5], [9.25, 9.75], [9.5, 10.0]]\n",
      "Final segment indexes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n",
      "Final segment ranges time stamps [[0.0, 0.5], [0.25, 0.75], [0.5, 1.0], [0.75, 1.25], [1.0, 1.5], [1.25, 1.75], [1.5, 2.0], [3.0, 3.5], [3.25, 3.75], [3.5, 4.0], [3.75, 4.25], [4.0, 4.5], [4.25, 4.75], [4.5, 5.0], [4.75, 5.25], [5.0, 5.5], [5.25, 5.75], [5.5, 6.0], [5.75, 6.25], [6.0, 6.5], [6.25, 6.75], [6.5, 7.0], [6.75, 7.25], [7.0, 7.5], [7.25, 7.75], [7.5, 8.0], [7.75, 8.25], [8.0, 8.5], [8.25, 8.75], [8.5, 9.0], [8.75, 9.25], [9.0, 9.5], [9.25, 9.75], [9.5, 10.0]]\n"
     ]
    }
   ],
   "source": [
    "# simulation of segmentation mechanism\n",
    "audio_buffer = torch.zeros_like(signal_source[sample_rate*0: int(sample_rate*(0+buffer_len_sec))])\n",
    "\n",
    "for k in range(n_secs):\n",
    "    print(f\"======= index {k} =====\")\n",
    "    \n",
    "    # VAD is assuming that all signal is speech. from k to (k+5)\n",
    "    # This is in \"seconds\"\n",
    "    vad_timestamps = torch.tensor([[k, (k+buffer_len_sec)]])\n",
    "    \n",
    "    # Fetch the time-series samples from signal source.\n",
    "    # This is in \"number of samples, integer\".\n",
    "    update_len = int(sample_rate * chunk_len)\n",
    "    \n",
    "    # We get 1 second samples (16000 samples) from the source audio\n",
    "    incoming_chunk = signal_source[sample_rate*k: int(sample_rate*(k+chunk_len))]\n",
    "    audio_buffer[:-update_len] = audio_buffer[update_len:].clone()\n",
    "    audio_buffer[-update_len:] = incoming_chunk\n",
    "\n",
    "    print(\"audio buffer shape\", audio_buffer.shape, len(audio_buffer))\n",
    "\n",
    "    # [ Important! ] at every step, we need to feed frame start, buffer start and buffer end\n",
    "    # This is implemented in _transfer_timestamps_to_segmentor() function in NeMo\n",
    "    # This is in \"seconds\"\n",
    "    online_segmentor.frame_start = (k+buffer_pad_half)*1.0\n",
    "    online_segmentor.buffer_start = k*1.0\n",
    "    online_segmentor.buffer_end = (k+buffer_len_sec)*1.0\n",
    "    \n",
    "    print(\"frame_start:\", online_segmentor.frame_start, \n",
    "          \"buffer_start:\", online_segmentor.buffer_start, \n",
    "          \"buffer_end:\", online_segmentor.buffer_end)\n",
    "    \n",
    "    audio_sigs, segment_ranges, range_inds = online_segmentor.run_online_segmentation(\n",
    "        audio_buffer=audio_buffer,\n",
    "        vad_timestamps=vad_timestamps,\n",
    "        segment_raw_audio=segment_raw_audio,\n",
    "        segment_range_ts=segment_range_ts,\n",
    "        segment_indexes=segment_indexes,\n",
    "        window=window,\n",
    "        shift=shift,\n",
    "    )\n",
    "    print(\"segment ranges time stamps\", segment_ranges)\n",
    "\n",
    "    \n",
    "    # We do this scale-for-scale in the real implementation.\n",
    "    segment_raw_audio = audio_sigs # Saves time-series signal to the online_diarizer memory\n",
    "    segment_range_ts = segment_ranges # Saves segment start-end time to the online diarizer memory\n",
    "    segment_indexes = range_inds # Saves segment index to the online diarizer memory\n",
    "    \n",
    "# Check out the segments from online segmentor module\n",
    "print(\"Final segment indexes\", segment_indexes)\n",
    "print(\"Final segment ranges time stamps\", segment_range_ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178003aa",
   "metadata": {},
   "source": [
    "Now that we checked segmentor, let's work on online clustering module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c935d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_clus = OnlineSpeakerClustering(\n",
    "    max_num_speakers=4,\n",
    "    max_rp_threshold=0.15,\n",
    "    sparse_search_volume=5,\n",
    "    history_buffer_size=100,\n",
    "    current_buffer_size=100,\n",
    ")\n",
    "\n",
    "# Export and save torch jit script module.\n",
    "online_clus = torch.jit.script(online_clus)\n",
    "torch.jit.save(online_clus, 'online_clus.pt')\n",
    "online_clus = torch.jit.load('online_clus.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8ec6f6",
   "metadata": {},
   "source": [
    "The following is a script to test clustering algorithm with a toy data.\n",
    "We can quickly check that scripted module is working without a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "45263c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check NeMo PATH: ['/home/taejinp/projects/_streaming_mulspk_asr/NeMo/nemo']\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.asr.data.audio_to_label import repeat_signal\n",
    "from nemo.collections.asr.parts.utils.offline_clustering import (\n",
    "    get_scale_interpolated_embs,\n",
    "    getCosAffinityMatrix,\n",
    "    split_input_data,\n",
    ")\n",
    "from nemo.collections.asr.parts.utils.online_clustering import (\n",
    "    OnlineSpeakerClustering,\n",
    "    get_closest_embeddings,\n",
    "    get_merge_quantity,\n",
    "    get_minimal_indices,\n",
    "    merge_vectors,\n",
    "    run_reducer,\n",
    "    stitch_cluster_labels,\n",
    ")\n",
    "from nemo.collections.asr.parts.utils.speaker_utils import (\n",
    "    check_ranges,\n",
    "    get_new_cursor_for_update,\n",
    "    get_online_subsegments_from_buffer,\n",
    "    get_speech_labels_for_update,\n",
    "    get_subsegments,\n",
    "    get_target_sig,\n",
    "    merge_float_intervals,\n",
    "    merge_int_intervals,\n",
    ")\n",
    "\n",
    "import nemo\n",
    "print(\"Check NeMo PATH:\", nemo.__path__)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def generate_orthogonal_embs(total_spks, perturb_sigma, emb_dim):\n",
    "    \"\"\"Generate a set of artificial orthogonal embedding vectors from random numbers\n",
    "    \"\"\"\n",
    "    gaus = torch.randn(emb_dim, emb_dim)\n",
    "    _svd = torch.linalg.svd(gaus)\n",
    "    orth = _svd[0] @ _svd[2]\n",
    "    orth_embs = orth[:total_spks]\n",
    "    # Assert orthogonality\n",
    "    assert torch.abs(getCosAffinityMatrix(orth_embs) - torch.diag(torch.ones(total_spks))).sum() < 1e-4\n",
    "    return orth_embs\n",
    "\n",
    "\n",
    "def generate_toy_data(\n",
    "    n_spks=2,\n",
    "    spk_dur=3,\n",
    "    emb_dim=192,\n",
    "    perturb_sigma=0.0,\n",
    "    ms_window=[1.5, 1.0, 0.5],\n",
    "    ms_shift=[0.75, 0.5, 0.25],\n",
    "    torch_seed=0,\n",
    "):\n",
    "    \"\"\"Generate a toy data to test clustering algorithms\n",
    "    \"\"\"\n",
    "    torch.manual_seed(torch_seed)\n",
    "    spk_timestamps = [(spk_dur * k, spk_dur) for k in range(n_spks)]\n",
    "    emb_list, seg_list = [], []\n",
    "    multiscale_segment_counts = [0 for _ in range(len(ms_window))]\n",
    "    ground_truth = []\n",
    "    random_orthogonal_embs = generate_orthogonal_embs(n_spks, perturb_sigma, emb_dim)\n",
    "    for scale_idx, (window, shift) in enumerate(zip(ms_window, ms_shift)):\n",
    "        for spk_idx, (offset, dur) in enumerate(spk_timestamps):\n",
    "            segments_stt_dur = get_subsegments(offset=offset, window=window, shift=shift, duration=dur)\n",
    "            segments = [[x[0], x[0] + x[1]] for x in segments_stt_dur]\n",
    "            emb_cent = random_orthogonal_embs[spk_idx, :]\n",
    "            emb = emb_cent.tile((len(segments), 1)) + 0.1 * torch.rand(len(segments), emb_dim)\n",
    "            seg_list.extend(segments)\n",
    "            emb_list.append(emb)\n",
    "            multiscale_segment_counts[scale_idx] += emb.shape[0]\n",
    "\n",
    "            if scale_idx == len(multiscale_segment_counts) - 1:\n",
    "                ground_truth.extend([spk_idx] * emb.shape[0])\n",
    "\n",
    "    emb_tensor = torch.concat(emb_list)\n",
    "    multiscale_segment_counts = torch.tensor(multiscale_segment_counts)\n",
    "    segm_tensor = torch.tensor(seg_list)\n",
    "    multiscale_weights = torch.ones(len(ms_window)).unsqueeze(0)\n",
    "    ground_truth = torch.tensor(ground_truth)\n",
    "    return emb_tensor, segm_tensor, multiscale_segment_counts, multiscale_weights, spk_timestamps, ground_truth\n",
    "\n",
    "\n",
    "def test_online_speaker_clustering(n_spks, total_sec, buffer_size, sigma, seed):\n",
    "    \"\"\"Test online speaker clustering algorithm with toy data parameters\n",
    "    \"\"\"\n",
    "    step_per_frame = 2\n",
    "    spk_dur = total_sec / n_spks\n",
    "    em, ts, mc, _, _, gt = generate_toy_data(n_spks, spk_dur=spk_dur, perturb_sigma=sigma, torch_seed=seed)\n",
    "    em_s, ts_s = split_input_data(em, ts, mc)\n",
    "\n",
    "    emb_gen = em_s[-1]\n",
    "    segment_indexes = ts_s[-1]\n",
    "    if torch.cuda.is_available():\n",
    "        emb_gen, segment_indexes = emb_gen.to(\"cuda\"), segment_indexes.to(\"cuda\")\n",
    "        cuda = True\n",
    "    else:\n",
    "        cuda = False\n",
    "\n",
    "    history_buffer_size = buffer_size\n",
    "    current_buffer_size = buffer_size\n",
    "\n",
    "    online_clus = OnlineSpeakerClustering(\n",
    "        max_num_speakers=8,\n",
    "        max_rp_threshold=0.15,\n",
    "        sparse_search_volume=30,\n",
    "        history_buffer_size=history_buffer_size,\n",
    "        current_buffer_size=current_buffer_size,\n",
    "    )\n",
    "    n_frames = int(emb_gen.shape[0] / step_per_frame)\n",
    "    evaluation_list = []\n",
    "\n",
    "    # Simulate online speaker clustering\n",
    "    for frame_index in range(n_frames):\n",
    "        curr_emb = emb_gen[0 : (frame_index + 1) * step_per_frame]\n",
    "        base_segment_indexes = np.arange(curr_emb.shape[0])\n",
    "        \n",
    "        curr_emb = torch.tensor(curr_emb)\n",
    "        base_segment_indexes = torch.tensor(base_segment_indexes)\n",
    "\n",
    "        # Call clustering function\n",
    "        merged_clus_labels = online_clus.forward(curr_emb=curr_emb, \n",
    "                                                 base_segment_indexes=base_segment_indexes, \n",
    "                                                 max_num_speakers=4,\n",
    "                                                 max_rp_threshold=0.15,\n",
    "                                                 enhanced_count_thres=40,\n",
    "                                                 sparse_search_volume=5,\n",
    "                                                 frame_index=frame_index, cuda=cuda)\n",
    "\n",
    "        gt = gt.to(merged_clus_labels.device)\n",
    "        \n",
    "        # Fix permutation to evaluatae clustering labels\n",
    "        merged_clus_labels = stitch_cluster_labels(Y_old=gt[: len(merged_clus_labels)], Y_new=merged_clus_labels)\n",
    "\n",
    "        evaluation_list.extend(list(merged_clus_labels == gt[: len(merged_clus_labels)]))\n",
    "        cumul_label_acc = sum(evaluation_list) / len(evaluation_list)\n",
    "        print(f\"Running Cumulative Label Acc. index-{frame_index} {100*cumul_label_acc.item():.4f}% Acc.\")\n",
    "        \n",
    "    assert online_clus.is_online\n",
    "    cumul_label_acc = sum(evaluation_list) / len(evaluation_list)\n",
    "    print(\"Final cumulative label accuracy\", cumul_label_acc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b75a286",
   "metadata": {},
   "source": [
    "Test if online speaker clustering works without error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "529dcfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-02-10 17:33:08 nemo_logging:349] /tmp/ipykernel_682115/2930441099.py:117: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "      curr_emb = torch.tensor(curr_emb)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Cumulative Label Acc. index-0 100.0000% Acc.\n",
      "Running Cumulative Label Acc. index-1 100.0000% Acc.\n",
      "Running Cumulative Label Acc. index-2 100.0000% Acc.\n",
      "Running Cumulative Label Acc. index-3 100.0000% Acc.\n",
      "Running Cumulative Label Acc. index-4 100.0000% Acc.\n",
      "Running Cumulative Label Acc. index-5 100.0000% Acc.\n",
      "Running Cumulative Label Acc. index-6 100.0000% Acc.\n",
      "Running Cumulative Label Acc. index-7 100.0000% Acc.\n",
      "Running Cumulative Label Acc. index-8 100.0000% Acc.\n",
      "Running Cumulative Label Acc. index-9 100.0000% Acc.\n",
      "Running Cumulative Label Acc. index-10 100.0000% Acc.\n",
      "Running Cumulative Label Acc. index-11 100.0000% Acc.\n",
      "Running Cumulative Label Acc. index-12 100.0000% Acc.\n",
      "Running Cumulative Label Acc. index-13 100.0000% Acc.\n",
      "Running Cumulative Label Acc. index-14 100.0000% Acc.\n",
      "Running Cumulative Label Acc. index-15 100.0000% Acc.\n",
      "Running Cumulative Label Acc. index-16 100.0000% Acc.\n",
      "Running Cumulative Label Acc. index-17 100.0000% Acc.\n",
      "Running Cumulative Label Acc. index-18 99.4737% Acc.\n",
      "Running Cumulative Label Acc. index-19 96.1905% Acc.\n",
      "Running Cumulative Label Acc. index-20 95.2381% Acc.\n",
      "Running Cumulative Label Acc. index-21 95.6522% Acc.\n",
      "Running Cumulative Label Acc. index-22 96.0145% Acc.\n",
      "Running Cumulative Label Acc. index-23 96.3333% Acc.\n",
      "Running Cumulative Label Acc. index-24 96.6154% Acc.\n",
      "Running Cumulative Label Acc. index-25 94.3020% Acc.\n",
      "Running Cumulative Label Acc. index-26 94.1799% Acc.\n",
      "Running Cumulative Label Acc. index-27 94.5813% Acc.\n",
      "Running Cumulative Label Acc. index-28 94.9425% Acc.\n",
      "Running Cumulative Label Acc. index-29 92.3656% Acc.\n",
      "Running Cumulative Label Acc. index-30 92.8427% Acc.\n",
      "Running Cumulative Label Acc. index-31 93.2765% Acc.\n",
      "Running Cumulative Label Acc. index-32 93.6720% Acc.\n",
      "Running Cumulative Label Acc. index-33 94.0336% Acc.\n",
      "Running Cumulative Label Acc. index-34 94.3651% Acc.\n",
      "Running Cumulative Label Acc. index-35 94.6697% Acc.\n",
      "Running Cumulative Label Acc. index-36 94.9502% Acc.\n",
      "Running Cumulative Label Acc. index-37 95.2092% Acc.\n",
      "Running Cumulative Label Acc. index-38 95.4487% Acc.\n",
      "Running Cumulative Label Acc. index-39 95.6707% Acc.\n",
      "Running Cumulative Label Acc. index-40 95.8769% Acc.\n",
      "Running Cumulative Label Acc. index-41 96.0687% Acc.\n",
      "Running Cumulative Label Acc. index-42 96.2474% Acc.\n",
      "Running Cumulative Label Acc. index-43 96.4141% Acc.\n",
      "Running Cumulative Label Acc. index-44 96.5700% Acc.\n",
      "Running Cumulative Label Acc. index-45 96.7160% Acc.\n",
      "Running Cumulative Label Acc. index-46 96.8528% Acc.\n",
      "Running Cumulative Label Acc. index-47 96.9813% Acc.\n",
      "Running Cumulative Label Acc. index-48 97.1020% Acc.\n",
      "Running Cumulative Label Acc. index-49 97.2157% Acc.\n",
      "Running Cumulative Label Acc. index-50 97.3228% Acc.\n",
      "Running Cumulative Label Acc. index-51 97.4238% Acc.\n",
      "Running Cumulative Label Acc. index-52 97.5192% Acc.\n",
      "Running Cumulative Label Acc. index-53 97.6094% Acc.\n",
      "Running Cumulative Label Acc. index-54 97.6948% Acc.\n",
      "Running Cumulative Label Acc. index-55 97.7757% Acc.\n",
      "Running Cumulative Label Acc. index-56 97.8524% Acc.\n",
      "Running Cumulative Label Acc. index-57 97.9252% Acc.\n",
      "Running Cumulative Label Acc. index-58 97.9944% Acc.\n",
      "Final cumulative label accuracy tensor(0.9799, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_online_speaker_clustering(n_spks=2, \n",
    "                               total_sec=30, \n",
    "                               buffer_size=40, \n",
    "                               sigma=0.05, \n",
    "                               seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecddb078",
   "metadata": {},
   "source": [
    "If overall accuracy is close to 1.0, then online clustering algorithm is working well. It gets affected by buffer size. Bigger buffer size leads to better performance but takes longer time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455044c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
