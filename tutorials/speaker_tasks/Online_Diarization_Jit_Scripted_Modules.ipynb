{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d3d4d3",
   "metadata": {},
   "source": [
    "# Speaker Clustering: Torch Scripted Module "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a331b6",
   "metadata": {},
   "source": [
    "Provide the NeMo path to `NEMO_BRANCH_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d90716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# NEMO_BRANCH_PATH = '/your/path/to/diar_torch/NeMo/'\n",
    "NEMO_BRANCH_PATH = '/home/taejinp/projects/_streaming_mulspk_asr/NeMo/'\n",
    "sys.path.insert(0, NEMO_BRANCH_PATH)\n",
    "import nemo\n",
    "print(\"Check NeMo PATH:\", nemo.__path__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b45939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc61cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.parts.utils.online_clustering import OnlineSpeakerClustering\n",
    "from nemo.collections.asr.parts.utils.speaker_utils import OnlineSegmentor\n",
    "import nemo\n",
    "print(\"Check NeMo PATH:\", nemo.__path__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cce91cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_segmentor = OnlineSegmentor(sample_rate=16000)\n",
    "\n",
    "# Export and save torch jit script module.\n",
    "online_segmentor = torch.jit.script(online_segmentor)\n",
    "torch.jit.save(online_segmentor, 'online_segmentor.pt')\n",
    "online_segmentor = torch.jit.load('online_segmentor.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163103ed",
   "metadata": {},
   "source": [
    "Setup a toy audio signal to check if online segmentor is working without an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d1f2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_secs = 7\n",
    "sample_rate = 16000\n",
    "chunk_len = 1.0\n",
    "buffer_pad_half = 2.0\n",
    "buffer_len_sec = 5.0\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "signal_source = torch.rand(sample_rate*(2*n_secs),).to(device)\n",
    "online_segmentor = OnlineSegmentor(sample_rate=sample_rate)\n",
    "\n",
    "\n",
    "segment_raw_audio = []\n",
    "segment_range_ts = []\n",
    "segment_indexes = []\n",
    "\n",
    "window = 0.5\n",
    "shift = 0.25\n",
    "\n",
    "# \"\"\"\n",
    "# Frame is in the middle of the buffer.\n",
    "# |___Buffer___[___________]____________|\n",
    "# |____________[   Frame   ]____________|\n",
    "# | <- buffer start\n",
    "# |____________| <- frame start\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b2239f",
   "metadata": {},
   "source": [
    "We assumee that we have 5 second buffer.\n",
    "in the middle of that buffer, we have 1 second frame (also called chunk in NeMo)\n",
    "In the very first segmentation, \n",
    "self.buffer_start = 0\n",
    "self.buffer_end = 5\n",
    "self.frame_start = 2\n",
    "\n",
    "And these values will be increased by 1 second per step.\n",
    "This 5 second buffer behaves like a Queue: \n",
    "new segment comes from the right and we delete the leftmost 1 second. \n",
    "\n",
    "Now let's run a loop that simulates incoming `audio_buffer` from the source signal `signal_source`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ccab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation of segmentation mechanism\n",
    "audio_buffer = torch.zeros_like(signal_source[sample_rate*0: int(sample_rate*(0+buffer_len_sec))])\n",
    "\n",
    "for k in range(n_secs):\n",
    "    print(f\"======= index {k} =====\")\n",
    "    \n",
    "    # VAD is assuming that all signal is speech. from k to (k+5)\n",
    "    # This is in \"seconds\"\n",
    "    vad_timestamps = torch.tensor([[k, (k+buffer_len_sec)]])\n",
    "    \n",
    "    # Fetch the time-series samples from signal source.\n",
    "    # This is in \"number of samples, integer\".\n",
    "    update_len = int(sample_rate * chunk_len)\n",
    "    \n",
    "    # We get 1 second samples (16000 samples) from the source audio\n",
    "    incoming_chunk = signal_source[sample_rate*k: int(sample_rate*(k+chunk_len))]\n",
    "    audio_buffer[:-update_len] = audio_buffer[update_len:].clone()\n",
    "    audio_buffer[-update_len:] = incoming_chunk\n",
    "\n",
    "    print(\"audio buffer shape\", audio_buffer.shape, len(audio_buffer))\n",
    "\n",
    "    # [ Important! ] at every step, we need to feed frame start, buffer start and buffer end\n",
    "    # This is implemented in _transfer_timestamps_to_segmentor() function in NeMo\n",
    "    # This is in \"seconds\"\n",
    "    online_segmentor.frame_start = (k+buffer_pad_half)*1.0\n",
    "    online_segmentor.buffer_start = k*1.0\n",
    "    online_segmentor.buffer_end = (k+buffer_len_sec)*1.0\n",
    "    \n",
    "    print(\"frame_start:\", online_segmentor.frame_start, \n",
    "          \"buffer_start:\", online_segmentor.buffer_start, \n",
    "          \"buffer_end:\", online_segmentor.buffer_end)\n",
    "    \n",
    "    audio_sigs, segment_ranges, range_inds = online_segmentor.run_online_segmentation(\n",
    "        audio_buffer=audio_buffer,\n",
    "        vad_timestamps=vad_timestamps,\n",
    "        segment_raw_audio=segment_raw_audio,\n",
    "        segment_range_ts=segment_range_ts,\n",
    "        segment_indexes=segment_indexes,\n",
    "        window=window,\n",
    "        shift=shift,\n",
    "    )\n",
    "    print(\"segment ranges time stamps\", segment_ranges)\n",
    "\n",
    "    \n",
    "    # We do this scale-for-scale in the real implementation.\n",
    "    segment_raw_audio = audio_sigs # Saves time-series signal to the online_diarizer memory\n",
    "    segment_range_ts = segment_ranges # Saves segment start-end time to the online diarizer memory\n",
    "    segment_indexes = range_inds # Saves segment index to the online diarizer memory\n",
    "    \n",
    "# Check out the segments from online segmentor module\n",
    "print(\"Final segment indexes\", segment_indexes)\n",
    "print(\"Final segment ranges time stamps\", segment_range_ts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de311b8",
   "metadata": {},
   "source": [
    "Now that we checked segmentor, let's work on online clustering module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "online_clus = OnlineSpeakerClustering(\n",
    "    max_num_speakers=4,\n",
    "    max_rp_threshold=0.15,\n",
    "    sparse_search_volume=5,\n",
    "    history_buffer_size=100,\n",
    "    current_buffer_size=100,\n",
    ")\n",
    "\n",
    "# Export and save torch jit script module.\n",
    "online_clus = torch.jit.script(online_clus)\n",
    "torch.jit.save(online_clus, 'online_clus.pt')\n",
    "online_clus = torch.jit.load('online_clus.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082350b4",
   "metadata": {},
   "source": [
    "The following is a script to test clustering algorithm with a toy data.\n",
    "We can quickly check that scripted module is working without a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfb0b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.data.audio_to_label import repeat_signal\n",
    "from nemo.collections.asr.parts.utils.offline_clustering import (\n",
    "    get_scale_interpolated_embs,\n",
    "    getCosAffinityMatrix,\n",
    "    split_input_data,\n",
    ")\n",
    "from nemo.collections.asr.parts.utils.online_clustering import (\n",
    "    OnlineSpeakerClustering,\n",
    "    get_closest_embeddings,\n",
    "    get_merge_quantity,\n",
    "    get_minimal_indices,\n",
    "    merge_vectors,\n",
    "    run_reducer,\n",
    "    stitch_cluster_labels,\n",
    ")\n",
    "from nemo.collections.asr.parts.utils.speaker_utils import (\n",
    "    check_ranges,\n",
    "    get_new_cursor_for_update,\n",
    "    get_online_subsegments_from_buffer,\n",
    "    get_speech_labels_for_update,\n",
    "    get_subsegments,\n",
    "    get_target_sig,\n",
    "    merge_float_intervals,\n",
    "    merge_int_intervals,\n",
    ")\n",
    "\n",
    "import nemo\n",
    "print(\"Check NeMo PATH:\", nemo.__path__)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def generate_orthogonal_embs(total_spks, perturb_sigma, emb_dim):\n",
    "    \"\"\"Generate a set of artificial orthogonal embedding vectors from random numbers\n",
    "    \"\"\"\n",
    "    gaus = torch.randn(emb_dim, emb_dim)\n",
    "    _svd = torch.linalg.svd(gaus)\n",
    "    orth = _svd[0] @ _svd[2]\n",
    "    orth_embs = orth[:total_spks]\n",
    "    # Assert orthogonality\n",
    "    assert torch.abs(getCosAffinityMatrix(orth_embs) - torch.diag(torch.ones(total_spks))).sum() < 1e-4\n",
    "    return orth_embs\n",
    "\n",
    "\n",
    "def generate_toy_data(\n",
    "    n_spks=2,\n",
    "    spk_dur=3,\n",
    "    emb_dim=192,\n",
    "    perturb_sigma=0.0,\n",
    "    ms_window=[1.5, 1.0, 0.5],\n",
    "    ms_shift=[0.75, 0.5, 0.25],\n",
    "    torch_seed=0,\n",
    "):\n",
    "    \"\"\"Generate a toy data to test clustering algorithms\n",
    "    \"\"\"\n",
    "    torch.manual_seed(torch_seed)\n",
    "    spk_timestamps = [(spk_dur * k, spk_dur) for k in range(n_spks)]\n",
    "    emb_list, seg_list = [], []\n",
    "    multiscale_segment_counts = [0 for _ in range(len(ms_window))]\n",
    "    ground_truth = []\n",
    "    random_orthogonal_embs = generate_orthogonal_embs(n_spks, perturb_sigma, emb_dim)\n",
    "    for scale_idx, (window, shift) in enumerate(zip(ms_window, ms_shift)):\n",
    "        for spk_idx, (offset, dur) in enumerate(spk_timestamps):\n",
    "            segments_stt_dur = get_subsegments(offset=offset, window=window, shift=shift, duration=dur)\n",
    "            segments = [[x[0], x[0] + x[1]] for x in segments_stt_dur]\n",
    "            emb_cent = random_orthogonal_embs[spk_idx, :]\n",
    "            emb = emb_cent.tile((len(segments), 1)) + 0.1 * torch.rand(len(segments), emb_dim)\n",
    "            seg_list.extend(segments)\n",
    "            emb_list.append(emb)\n",
    "            multiscale_segment_counts[scale_idx] += emb.shape[0]\n",
    "\n",
    "            if scale_idx == len(multiscale_segment_counts) - 1:\n",
    "                ground_truth.extend([spk_idx] * emb.shape[0])\n",
    "\n",
    "    emb_tensor = torch.concat(emb_list)\n",
    "    multiscale_segment_counts = torch.tensor(multiscale_segment_counts)\n",
    "    segm_tensor = torch.tensor(seg_list)\n",
    "    multiscale_weights = torch.ones(len(ms_window)).unsqueeze(0)\n",
    "    ground_truth = torch.tensor(ground_truth)\n",
    "    return emb_tensor, segm_tensor, multiscale_segment_counts, multiscale_weights, spk_timestamps, ground_truth\n",
    "\n",
    "\n",
    "def test_online_speaker_clustering(n_spks, total_sec, buffer_size, sigma, seed):\n",
    "    \"\"\"Test online speaker clustering algorithm with toy data parameters\n",
    "    \"\"\"\n",
    "    step_per_frame = 2\n",
    "    spk_dur = total_sec / n_spks\n",
    "    em, ts, mc, _, _, gt = generate_toy_data(n_spks, spk_dur=spk_dur, perturb_sigma=sigma, torch_seed=seed)\n",
    "    em_s, ts_s = split_input_data(em, ts, mc)\n",
    "\n",
    "    emb_gen = em_s[-1]\n",
    "    segment_indexes = ts_s[-1]\n",
    "    if torch.cuda.is_available():\n",
    "        emb_gen, segment_indexes = emb_gen.to(\"cuda\"), segment_indexes.to(\"cuda\")\n",
    "        cuda = True\n",
    "    else:\n",
    "        cuda = False\n",
    "\n",
    "    history_buffer_size = buffer_size\n",
    "    current_buffer_size = buffer_size\n",
    "\n",
    "    online_clus = OnlineSpeakerClustering(\n",
    "        max_num_speakers=8,\n",
    "        max_rp_threshold=0.15,\n",
    "        sparse_search_volume=30,\n",
    "        history_buffer_size=history_buffer_size,\n",
    "        current_buffer_size=current_buffer_size,\n",
    "    )\n",
    "    n_frames = int(emb_gen.shape[0] / step_per_frame)\n",
    "    evaluation_list = []\n",
    "\n",
    "    # Simulate online speaker clustering\n",
    "    for frame_index in range(n_frames):\n",
    "        curr_emb = emb_gen[0 : (frame_index + 1) * step_per_frame]\n",
    "        base_segment_indexes = np.arange(curr_emb.shape[0])\n",
    "        \n",
    "        curr_emb = torch.tensor(curr_emb)\n",
    "        base_segment_indexes = torch.tensor(base_segment_indexes)\n",
    "\n",
    "        # Call clustering function\n",
    "        # Two inputs are needed \n",
    "        merged_clus_labels = online_clus.forward(curr_emb=curr_emb, \n",
    "                                                 base_segment_indexes=base_segment_indexes, \n",
    "                                                 max_num_speakers=4,\n",
    "                                                 max_rp_threshold=0.15,\n",
    "                                                 enhanced_count_thres=40,\n",
    "                                                 sparse_search_volume=5,\n",
    "                                                 frame_index=frame_index, cuda=cuda)\n",
    "\n",
    "        gt = gt.to(merged_clus_labels.device)\n",
    "        \n",
    "        # Fix permutation to evaluatae clustering labels\n",
    "        merged_clus_labels = stitch_cluster_labels(Y_old=gt[: len(merged_clus_labels)], Y_new=merged_clus_labels)\n",
    "\n",
    "        evaluation_list.extend(list(merged_clus_labels == gt[: len(merged_clus_labels)]))\n",
    "        cumul_label_acc = sum(evaluation_list) / len(evaluation_list)\n",
    "        print(f\"Running Cumulative Label Acc. index-{frame_index} {100*cumul_label_acc.item():.4f}% Acc.\")\n",
    "        \n",
    "    assert online_clus.is_online\n",
    "    cumul_label_acc = sum(evaluation_list) / len(evaluation_list)\n",
    "    print(\"Final cumulative label accuracy\", cumul_label_acc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca63b6a",
   "metadata": {},
   "source": [
    "Test if online speaker clustering works without error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba72983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_online_speaker_clustering(n_spks=2, \n",
    "                               total_sec=30, \n",
    "                               buffer_size=40, \n",
    "                               sigma=0.05, \n",
    "                               seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7506dadf",
   "metadata": {},
   "source": [
    "If overall accuracy is close to 1.0, then online clustering algorithm is working well. It gets affected by buffer size. Bigger buffer size leads to better performance but takes longer time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d4c4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
