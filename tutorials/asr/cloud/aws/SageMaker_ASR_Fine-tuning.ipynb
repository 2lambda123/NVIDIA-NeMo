{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e86a2b3",
   "metadata": {},
   "source": [
    "# NeMo ASR Fine-tuning Using AWS SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215e3d3c",
   "metadata": {},
   "source": [
    "In this tutorial we show how you can fine-tune a pre-trained NeMo ASR Model using [Amazon Sagemaker](https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html).\n",
    "\n",
    "Using AWS SageMaker we fine-tune a Conformer CTC model using the AN4 dataset on a remote instance.\n",
    "\n",
    "The overall steps are:\n",
    "\n",
    "1. Setup your AWS Credentials to access SageMaker\n",
    "2. Download the source code we'll be running\n",
    "3. Configure the fine-tuning job\n",
    "4. Setup the AN4 dataset, upload data to S3\n",
    "5. Run fine-tuning job on SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac621da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "5. Restart the runtime (Runtime -> Restart Runtime) for any upgraded packages to take effect\n",
    "\"\"\"\n",
    "# If you're using Google Colab and not running locally, run this cell.\n",
    "\n",
    "## Install dependencies\n",
    "!pip install wget\n",
    "!apt-get install sox libsndfile1 ffmpeg\n",
    "!pip install text-unidecode\n",
    "!pip install matplotlib>=3.3.2\n",
    "\n",
    "## Install NeMo\n",
    "BRANCH = 'main'\n",
    "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[all]\n",
    "\n",
    "\"\"\"\n",
    "Remember to restart the runtime for the kernel to pick up any upgraded packages (e.g. matplotlib)!\n",
    "Alternatively, you can uncomment the exit() below to crash and restart the kernel, in the case\n",
    "that you want to use the \"Run All Cells\" (or similar) option.\n",
    "\"\"\"\n",
    "# exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c4fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sagemaker awscli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876f553d",
   "metadata": {},
   "source": [
    "### 1. Setup SageMaker with AWS Credentials\n",
    "\n",
    "If you haven't setup your AWS credentials, setup using the configuration CLI.\n",
    "You will need your access and Secret key, with permissions to use SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1328482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01477d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import sagemaker\n",
    "import wget\n",
    "from omegaconf import OmegaConf\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "from nemo.utils.notebook_utils import download_an4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405806f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d099a96",
   "metadata": {},
   "source": [
    "### 2. Download the NeMo source code\n",
    "\n",
    "SageMaker allows you to pass in your own source code, with an entrypoint script.\n",
    "\n",
    "Below we download the AWS NeMo `config.yaml` which contains our configuration, and the `speech_to_text_ctc_finetune.py` script to run fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b456c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dir = Path('./code/')\n",
    "config_dir = code_dir / 'conf/'\n",
    "data_dir = Path('./data/')\n",
    "code_dir.mkdir(exist_ok=True)\n",
    "config_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721c7b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = str(config_dir / \"config.yaml\")\n",
    "wget.download(\n",
    "    \"https://raw.githubusercontent.com/NVIDIA/NeMo/feat/aws-asr/tutorials/asr/cloud/aws/conf/config.yaml\", config_path\n",
    ")\n",
    "wget.download(\n",
    "    \"https://raw.githubusercontent.com/NVIDIA/NeMo/feat/aws-asr/tutorials/asr/cloud/aws/speech_to_text_ctc_finetune.py\",\n",
    "    str(code_dir),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7934baab",
   "metadata": {},
   "source": [
    "We also create a `requirements.txt` file within our source code to install NeMo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397d8eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(code_dir / 'requirements.txt', 'w') as f:\n",
    "    f.write(\"nemo_toolkit[all]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6321e3a9",
   "metadata": {},
   "source": [
    "### 3. Configure the fine-tuning job\n",
    "\n",
    "Now we configure the fine-tuning job, by modifying the `config.yaml` file that is stored in our source code directory.\n",
    "We pass relative directory paths for the data, and the path to the `pretrained_model` we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb61640",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = OmegaConf.load(config_path)\n",
    "\n",
    "conf.pretrained_model_name = \"nvidia/stt_en_conformer_ctc_large\"\n",
    "\n",
    "conf.model.train_ds.manifest_filepath = (\"/opt/ml/input/data/training/an4/train_manifest.json\",)\n",
    "conf.model.validation_ds.manifest_filepath = \"/opt/ml/input/data/testing/an4/test_manifest.json\"\n",
    "conf.trainer.accelerator = \"gpu\"\n",
    "conf.trainer.max_epochs = 1\n",
    "OmegaConf.save(conf, config_dir / 'config.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa2199e",
   "metadata": {},
   "source": [
    "### 4. Setup the AN4 Dataset, upload data to S3\n",
    "\n",
    "We now download our training and validation data, uploading to S3 so that SageMaker can mount our data to the instance at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c5a60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# within the SageMaker container, mount_dir will be where our data is stored.\n",
    "download_an4(\n",
    "    data_dir=str(data_dir),\n",
    "    train_mount_dir=\"/opt/ml/input/data/training/\",\n",
    "    test_mount_dir=\"/opt/ml/input/data/testing/\",\n",
    ")\n",
    "\n",
    "# Upload to the default bucket\n",
    "prefix = \"an4\"\n",
    "bucket = sess.default_bucket()\n",
    "loc = sess.upload_data(path=str(data_dir), bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959da702",
   "metadata": {},
   "source": [
    "### 4. Run fine-tuning job on SageMaker\n",
    "\n",
    "Finally we pass the path of the training and validation data on S3 + the output directory on S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2e44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = {\"training\": loc, \"testing\": loc}\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "output_path = \"s3://\" + sess.default_bucket() + \"/nemo-output/\"\n",
    "\n",
    "local_mode = True\n",
    "\n",
    "if local_mode:\n",
    "    instance_type = \"local_gpu\"\n",
    "else:\n",
    "    instance_type = \"ml.p2.xlarge\"\n",
    "\n",
    "est = PyTorch(\n",
    "    entry_point=\"speech_to_text_ctc_finetune.py\",\n",
    "    source_dir=\"code\",  # directory of your training script\n",
    "    role=role,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    framework_version=\"1.12.0\",\n",
    "    py_version=\"py38\",\n",
    "    volume_size=250,\n",
    "    output_path=output_path,\n",
    "    hyperparameters={'config-path': 'conf'},\n",
    ")\n",
    "\n",
    "est.fit(inputs=channels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
